{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4b3c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import random\n",
    "# from openai import AzureOpenAI # Assuming 'client' is already initialized\n",
    "import time # Import time for delays between API calls\n",
    "import json # To save comparison results\n",
    "import math # For entropy and log in PMI\n",
    "from collections import Counter # For entropy and PMI counts\n",
    "import sacrebleu # For BLEU\n",
    "from bert_score import score as bert_score_fn # For BERTScore\n",
    "import numpy as np # For averaging\n",
    "# from scipy.stats import rel_entr # For KL Divergence (relative entropy) - Requires SciPy >= 0.15.0\n",
    "from scipy.stats import entropy # Alternative for KL divergence calculation, available in older SciPy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer # For classifier features\n",
    "from sklearn.linear_model import LogisticRegression # Simple classifier for demonstration\n",
    "from sklearn.model_selection import train_test_split # For splitting data\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score # For AUC and AUPRC\n",
    "from sklearn.pipeline import Pipeline # To chain vectorizer and classifier\n",
    "\n",
    "# --- Configuration ---\n",
    "# Assume AZURE_OPENAI_ENDPOINT, AZURE_OPENAI_API_KEY, AZURE_OPENAI_DEPLOYMENT_NAME, API_VERSION\n",
    "# and the 'client' AzureOpenAI object are initialized and available from previous blocks.\n",
    "\n",
    "# Specify the directory containing the original pseudonymized Markdown files for comparison\n",
    "# NOTE: This path might differ from the input PDF path if you saved pseudonymized files elsewhere\n",
    "PSEUDO_MD_DIRECTORY_PATH_COMPARE = r\".\\Lagerugpijn\\pseudonymized-epds\" # Assuming pseudo_*.md are here\n",
    "\n",
    "# Specify the directory containing the generated synthetic Markdown files\n",
    "SYNTHETIC_MD_DIRECTORY_PATH = r\".\\Lagerugpijn\\synthetic_epds\"\n",
    "\n",
    "# Configure how many comparison pairs to evaluate using GPT-4\n",
    "# This should not exceed the number of synthetic files available\n",
    "NUM_COMPARISON_PAIRS_TO_EVALUATE = 5 # Example: Compare 5 synthetic files using GPT-4\n",
    "\n",
    "# Output file for comparison results (optional)\n",
    "COMPARISON_RESULTS_FILE = os.path.join(os.path.dirname(SYNTHETIC_MD_DIRECTORY_PATH), \"comparison_results.json\")\n",
    "\n",
    "# Minimum frequency for a bigram to be included in Avg PMI calculation\n",
    "PMI_MIN_BIGRAM_FREQ = 3\n",
    "\n",
    "# Parameters for Classifier Performance evaluation\n",
    "CLASSIFIER_TEST_SIZE = 0.3 # Percentage of data to use for testing\n",
    "CLASSIFIER_RANDOM_STATE = 42 # Random state for reproducibility\n",
    "CLASSIFIER_MAX_FEATURES = 1000 # Max features for TF-IDF Vectorizer\n",
    "\n",
    "\n",
    "# --- Helper Function to Load File Content ---\n",
    "def load_file_content(filepath):\n",
    "    \"\"\"Loads content from a markdown file.\"\"\"\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            return f.read()\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file {filepath}: {e}\")\n",
    "        return None\n",
    "\n",
    "# --- Benchmark Calculation Functions ---\n",
    "\n",
    "def calculate_entropy(text, unit='char'):\n",
    "    \"\"\"Calculates Shannon's Entropy for text.\"\"\"\n",
    "    if not text:\n",
    "        return 0.0\n",
    "    if unit == 'char':\n",
    "        tokens = list(text)\n",
    "    elif unit == 'word':\n",
    "        # Simple word tokenization by whitespace and lowercase\n",
    "        tokens = text.lower().split()\n",
    "    else:\n",
    "        raise ValueError(\"Unit must be 'char' or 'word'\")\n",
    "\n",
    "    if not tokens:\n",
    "        return 0.0\n",
    "\n",
    "    counts = Counter(tokens)\n",
    "    total_count = len(tokens)\n",
    "    entropy = 0.0\n",
    "    # Use log2 for entropy in bits\n",
    "    for count in counts.values():\n",
    "        p = count / total_count\n",
    "        entropy -= p * math.log2(p)\n",
    "    return entropy\n",
    "\n",
    "def calculate_avg_bigram_pmi(text, min_freq=3):\n",
    "    \"\"\"\n",
    "    Calculates the average Pointwise Mutual Information (PMI) for word bigrams\n",
    "    that occur at least min_freq times.\n",
    "    A proxy metric related to Mutual Information, measuring word association strength.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return 0.0\n",
    "\n",
    "    # Simple word tokenization and lowercase\n",
    "    words = text.lower().split()\n",
    "    if len(words) < 2:\n",
    "        return 0.0\n",
    "\n",
    "    word_counts = Counter(words)\n",
    "    bigram_counts = Counter(zip(words[:-1], words[1:])) # Count occurrences of bigrams\n",
    "\n",
    "    total_words = len(words)\n",
    "    # total_bigrams = len(list(zip(words[:-1], words[1:]))) # Count actual bigram instances\n",
    "\n",
    "    pmi_values = []\n",
    "    for bigram, bigram_count in bigram_counts.items():\n",
    "        # Only consider bigrams that meet the minimum frequency threshold\n",
    "        if bigram_count >= min_freq:\n",
    "            word1, word2 = bigram\n",
    "\n",
    "            # Calculate probabilities (using total_words for marginals is common)\n",
    "            p_w1 = word_counts[word1] / total_words if total_words > 0 else 0\n",
    "            p_w2 = word_counts[word2] / total_words if total_words > 0 else 0\n",
    "            p_w1_w2 = bigram_count / total_words if total_words > 0 else 0 # Use total words as normalization\n",
    "\n",
    "            # Avoid log(0) - check if probabilities are positive\n",
    "            if p_w1 > 0 and p_w2 > 0 and p_w1_w2 > 0:\n",
    "                 # PMI formula: log2( P(w1,w2) / (P(w1) * P(w2)) )\n",
    "                 pmi = math.log2(p_w1_w2 / (p_w1 * p_w2))\n",
    "                 pmi_values.append(pmi)\n",
    "            # Note: Bigrams that never appear together with positive marginals would have PMI -infinity.\n",
    "            # We only average over bigrams that *do* appear (with >= min_freq).\n",
    "\n",
    "    if not pmi_values:\n",
    "        return 0.0 # Return 0 if no bigrams meet min_freq or text was empty/too short\n",
    "\n",
    "    return np.mean(pmi_values)\n",
    "\n",
    "\n",
    "def calculate_kl_divergence(text1, text2, unit='word'):\n",
    "    \"\"\"\n",
    "    Calculates the symmetric Kullback-Leibler Divergence (JSD)\n",
    "    between the distributions of tokens (chars or words) in two texts.\n",
    "    KL(P || Q) is asymmetric, JSD is symmetric and always finite.\n",
    "    We'll use JSD = 0.5 * (KL(P || Q) + KL(Q || P)).\n",
    "    Uses scipy.stats.entropy for KL calculation, compatible with older SciPy.\n",
    "    \"\"\"\n",
    "    if not text1 or not text2:\n",
    "        return np.nan # Cannot compute divergence with empty text\n",
    "\n",
    "    if unit == 'char':\n",
    "        tokens1 = list(text1)\n",
    "        tokens2 = list(text2)\n",
    "    elif unit == 'word':\n",
    "        # Simple word tokenization and lowercase\n",
    "        tokens1 = text1.lower().split()\n",
    "        tokens2 = text2.lower().split()\n",
    "    else:\n",
    "        raise ValueError(\"Unit must be 'char' or 'word'\")\n",
    "\n",
    "    if not tokens1 or not tokens2:\n",
    "         return np.nan\n",
    "\n",
    "    # Build a combined vocabulary\n",
    "    vocab = list(set(tokens1 + tokens2))\n",
    "    vocab_size = len(vocab)\n",
    "\n",
    "    # Create frequency distributions\n",
    "    counts1 = Counter(tokens1)\n",
    "    counts2 = Counter(tokens2)\n",
    "\n",
    "    # Create probability distributions over the combined vocabulary\n",
    "    # Add a small smoothing value to avoid zero probabilities, which cause log(0) issues\n",
    "    smoothing = 1e-9\n",
    "    p1 = np.array([counts1.get(token, 0) + smoothing for token in vocab])\n",
    "    p2 = np.array([counts2.get(token, 0) + smoothing for token in vocab])\n",
    "\n",
    "    # Normalize to get probability distributions\n",
    "    p1 = p1 / p1.sum()\n",
    "    p2 = p2 / p2.sum()\n",
    "\n",
    "    # Calculate KL Divergence using scipy.stats.entropy\n",
    "    # entropy(pk, qk) calculates KL(pk || qk)\n",
    "    kl_pq = entropy(p1, qk=p2, base=2) # Use base=2 for bits\n",
    "    kl_qp = entropy(p2, qk=p1, base=2) # Use base=2 for bits\n",
    "\n",
    "\n",
    "    # Calculate Jensen-Shannon Divergence (JSD) - symmetric and bounded\n",
    "    # JSD = 0.5 * (KL(P || Q) + KL(Q || P))\n",
    "    jsd = 0.5 * (kl_pq + kl_qp)\n",
    "\n",
    "    # Note: Another common approach is to use JSD = H(M) - 0.5 * (H(P) + H(Q))\n",
    "    # where M = 0.5 * (P + Q) and H is Shannon Entropy. rel_entr is more direct,\n",
    "    # but using entropy(pk, qk) is also a valid way to get KL.\n",
    "\n",
    "    return jsd\n",
    "\n",
    "\n",
    "def calculate_corpus_bleu(synthetic_contents, pseudo_contents_list):\n",
    "    \"\"\"Calculates BLEU score for a corpus of synthetic texts against a list of references.\"\"\"\n",
    "    if not synthetic_contents or not pseudo_contents_list:\n",
    "        return np.nan # Use NaN for scores if input is empty\n",
    "\n",
    "    # sacrebleu expects references as list of lists, where each inner list\n",
    "    # contains all reference translations for one candidate.\n",
    "    # Here, all pseudo_contents_list serve as references for EACH synthetic_content.\n",
    "    references_for_all_candidates = [pseudo_contents_list] * len(synthetic_contents)\n",
    "\n",
    "    # sacrebleu.corpus_bleu expects candidates as a list of strings\n",
    "    # and references as a list of lists of strings.\n",
    "    try:\n",
    "        bleu = sacrebleu.corpus_bleu(synthetic_contents, references_for_all_candidates)\n",
    "        return bleu.score # Return the BLEU score (float)\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating BLEU: {e}\")\n",
    "        return np.nan # Return NaN in case of error\n",
    "\n",
    "\n",
    "def calculate_corpus_bertscore(synthetic_contents, pseudo_contents_list, lang='nl'):\n",
    "    \"\"\"Calculates BERTScore F1 for a corpus of synthetic texts against references.\"\"\"\n",
    "    if not synthetic_contents or not pseudo_contents_list:\n",
    "         # Return NaN for scores if input is empty\n",
    "         return np.nan, np.nan, np.nan\n",
    "\n",
    "    # BERT Score can be computationally intensive and requires torch\n",
    "    try:\n",
    "        # bert_score.score expects candidates as a list of strings\n",
    "        # and references as a list of strings. It computes pairwise scores,\n",
    "        # and we get the max reference score for each candidate.\n",
    "        # We need to provide ALL pseudo_contents as references for the entire set of candidates.\n",
    "\n",
    "        # Creating the references list of lists format needed by bert_score\n",
    "        references_bert = [pseudo_contents_list] * len(synthetic_contents)\n",
    "\n",
    "        print(\"  Running BERT Score...\")\n",
    "        # Note: verbose=True prints progress\n",
    "        P, R, F1 = bert_score_fn(synthetic_contents, references_bert, lang=lang, verbose=False) # Set verbose=False for cleaner output\n",
    "\n",
    "        # BERTScore returns tensors. We usually want the mean score across the corpus.\n",
    "        # .item() extracts the scalar value from a tensor\n",
    "        return P.mean().item(), R.mean().item(), F1.mean().item()\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating BERTScore: {e}\")\n",
    "        # Return NaN for scores in case of error\n",
    "        return np.nan, np.nan, np.nan\n",
    "\n",
    "def evaluate_classifier_performance(pseudo_contents, synthetic_contents, test_size=0.3, random_state=42, max_features=1000):\n",
    "    \"\"\"\n",
    "    Trains a classifier to distinguish between pseudonymized and synthetic data\n",
    "    and reports AUC and AUPRC.\n",
    "    Lower scores indicate better synthetic data (harder to distinguish).\n",
    "    \"\"\"\n",
    "    if not pseudo_contents or not synthetic_contents:\n",
    "        print(\"  Skipping Classifier Performance: Insufficient data.\")\n",
    "        return np.nan, np.nan\n",
    "\n",
    "    # Create labels: 0 for pseudo, 1 for synthetic\n",
    "    X = pseudo_contents + synthetic_contents\n",
    "    y = [0] * len(pseudo_contents) + [1] * len(synthetic_contents)\n",
    "\n",
    "    if len(X) < 2 or len(set(y)) < 2:\n",
    "        print(\"  Skipping Classifier Performance: Need at least two samples from each class.\")\n",
    "        return np.nan, np.nan\n",
    "\n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state, stratify=y)\n",
    "\n",
    "    # Create a pipeline: TF-IDF Vectorizer + Logistic Regression\n",
    "    model_pipeline = Pipeline([\n",
    "        ('tfidf', TfidfVectorizer(max_features=max_features)),\n",
    "        ('classifier', LogisticRegression(random_state=random_state, solver='liblinear')) # Use liblinear for smaller datasets\n",
    "    ])\n",
    "\n",
    "    print(\"  Training classifier to distinguish pseudo vs synthetic...\")\n",
    "    try:\n",
    "        # Train the model\n",
    "        model_pipeline.fit(X_train, y_train)\n",
    "\n",
    "        # Predict probabilities on the test set\n",
    "        y_pred_proba = model_pipeline.predict_proba(X_test)[:, 1] # Probability of belonging to the synthetic class (label 1)\n",
    "\n",
    "        # Calculate AUC and AUPRC\n",
    "        auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "        auprc_score = average_precision_score(y_test, y_pred_proba)\n",
    "\n",
    "        print(f\"  Classifier Performance calculated on {len(X_test)} test samples.\")\n",
    "        return auc_score, auprc_score\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during Classifier Performance evaluation: {e}\")\n",
    "        return np.nan, np.nan\n",
    "\n",
    "\n",
    "# --- Function to Compare Documents using Azure OpenAI GPT-4 (Reuse) ---\n",
    "def compare_docs_with_gpt4(client, pseudo_content, synthetic_content, pseudo_filename, synthetic_filename):\n",
    "    \"\"\"\n",
    "    Sends a pair of document contents to GPT-4 for similarity comparison\n",
    "    based on structure, style, clinical patterns, and realism.\n",
    "    \"\"\"\n",
    "    if not pseudo_content or not synthetic_content:\n",
    "        return {\"pseudo_file\": pseudo_filename, \"synthetic_file\": synthetic_filename,\n",
    "                \"status\": \"Skipped\", \"reason\": \"Failed to load file content\",\n",
    "                \"description\": \"N/A\", \"rating\": \"N/A\"}\n",
    "\n",
    "    # Prompt designed to instruct GPT-4 on the comparison task (Reuse)\n",
    "    system_prompt = \"\"\"Je bent een expert in het beoordelen van klinische documentatie en analyseert de gelijkenis in structuur, schrijfstijl en inhoudelijke patronen tussen paren van Nederlandse fysiotherapeutische patiëntdossiers.\"\"\"\n",
    "\n",
    "    user_prompt = f\"\"\"Beoordeel de gelijkenis tussen de twee onderstaande fysiotherapeutische patiëntdossiers. Dossier 1 is een gepseudonimiseerd voorbeeld uit de praktijk ('{pseudo_filename}'). Dossier 2 is een synthetisch gegenereerd dossier ('{synthetic_filename}').\n",
    "\n",
    "Focus je beoordeling op de volgende aspecten:\n",
    "- **Structuur:** Komen de belangrijke secties overeen (anamnese, ICF-diagnose, doelen, behandelplan, SOEP-notities)? Is de algehele opbouw vergelijkbaar?\n",
    "- **Schrijfstijl en toon:** Komt het taalgebruik, de formaliteit en de professionele toon overeen met realistische fysiotherapeutische verslaglegging in Nederland? Worden afkortingen (indien aanwezig) realistisch gebruikt en uitgebreid (indien nodig)?\n",
    "- **Klinische patronen en realisme:** Zijn de beschreven klachten, diagnoses, behandelinterventies en het verloop van de behandeling (in de SOEP-notities) klinisch plausibel en realistisch voor patiënten met lage rugpijn? Is de variatie in de voortgangsnotities (aantal sessies, beschreven progressie, setbacks, aanpassingen) realistisch en gevarieerd, vergelijkbaar met de voorbeelden?\n",
    "- **Adherentie aan format:** Volgen de voortgangsnotities consistent het SOEP-formaat (Subjectief, Objectief, Evaluatie, Plan)?\n",
    "\n",
    "**Belangrijk:** Vergelijk **NIET** de specifieke persoonsgegevens of de exacte inhoudelijke details van de klacht, doelen, scores (zoals specifieke NRS-waardes of PSK-scores), behandelingen of data, aangezien Dossier 2 een **nieuw, verzonnen geval** is en geen kopie van Dossier 1 of een ander voorbeeld. Beoordeel de gelijkenis op het niveau van het **sjabloon, de opbouw, de stijl, het detailniveau en de realistische weergave** van een fysiotherapeutisch proces voor lage rugpijn.\n",
    "\n",
    "Presenteer de twee dossiers:\n",
    "\n",
    "--- GEPSEUDONIMISEERD VOORBEELD DOSSIER: {pseudo_filename} ---\n",
    "{pseudo_content.strip()}\n",
    "--- EINDE GEPSEUDONIMISEERD VOORBEELD ---\n",
    "\n",
    "--- SYNTHETISCH GEGENEREERD DOSSIER: {synthetic_filename} ---\n",
    "{synthetic_content.strip()}\n",
    "--- EINDE SYNTHETISCH GEGENEREERD DOSSIER ---\n",
    "\n",
    "Geef nu je beoordeling. Begin met een beschrijving van de belangrijkste overeenkomsten en verschillen gebaseerd op de bovengenoemde criteria. Eindig op a new line with a summarizing judgment on the overall degree of similarity on a scale of 'Laag', 'Matig', or 'Hoog', followed by a brief explanation of why you reached this judgment.\n",
    "\n",
    "Formaat van de output:\n",
    "[Beschrijving van overeenkomsten en verschillen]\n",
    "Oordeel: [Laag/Matig/Hoog] - [Korte toelichting]\n",
    "\"\"\"\n",
    "\n",
    "    print(f\"  Comparing '{synthetic_filename}' with '{pseudo_filename}' using GPT-4...\")\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=AZURE_OPENAI_DEPLOYMENT_NAME, # Your GPT-4 deployment name\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            temperature=0.1, # Keep temperature low for consistent analysis\n",
    "            max_tokens=2000 # Enough tokens for the analysis output\n",
    "        )\n",
    "        comparison_text = response.choices[0].message.content.strip()\n",
    "\n",
    "        # Parse the output to extract description and rating\n",
    "        description = comparison_text\n",
    "        rating = \"N/A\"\n",
    "        if \"\\nOordeel: \" in comparison_text:\n",
    "            parts = comparison_text.split(\"\\nOordeel: \", 1)\n",
    "            description = parts[0].strip()\n",
    "            rating = parts[1].strip()\n",
    "\n",
    "        print(f\"  Comparison successful. Rating: {rating}\")\n",
    "\n",
    "        return {\"pseudo_file\": pseudo_filename, \"synthetic_file\": synthetic_filename,\n",
    "                \"status\": \"Success\", \"description\": description, \"rating\": rating}\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error calling Azure OpenAI API for comparison: {e}\")\n",
    "        # Consider adding a small delay or retry logic here\n",
    "        # time.sleep(5)\n",
    "        return {\"pseudo_file\": pseudo_filename, \"synthetic_file\": synthetic_filename,\n",
    "                \"status\": \"Failed\", \"reason\": str(e),\n",
    "                \"description\": \"N/A\", \"rating\": \"N/A\"}\n",
    "\n",
    "\n",
    "# --- Main Execution Logic for Evaluation ---\n",
    "if __name__ == \"__main__\":\n",
    "    # --- Assuming the previous script's main block finished and the client is available ---\n",
    "\n",
    "    print(\"\\n--- Starting Synthetic Data Evaluation ---\")\n",
    "\n",
    "    # Get lists of available files\n",
    "    # Using the comparison path for pseudo files as specified by the user\n",
    "    pseudo_files = glob.glob(os.path.join(PSEUDO_MD_DIRECTORY_PATH_COMPARE, \"pseudo_*.md\"))\n",
    "    synthetic_files = glob.glob(os.path.join(SYNTHETIC_MD_DIRECTORY_PATH, \"synthetic_patient_*.md\"))\n",
    "\n",
    "    if not pseudo_files:\n",
    "        print(f\"Error: No pseudonymized markdown files found in '{PSEUDO_MD_DIRECTORY_PATH_COMPARE}'. Cannot perform evaluation.\")\n",
    "        # exit() # Don't exit, allow partial evaluation if synthetic files exist\n",
    "\n",
    "    if not synthetic_files:\n",
    "        print(f\"Error: No synthetic markdown files found in '{SYNTHETIC_MD_DIRECTORY_PATH}'. Cannot perform evaluation.\")\n",
    "        # exit() # Don't exit if pseudo files exist\n",
    "\n",
    "    print(f\"Found {len(pseudo_files)} pseudonymized files.\")\n",
    "    print(f\"Found {len(synthetic_files)} synthetic files.\")\n",
    "\n",
    "    # Load ALL content for benchmarks\n",
    "    print(\"\\nLoading all file contents for benchmark calculations...\")\n",
    "    all_pseudo_contents = []\n",
    "    for f in pseudo_files:\n",
    "        content = load_file_content(f)\n",
    "        if content is not None:\n",
    "            all_pseudo_contents.append(content)\n",
    "        else:\n",
    "            print(f\"Warning: Could not load content for file {f}\")\n",
    "\n",
    "    all_synthetic_contents = []\n",
    "    for f in synthetic_files:\n",
    "        content = load_file_content(f)\n",
    "        if content is not None:\n",
    "            all_synthetic_contents.append(content)\n",
    "        else:\n",
    "            print(f\"Warning: Could not load content for file {f}\")\n",
    "\n",
    "\n",
    "    if not all_pseudo_contents or not all_synthetic_contents:\n",
    "        print(\"\\nInsufficient data loaded for comprehensive benchmarks. Skipping some calculations.\")\n",
    "        skip_benchmarks = True\n",
    "    else:\n",
    "        skip_benchmarks = False\n",
    "        print(f\"Loaded content for {len(all_pseudo_contents)} pseudonymized files.\")\n",
    "        print(f\"Loaded content for {len(all_synthetic_contents)} synthetic files.\")\n",
    "\n",
    "\n",
    "    # --- Calculate Benchmarks ---\n",
    "    benchmark_results = {}\n",
    "\n",
    "    if not skip_benchmarks:\n",
    "        print(\"\\nCalculating quantitative benchmark metrics...\")\n",
    "\n",
    "        # Concatenate all content for corpus-level metrics\n",
    "        corpus_pseudo_text = \"\\n\".join(all_pseudo_contents)\n",
    "        corpus_synthetic_text = \"\\n\".join(all_synthetic_contents)\n",
    "\n",
    "        # Shannon's Entropy (character level)\n",
    "        entropy_pseudo = calculate_entropy(corpus_pseudo_text, unit='char')\n",
    "        entropy_synthetic = calculate_entropy(corpus_synthetic_text, unit='char')\n",
    "        benchmark_results['shannon_entropy_char'] = {'pseudonymized': entropy_pseudo, 'synthetic': entropy_synthetic}\n",
    "\n",
    "        # Shannon's Entropy (word level)\n",
    "        entropy_pseudo_word = calculate_entropy(corpus_pseudo_text, unit='word')\n",
    "        entropy_synthetic_word = calculate_entropy(corpus_synthetic_text, unit='word')\n",
    "        benchmark_results['shannon_entropy_word'] = {'pseudonymized': entropy_pseudo_word, 'synthetic': entropy_synthetic_word}\n",
    "\n",
    "        # Average Document Length (Character Count) as a simple proxy\n",
    "        avg_len_pseudo = np.mean([len(c) for c in all_pseudo_contents]) if all_pseudo_contents else 0\n",
    "        avg_len_synthetic = np.mean([len(c) for c in all_synthetic_contents]) if all_synthetic_contents else 0\n",
    "        benchmark_results['avg_doc_length_chars'] = {'pseudonymized': avg_len_pseudo, 'synthetic': avg_len_synthetic}\n",
    "\n",
    "        # Average Bigram Pointwise Mutual Information (PMI)\n",
    "        avg_pmi_pseudo = calculate_avg_bigram_pmi(corpus_pseudo_text, min_freq=PMI_MIN_BIGRAM_FREQ)\n",
    "        avg_pmi_synthetic = calculate_avg_bigram_pmi(corpus_synthetic_text, min_freq=PMI_MIN_BIGRAM_FREQ)\n",
    "        benchmark_results['avg_bigram_pmi'] = {'pseudonymized': avg_pmi_pseudo, 'synthetic': avg_pmi_synthetic}\n",
    "\n",
    "        # KL Divergence (Word Distribution) - using JSD\n",
    "        # Note: KL divergence is asymmetric, JSD is symmetric. JSD is often preferred for comparing distributions.\n",
    "        # We calculate JSD between the word distributions of the two corpora.\n",
    "        jsd_word = calculate_kl_divergence(corpus_pseudo_text, corpus_synthetic_text, unit='word')\n",
    "        benchmark_results['jsd_word'] = jsd_word # Store JSD, note it's related to KL\n",
    "\n",
    "        # BLEU Score (Synthetic vs Pseudonymized References)\n",
    "        if all_synthetic_contents and all_pseudo_contents:\n",
    "            bleu_score = calculate_corpus_bleu(all_synthetic_contents, all_pseudo_contents)\n",
    "            benchmark_results['bleu_score'] = bleu_score\n",
    "        else:\n",
    "            benchmark_results['bleu_score'] = np.nan\n",
    "            print(\"Skipping BLEU calculation due to insufficient data.\")\n",
    "\n",
    "        # BERT Score (Synthetic vs Pseudonymized References)\n",
    "        # BERT Score can be computationally intensive.\n",
    "        if all_synthetic_contents and all_pseudo_contents:\n",
    "            print(\"Calculating BERT Score (this may take some time)...\")\n",
    "            bert_p, bert_r, bert_f1 = calculate_corpus_bertscore(all_synthetic_contents, all_pseudo_contents, lang='nl')\n",
    "            benchmark_results['bert_score'] = {'precision': bert_p, 'recall': bert_r, 'f1': bert_f1}\n",
    "        else:\n",
    "            benchmark_results['bert_score'] = {'precision': np.nan, 'recall': np.nan, 'f1': np.nan}\n",
    "            print(\"Skipping BERT Score calculation due to insufficient data.\")\n",
    "\n",
    "        # Classifier Performance (AUC/AUPRC)\n",
    "        # Requires a mix of pseudo and synthetic documents\n",
    "        if len(all_pseudo_contents) >= 1 and len(all_synthetic_contents) >= 1: # Need at least one of each\n",
    "             print(\"\\nCalculating Classifier Performance (AUC/AUPRC)...\")\n",
    "             auc_score, auprc_score = evaluate_classifier_performance(\n",
    "                 all_pseudo_contents,\n",
    "                 all_synthetic_contents,\n",
    "                 test_size=CLASSIFIER_TEST_SIZE,\n",
    "                 random_state=CLASSIFIER_RANDOM_STATE,\n",
    "                 max_features=CLASSIFIER_MAX_FEATURES\n",
    "             )\n",
    "             benchmark_results['classifier_performance'] = {'auc': auc_score, 'auprc': auprc_score}\n",
    "        else:\n",
    "             benchmark_results['classifier_performance'] = {'auc': np.nan, 'auprc': np.nan}\n",
    "             print(\"Skipping Classifier Performance calculation due to insufficient data.\")\n",
    "\n",
    "\n",
    "        # --- Add placeholder/note for Informational Accuracy ---\n",
    "        benchmark_results['informational_accuracy_note'] = \"A standard, generalizable metric for 'Informational Accuracy' between synthetic and real clinical text is not straightforward without a specific definition or clinical ontology. Aspects of information capture and clinical plausibility are covered qualitatively by the GPT-4 comparison and partially by BERTScore (semantic similarity) and length comparison.\"\n",
    "\n",
    "\n",
    "    # --- Perform Pairwise GPT-4 Comparison ---\n",
    "    # Need to get file paths corresponding to the loaded contents for pairwise comparison\n",
    "    # Filter file paths to only include those whose content was successfully loaded\n",
    "    loaded_pseudo_filepaths = [f for f in pseudo_files if load_file_content(f) in all_pseudo_contents]\n",
    "    loaded_synthetic_filepaths = [f for f in synthetic_files if load_file_content(f) in all_synthetic_contents]\n",
    "\n",
    "\n",
    "    actual_num_comparisons = min(NUM_COMPARISON_PAIRS_TO_EVALUATE, len(loaded_synthetic_filepaths))\n",
    "\n",
    "    comparison_results_gpt4 = [] # Initialize the list even if no comparisons run\n",
    "\n",
    "    if actual_num_comparisons > 0 and loaded_pseudo_filepaths:\n",
    "        print(f\"\\nInitiating {actual_num_comparisons} pairwise GPT-4 comparisons.\")\n",
    "\n",
    "        # Select synthetic files to compare (e.g., take the first N)\n",
    "        synthetic_files_for_gpt4 = loaded_synthetic_filepaths[:actual_num_comparisons]\n",
    "        # If you want random sampling instead: synthetic_files_for_gpt4 = random.sample(loaded_synthetic_filepaths, actual_num_comparisons)\n",
    "\n",
    "\n",
    "        for i, synthetic_filepath in enumerate(synthetic_files_for_gpt4):\n",
    "            comparison_index = i + 1\n",
    "            synthetic_filename = os.path.basename(synthetic_filepath)\n",
    "\n",
    "            # Randomly select one pseudonymized file for this comparison\n",
    "            if not loaded_pseudo_filepaths:\n",
    "                 print(f\"Skipping GPT-4 comparison {comparison_index}: No pseudonymized files available.\")\n",
    "                 continue\n",
    "\n",
    "            pseudo_filepath = random.choice(loaded_pseudo_filepaths)\n",
    "            pseudo_filename = os.path.basename(pseudo_filepath)\n",
    "\n",
    "            print(f\"\\n--- GPT-4 Comparison {comparison_index} of {actual_num_comparisons} ---\")\n",
    "            print(f\"  Synthetic: {synthetic_filename}\")\n",
    "            print(f\"  Pseudonymized (randomly selected): {pseudo_filename}\")\n",
    "\n",
    "            # Load file contents again for this specific pair (could optimize by using loaded content list)\n",
    "            synthetic_content = load_file_content(synthetic_filepath)\n",
    "            pseudo_content = load_file_content(pseudo_filepath)\n",
    "\n",
    "            # Perform the comparison using GPT-4\n",
    "            result = compare_docs_with_gpt4(client, pseudo_content, synthetic_content, pseudo_filename, synthetic_filename)\n",
    "            comparison_results_gpt4.append(result)\n",
    "\n",
    "            # Add a delay between API calls to manage rate limits\n",
    "            time.sleep(2) # Adjust delay as needed\n",
    "\n",
    "\n",
    "        print(\"\\n--- Pairwise GPT-4 Comparison Complete ---\")\n",
    "        print(\"\\nSummary of Pairwise GPT-4 Results:\")\n",
    "\n",
    "        # Print summary of results (Reuse)\n",
    "        for i, result in enumerate(comparison_results_gpt4):\n",
    "            print(f\"\\nComparison Pair {i+1}:\")\n",
    "            print(f\"  Synthetic: {result['synthetic_file']}\")\n",
    "            print(f\"  Pseudonymized: {result['pseudo_file']}\")\n",
    "            if result['status'] == 'Success':\n",
    "                print(f\"  Status: Success\")\n",
    "                print(f\"  Rating: {result['rating']}\")\n",
    "                # print(f\"  Description:\\n{result['description']}\") # Uncomment to print full description\n",
    "            else:\n",
    "                 print(f\"  Status: {result['status']}\")\n",
    "                 print(f\"  Reason: {result['reason']}\")\n",
    "    elif not loaded_pseudo_filepaths:\n",
    "         print(\"\\nSkipping pairwise GPT-4 comparisons: No pseudonymized files loaded successfully.\")\n",
    "    else: # actual_num_comparisons <= 0 or no synthetic files loaded\n",
    "        print(\"\\nSkipping pairwise GPT-4 comparisons as no synthetic files were loaded successfully or requested.\")\n",
    "\n",
    "\n",
    "    # --- Final Report ---\n",
    "    print(\"\\n--- Comprehensive Evaluation Report ---\")\n",
    "\n",
    "    # Report Benchmarks\n",
    "    print(\"\\nQuantitative Benchmark Metrics:\")\n",
    "    if 'shannon_entropy_char' in benchmark_results:\n",
    "         print(f\"  Shannon Entropy (Character):\")\n",
    "         print(f\"    Pseudonymized Corpus: {benchmark_results['shannon_entropy_char']['pseudonymized']:.4f}\")\n",
    "         print(f\"    Synthetic Corpus:     {benchmark_results['shannon_entropy_char']['synthetic']:.4f}\")\n",
    "    if 'shannon_entropy_word' in benchmark_results:\n",
    "         print(f\"  Shannon Entropy (Word):\")\n",
    "         print(f\"    Pseudonymized Corpus: {benchmark_results['shannon_entropy_word']['pseudonymized']:.4f}\")\n",
    "         print(f\"    Synthetic Corpus:     {benchmark_results['shannon_entropy_word']['synthetic']:.4f}\")\n",
    "    if 'avg_doc_length_chars' in benchmark_results:\n",
    "         print(f\"  Average Document Length (Characters):\")\n",
    "         print(f\"    Pseudonymized Files: {benchmark_results['avg_doc_length_chars']['pseudonymized']:.2f}\")\n",
    "         print(f\"    Synthetic Files:     {benchmark_results['avg_doc_length_chars']['synthetic']:.2f}\")\n",
    "    if 'avg_bigram_pmi' in benchmark_results:\n",
    "         print(f\"  Average Bigram Pointwise Mutual Information (PMI, min_freq={PMI_MIN_BIGRAM_FREQ}):\")\n",
    "         print(f\"    Pseudonymized Corpus: {benchmark_results['avg_bigram_pmi']['pseudonymized']:.4f}\")\n",
    "         print(f\"    Synthetic Corpus:     {benchmark_results['avg_bigram_pmi']['synthetic']:.4f}\")\n",
    "    if 'jsd_word' in benchmark_results and not np.isnan(benchmark_results['jsd_word']):\n",
    "         print(f\"  Jensen-Shannon Divergence (Word Distribution): {benchmark_results['jsd_word']:.4f}\")\n",
    "    else:\n",
    "         print(f\"  Jensen-Shannon Divergence (Word Distribution): N/A (Insufficient data)\")\n",
    "    if 'bleu_score' in benchmark_results and not np.isnan(benchmark_results['bleu_score']):\n",
    "         print(f\"  BLEU Score (Synthetic vs All Pseudonymized): {benchmark_results['bleu_score']:.4f}\")\n",
    "    else:\n",
    "         print(f\"  BLEU Score (Synthetic vs All Pseudonymized): N/A (Insufficient data)\")\n",
    "    if 'bert_score' in benchmark_results and not np.isnan(benchmark_results['bert_score']['f1']):\n",
    "         print(f\"  BERT Score (Synthetic vs All Pseudonymized):\")\n",
    "         print(f\"    Precision: {benchmark_results['bert_score']['precision']:.4f}\")\n",
    "         print(f\"    Recall:    {benchmark_results['bert_score']['recall']:.4f}\")\n",
    "         print(f\"    F1:        {benchmark_results['bert_score']['f1']:.4f}\")\n",
    "    else:\n",
    "         print(f\"  BERT Score (Synthetic vs All Pseudonymized): N/A (Insufficient data)\")\n",
    "\n",
    "    if 'classifier_performance' in benchmark_results and not np.isnan(benchmark_results['classifier_performance']['auc']):\n",
    "         print(f\"\\n  Classifier Performance (Distinguishing Pseudo vs Synthetic):\")\n",
    "         print(f\"    AUC:   {benchmark_results['classifier_performance']['auc']:.4f}\")\n",
    "         print(f\"    AUPRC: {benchmark_results['classifier_performance']['auprc']:.4f}\")\n",
    "         print(f\"    (Lower scores indicate better synthetic data - harder to distinguish)\")\n",
    "    else:\n",
    "         print(f\"\\n  Classifier Performance (Distinguishing Pseudo vs Synthetic): N/A (Insufficient data)\")\n",
    "\n",
    "\n",
    "    if 'informational_accuracy_note' in benchmark_results:\n",
    "         print(f\"\\n  Note on Informational Accuracy:\")\n",
    "         print(f\"    {benchmark_results['informational_accuracy_note']}\")\n",
    "\n",
    "\n",
    "    # Report GPT-4 Summary\n",
    "    print(\"\\nQualitative Pairwise GPT-4 Comparison Summary:\")\n",
    "    # Ensure comparison_results_gpt4 is defined\n",
    "    if comparison_results_gpt4:\n",
    "        ratings = [r['rating'] for r in comparison_results_gpt4 if r['status'] == 'Success' and r['rating'] in ['Laag', 'Matig', 'Hoog']]\n",
    "        if ratings:\n",
    "             rating_counts = Counter(ratings)\n",
    "             print(f\"  Overall GPT-4 Ratings Distribution: {rating_counts}\")\n",
    "        else:\n",
    "             print(\"  No successful GPT-4 comparisons yielded a Laag/Matig/Hoog rating.\")\n",
    "\n",
    "        # You could add code here to summarize the descriptions if needed\n",
    "\n",
    "    else:\n",
    "        print(\"  No pairwise GPT-4 comparisons were performed or successful.\")\n",
    "\n",
    "\n",
    "    # Save all results (benchmarks + GPT-4 summaries)\n",
    "    # Ensure comparison_results_gpt4 is included safely\n",
    "    full_results = {\n",
    "         \"benchmarks\": benchmark_results,\n",
    "         \"pairwise_gpt4_comparisons\": comparison_results_gpt4\n",
    "    }\n",
    "    try:\n",
    "        # Create the output directory if it doesn't exist\n",
    "        os.makedirs(os.path.dirname(COMPARISON_RESULTS_FILE), exist_ok=True)\n",
    "        with open(COMPARISON_RESULTS_FILE, 'w', encoding='utf-8') as f:\n",
    "            json.dump(full_results, f, indent=4, ensure_ascii=False)\n",
    "        print(f\"\\nFull evaluation report saved to: {COMPARISON_RESULTS_FILE}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving full results to JSON: {e}\")\n",
    "\n",
    "\n",
    "print(\"\\nEvaluation process complete.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azure-openai-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
