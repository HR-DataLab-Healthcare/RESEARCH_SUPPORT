{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6699454",
   "metadata": {},
   "source": [
    "To run the Python code in this notebook locally, you will need to install the following packages. You can install them using pip:\n",
    "\n",
    "```bash\n",
    "pip install PyMuPDF openai\n",
    "```\n",
    "\n",
    "*   **PyMuPDF (`fitz`)**: Used for PDF manipulation, though in the provided synthetic data generation script, its import might be a remnant if not directly used for generation output. It was likely used in prior data processing steps.\n",
    "*   **openai**: The official Python library for interacting with the OpenAI API, including Azure OpenAI services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e863c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import fitz  # PyMuPDF\n",
    "# Conditional imports based on LLM provider choice by the user\n",
    "# If using Azure OpenAI (default):\n",
    "from openai import AzureOpenAI\n",
    "# If adapting for Ollama, you might need to install and import its library:\n",
    "# e.g., import ollama # (after `pip install ollama`)\n",
    "# Or use standard libraries like requests for direct API calls:\n",
    "# import requests\n",
    "import glob # Using glob is often easier for pattern matching files\n",
    "\n",
    "# --- LLM Configuration ---\n",
    "# This script can be configured to use Azure OpenAI or adapted for Ollama (or other LLMs).\n",
    "#\n",
    "# --- 1. Azure OpenAI Configuration (Default) ---\n",
    "# To use Azure OpenAI, set the following as environment variables in your system,\n",
    "# or replace the placeholder strings in this script with your actual values.\n",
    "#\n",
    "# - AZURE_OPENAI_ENDPOINT: Your Azure OpenAI resource endpoint.\n",
    "#   (e.g., \"https://your-resource-name.openai.azure.com/\")\n",
    "# - AZURE_OPENAI_API_KEY: Your Azure OpenAI API key.\n",
    "# - AZURE_OPENAI_DEPLOYMENT_NAME: The name of your model deployment in Azure OpenAI Studio.\n",
    "#   This will be used as the 'model' in API calls (e.g., \"GPT4.1\", \"gpt-4-turbo\").\n",
    "# - AZURE_OPENAI_API_VERSION: The API version for Azure OpenAI.\n",
    "#   The original script used \"2024-12-01-preview\".\n",
    "\n",
    "AZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\", \"YOUR_AZURE_OPENAI_ENDPOINT_HERE\")\n",
    "AZURE_OPENAI_API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\", \"YOUR_AZURE_OPENAI_API_KEY_HERE\")\n",
    "AZURE_OPENAI_DEPLOYMENT_NAME = os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\", \"YOUR_AZURE_DEPLOYMENT_NAME_HERE\")\n",
    "AZURE_OPENAI_API_VERSION = os.getenv(\"AZURE_OPENAI_API_VERSION\", \"2024-12-01-preview\") # Original API version\n",
    "\n",
    "# --- 2. Ollama Configuration (Alternative - Requires Code Modification) ---\n",
    "# To use Ollama (e.g., running locally via `ollama serve`):\n",
    "#\n",
    "# a) Define your Ollama settings (can also be environment variables):\n",
    "#    OLLAMA_BASE_URL = os.getenv(\"OLLAMA_BASE_URL\", \"http://localhost:11434\")\n",
    "#    OLLAMA_MODEL = os.getenv(\"OLLAMA_MODEL\", \"your-ollama-model-name\") # e.g., \"llama3\", \"mistral\"\n",
    "#\n",
    "# b) Modify the 'LLM Client Initialization' section below.\n",
    "#    You'll need to instantiate an Ollama client or set up for direct HTTP requests.\n",
    "#    Example using the 'ollama' Python library (`pip install ollama`):\n",
    "#    --------------------------------------------------------------------\n",
    "#    # import ollama\n",
    "#    # try:\n",
    "#    #     client = ollama.Client(host=OLLAMA_BASE_URL)\n",
    "#    #     # Test connection (optional): client.list()\n",
    "#    #     print(f\"Ollama client initialized for model: {OLLAMA_MODEL} at {OLLAMA_BASE_URL}\")\n",
    "#    #     LLM_PROVIDER = \"Ollama\" # Set a flag\n",
    "#    # except Exception as e:\n",
    "#    #     print(f\"Error initializing Ollama client: {e}\")\n",
    "#    #     client = None\n",
    "#    #     LLM_PROVIDER = None\n",
    "#    --------------------------------------------------------------------\n",
    "#\n",
    "# c) Adapt the `generate_synthetic_record` function:\n",
    "#    - The call `client.chat.completions.create(...)` is specific to the OpenAI library.\n",
    "#      Replace it with the Ollama client's method for chat completions\n",
    "#      (e.g., `client.chat(model=OLLAMA_MODEL, messages=[...])`).\n",
    "#    - The `model` parameter should use `OLLAMA_MODEL`.\n",
    "#    - The structure of `messages` might need adjustment.\n",
    "#    - Parsing the response (e.g., `response.choices[0].message.content`) will change\n",
    "#      to match Ollama's response format (e.g., `response['message']['content']`).\n",
    "\n",
    "# --- LLM Client Initialization ---\n",
    "# By default, this script attempts to initialize the Azure OpenAI client.\n",
    "# Modify this section if you are using Ollama or another LLM provider.\n",
    "\n",
    "client = None\n",
    "LLM_PROVIDER = None # Can be 'Azure', 'Ollama', etc.\n",
    "\n",
    "# Attempt Azure OpenAI Initialization if not using placeholders\n",
    "if not (AZURE_OPENAI_ENDPOINT == \"YOUR_AZURE_OPENAI_ENDPOINT_HERE\" or \\\n",
    "    AZURE_OPENAI_API_KEY == \"YOUR_AZURE_OPENAI_API_KEY_HERE\" or \\\n",
    "    AZURE_OPENAI_DEPLOYMENT_NAME == \"YOUR_AZURE_DEPLOYMENT_NAME_HERE\"):\n",
    "    try:\n",
    "    print(\"Attempting to initialize Azure OpenAI client...\")\n",
    "    client = AzureOpenAI(\n",
    "        azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "        api_key=AZURE_OPENAI_API_KEY,\n",
    "        api_version=AZURE_OPENAI_API_VERSION\n",
    "    )\n",
    "    print(f\"Azure OpenAI client initialized. Using deployment: '{AZURE_OPENAI_DEPLOYMENT_NAME}'.\")\n",
    "    LLM_PROVIDER = \"Azure\"\n",
    "    except Exception as e:\n",
    "    print(f\"Error initializing Azure OpenAI client: {e}\")\n",
    "    print(\"Please ensure your Azure OpenAI ENDPOINT, API_KEY, DEPLOYMENT_NAME, and API_VERSION are correctly set as environment variables or in the script.\")\n",
    "    client = None # Ensure client is None if initialization fails\n",
    "else:\n",
    "    print(\"Azure OpenAI credentials are set to placeholder values or not fully provided.\")\n",
    "    print(\"If you intend to use Azure OpenAI, please set the following environment variables or update their values directly in the script:\")\n",
    "    print(\"  - AZURE_OPENAI_ENDPOINT\")\n",
    "    print(\"  - AZURE_OPENAI_API_KEY\")\n",
    "    print(\"  - AZURE_OPENAI_DEPLOYMENT_NAME\")\n",
    "    print(\"  - AZURE_OPENAI_API_VERSION (currently defaults to '2024-12-01-preview')\")\n",
    "    print(\"The script will proceed, but API calls will fail if Azure OpenAI is the intended provider and is not configured.\")\n",
    "\n",
    "# IMPORTANT FOR USERS:\n",
    "# The `generate_synthetic_record` function currently uses `client.chat.completions.create`\n",
    "# and `model=AZURE_OPENAI_DEPLOYMENT_NAME`, which are specific to Azure OpenAI.\n",
    "# If you switch to Ollama or another LLM, you MUST adapt that function accordingly.\n",
    "\n",
    "# --- Path Configurations ---\n",
    "# It's good practice to make paths configurable, e.g., via environment variables.\n",
    "DEFAULT_PSEUDO_MD_PATH = r\".\\Lagerugpijn\\LR_EPDs\" # Original path\n",
    "PSEUDO_MD_DIRECTORY_PATH = os.getenv(\"PSEUDO_MD_DIRECTORY_PATH\", DEFAULT_PSEUDO_MD_PATH)\n",
    "print(f\"Using pseudonymized examples from: {PSEUDO_MD_DIRECTORY_PATH}\")\n",
    "\n",
    "# Specify the base directory for synthetic data output (derived from input or configurable)\n",
    "# SYNTHETIC_OUTPUT_BASE_DIR = os.getenv(\"SYNTHETIC_OUTPUT_BASE_DIR\", os.path.dirname(PSEUDO_MD_DIRECTORY_PATH))\n",
    "SYNTHETIC_OUTPUT_BASE_DIR = os.path.dirname(PSEUDO_MD_DIRECTORY_PATH) # As per original logic\n",
    "\n",
    "# Specify the output directory for the synthetic files\n",
    "SYNTHETIC_OUTPUT_DIR_NAME = \"synthetic_epds\" # Name of the subfolder for synthetic EPDs\n",
    "SYNTHETIC_OUTPUT_DIR = os.path.join(SYNTHETIC_OUTPUT_BASE_DIR, SYNTHETIC_OUTPUT_DIR_NAME)\n",
    "# Ensure this path is created later if it doesn't exist (os.makedirs in save function)\n",
    "print(f\"Synthetic data will be saved to: {SYNTHETIC_OUTPUT_DIR}\")\n",
    "\n",
    "# Configure how many synthetic records to generate\n",
    "# NUM_SYNTHETIC_RECORDS_TO_GENERATE = int(os.getenv(\"NUM_SYNTHETIC_RECORDS\", \"20\")) # Example: make configurable\n",
    "NUM_SYNTHETIC_RECORDS_TO_GENERATE = 20 # As per original script\n",
    "\n",
    "# --- Helper Function to Load Pseudonymized Examples ---\n",
    "# (load_pseudonymized_examples function definition remains here)\n",
    "# ...\n",
    "\n",
    "# --- Function to Generate a Single Synthetic Record ---\n",
    "# (generate_synthetic_record function definition remains here)\n",
    "# REMINDER: This function needs adaptation if not using Azure OpenAI.\n",
    "# Specifically, the client call and model parameter.\n",
    "# ...\n",
    "\n",
    "# --- Function to Save a Single Synthetic Record ---\n",
    "# (save_synthetic_record function definition remains here)\n",
    "# ...\n",
    "\n",
    "# --- Main Execution Logic for Synthetic Data Generation ---\n",
    "# (if __name__ == \"__main__\": block remains here)\n",
    "# ...\n",
    "import fitz  # PyMuPDF\n",
    "from openai import AzureOpenAI\n",
    "import glob # Using glob is often easier for pattern matching files\n",
    "\n",
    "\n",
    "AZURE_OPENAI_ENDPOINT = \"https://xxxxxxx.openai.azure.com/\" # Your Azure OpenAI Endpoint\n",
    "AZURE_OPENAI_API_KEY = \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\" # Your Azure OpenAI API Key\n",
    "AZURE_OPENAI_DEPLOYMENT_NAME = \"GPT4.1\" # The name of your GPT-4 deployment in Azure OpenAI Studio\n",
    "API_VERSION = \"2024-12-01-preview\" \n",
    "\n",
    "# --- Azure OpenAI Client Initialization (Remains the same) ---\n",
    "client = AzureOpenAI(\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "    api_key=AZURE_OPENAI_API_KEY,\n",
    "    api_version=API_VERSION\n",
    ")\n",
    "\n",
    "# Use the directory containing the pseudonymized Markdown files as input examples\n",
    "# This should be the same directory where the pseudo_*.md files were saved\n",
    "# PSEUDO_MD_DIRECTORY_PATH = r\".\\Dataset\\Lagerugpijn\\LR_EPDs\" # Reuse the input directory from GIVEN\n",
    "\n",
    "# Specify the base directory for synthetic data output (e.g., parent of the input dir)\n",
    "SYNTHETIC_OUTPUT_BASE_DIR = os.path.dirname(PSEUDO_MD_DIRECTORY_PATH)\n",
    "\n",
    "# Specify the output directory for the synthetic files\n",
    "SYNTHETIC_OUTPUT_DIR = os.path.join(SYNTHETIC_OUTPUT_BASE_DIR, \"synthetic_epds\")\n",
    "\n",
    "# Configure how many synthetic records to generate\n",
    "NUM_SYNTHETIC_RECORDS_TO_GENERATE = 20 # Example: Generate 20 synthetic records\n",
    "\n",
    "# --- Helper Function to Load Pseudonymized Examples ---\n",
    "def load_pseudonymized_examples(directory_path):\n",
    "    \"\"\"\n",
    "    Loads content from pseudonymized markdown files in a directory.\n",
    "    Adds clear separators to help the AI distinguish examples.\n",
    "    \"\"\"\n",
    "    # Look for files starting with 'pseudo_' and ending with '.md'\n",
    "    example_files = glob.glob(os.path.join(directory_path, \"pseudo_*.md\"))\n",
    "    example_content = []\n",
    "\n",
    "    if not example_files:\n",
    "        print(f\"Warning: No pseudonymized example files found in '{directory_path}'. \"\n",
    "              f\"Cannot load examples for synthetic data generation. \"\n",
    "              f\"Generation will proceed without specific structure/style examples.\")\n",
    "        return \"\"\n",
    "\n",
    "    print(f\"Loading {len(example_files)} pseudonymized example files from '{directory_path}'...\")\n",
    "    for file_path in example_files:\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "                # Add a clear separator and header for each example\n",
    "                example_content.append(f\"\\n--- BEGIN VOORBEELD DOSSIER: {os.path.basename(file_path)} ---\\n{content.strip()}\\n--- EINDE VOORBEELD DOSSIER ---\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading example file {file_path}: {e}\")\n",
    "\n",
    "    return \"\\n\".join(example_content)\n",
    "\n",
    "\n",
    "# --- Function to Generate a Single Synthetic Record ---\n",
    "def generate_synthetic_record(client, example_markdown_content, record_number):\n",
    "    \"\"\"\n",
    "    Generates a single synthetic Dutch physiotherapeutic EHR record\n",
    "    using Azure OpenAI, guided by the prompts and examples.\n",
    "    \"\"\"\n",
    "\n",
    "    # System prompt based on the Worker's core role\n",
    "    system_prompt = \"\"\"Je bent een fysiotherapeut die realistische synthetische Nederlandse patiëntdossiers (EHR) genereert op basis van geanonimiseerde intake-informatie en expertbegeleiding. Je past het International Classification of Functioning (ICF) kader toe en volgt de KNGF klinische richtlijn voor lage rugpijn. Produceer uitsluitend het gevraagde patiëntdossier.\"\"\"\n",
    "\n",
    "    # User prompt incorporating instructions from both Supervisor and Worker prompts\n",
    "    user_prompt = f\"\"\"Genereer EEN compleet en realistisch synthetisch fysiotherapeutisch patiëntdossier in het Nederlands, uitsluitend voor een patiënt met **acute, subacute of chronische lage rugpijn**. Genereer **geen** dossiers voor andere klachten.\n",
    "\n",
    "Het dossier moet de volgende onderdelen bevatten, in deze volgorde en met de volgende specificaties:\n",
    "\n",
    "1.  **Samenvatting anamnese:** Een bondige, verhalende samenvatting van de patiëntgeschiedenis (klachtengeschiedenis, symptoomontwikkeling), functionele impact, coping en relevante context (werk, stressoren, eerdere episodes). Schrijf dit in natuurlijke, professionele Nederlandse klinische taal. Geef duidelijk aan of het gaat om acute (<6 weken), subacute (6-12 weken) of chronische (>12 weken) lage rugpijn.\n",
    "2.  **ICF-gebaseerde diagnose:** Een volledige ICF-diagnose met de volgende componenten:\n",
    "    * Stoornissen in functies (bijv. pijn, stijfheid, verminderde mobiliteit, spierzwakte)\n",
    "    * Beperkingen in activiteiten (bijv. moeite met zitten, tillen, bukken, lopen, traplopen)\n",
    "    * Beperkingen in participatie (bijv. problemen met werk, sport, hobby's, sociale activiteiten)\n",
    "    * Persoonlijke factoren (bijv. leeftijd, copingstijl, overtuigingen, conditie)\n",
    "    * Omgevingsfactoren (bijv. werkomgeving, sociale steun, fysieke omgeving)\n",
    "    * Risico- en prognostische factoren (bijv. gele vlaggen, rode vlaggen (indien van toepassing en realistisch), duur van de klachten, eerdere episodes)\n",
    "    * **Herformulering van de hulpvraag** van de patiënt.\n",
    "3.  **Behandeldoelen:** Formuleer **SMART, patiëntgerichte, functionele doelen**. Beschrijf specifiek **wat de patiënt weer wil kunnen doen**. Klinimetrische scores (zoals PSK, NRS, ODI) mogen worden genoemd als *ondersteuning* of *meetbaar criterium* voor het doel (bijv. \"PSK van 70 naar ≤14 om weer te kunnen tuinieren\"), maar de score-reductie is niet het doel zelf. Geef aan *wanneer* het doel bereikt moet zijn.\n",
    "4.  **Behandelplan:** Beschrijf de voorgestelde interventies (bijv. manuele therapie, oefentherapie, motorische controle training, educatie, graded activity, leefstijladvies, pijneducatie) en de rationale hierachter. Baseer dit plan op de KNGF richtlijn lage rugpijn en de gestelde doelen.\n",
    "5.  **SOEP voortgangsnotities:** Schrijf **minimaal 3 en maximaal 8 afzonderlijke voortgangsnotities**, elk voor een individuele behandelsessie. Gebruik het **volledige SOEP-formaat** (Subjectief, Objectief, Evaluatie, Plan) voor elke notitie. Toon progressie, eventuele stagnatie, terugval, aanpassing van het plan en klinische besluitvorming over de sessies heen. Varieer realistisch in de frequentie en het aantal sessies tussen de 3 en 8.\n",
    "6.  **Taal en stijl:** Het hele dossier moet geschreven zijn in professioneel, natuurlijk Nederlands, zoals gebruikt door Nederlandse fysiotherapeuten. Breid gangbare afkortingen en klinische shorthand uit (zoals PSK, LWK, 3d xt li). Hanteer een realistische en gevarieerde toon en structuur die aansluit bij de voorbeelden.\n",
    "\n",
    "Hieronder staan voorbeelden van gepseudonimiseerde patiëntdossiers. Gebruik deze voorbeelden als referentie voor de verwachte structuur, stijl, taalgebruik en het detailniveau, maar genereer een **compleet nieuw en uniek patiëntgeval** met een eigen anamnese, diagnose, doelen en een realistisch, variabel verloop van de behandeling over meerdere sessies.\n",
    "\n",
    "{example_markdown_content}\n",
    "\n",
    "Genereer nu **uitsluitend** het nieuwe patiëntdossier hieronder, beginnend met de anamnese samenvatting en eindigend met 'FINISH'. Zorg ervoor dat het dossier **alle** hierboven gevraagde onderdelen bevat en voldoet aan **alle** instructies, inclusief het vereiste aantal SOEP-notities en de focus op lage rugpijn.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    print(f\"  Generating synthetic record {record_number}...\")\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=AZURE_OPENAI_DEPLOYMENT_NAME, # Your deployment name for generation\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            temperature=0.8, # Higher temperature for creativity and variation\n",
    "            max_tokens=8000 # Sufficient tokens for a full record with multiple notes\n",
    "        )\n",
    "        synthetic_output = response.choices[0].message.content\n",
    "        print(f\"  Synthetic record {record_number} generation successful.\")\n",
    "        return synthetic_output\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error calling Azure OpenAI API for synthetic record {record_number}: {e}\")\n",
    "        # Consider adding a small delay or retry logic here for robustness\n",
    "        # time.sleep(5) # Example delay\n",
    "        return None\n",
    "\n",
    "# --- Function to Save a Single Synthetic Record ---\n",
    "def save_synthetic_record(synthetic_content, output_dir, record_number):\n",
    "    \"\"\"Saves a single synthetic record content string to a specified file.\"\"\"\n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    output_path = os.path.join(output_dir, f\"synthetic_patient_{record_number:03d}.md\") # Use padding for sorting\n",
    "\n",
    "    try:\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(synthetic_content)\n",
    "        print(f\"  Saved synthetic record {record_number} to: {output_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing synthetic record file {output_path}: {e}\")\n",
    "\n",
    "\n",
    "# --- Main Execution Logic for Synthetic Data Generation ---\n",
    "if __name__ == \"__main__\":\n",
    "    # --- Assuming the previous script's main block finished and the client is available ---\n",
    "\n",
    "    print(\"\\n--- Starting Synthetic Data Generation ---\")\n",
    "\n",
    "    # Check if the pseudonymized examples directory exists\n",
    "    if not os.path.isdir(PSEUDO_MD_DIRECTORY_PATH):\n",
    "        print(f\"Error: Pseudonymized examples directory not found at '{PSEUDO_MD_DIRECTORY_PATH}'. \"\n",
    "              f\"Please ensure the previous script ran successfully and generated pseudo_*.md files.\")\n",
    "        exit()\n",
    "\n",
    "    # Load pseudonymized example content\n",
    "    # This content will be included in the prompt to provide context/style examples\n",
    "    example_content = load_pseudonymized_examples(PSEUDO_MD_DIRECTORY_PATH)\n",
    "\n",
    "    # Proceed even if no examples were loaded, but warn the user\n",
    "    if not example_content:\n",
    "         print(\"Continuing synthetic data generation without examples. The AI will rely solely on the prompts.\")\n",
    "\n",
    "\n",
    "    print(f\"\\nGenerating {NUM_SYNTHETIC_RECORDS_TO_GENERATE} synthetic records in '{SYNTHETIC_OUTPUT_DIR}'.\")\n",
    "\n",
    "    # Loop to generate the specified number of synthetic records\n",
    "    for i in range(NUM_SYNTHETIC_RECORDS_TO_GENERATE):\n",
    "        record_index = i + 1\n",
    "        print(f\"\\n--- Generating Synthetic Record {record_index} of {NUM_SYNTHETIC_RECORDS_TO_GENERATE} ---\")\n",
    "\n",
    "        # Generate the synthetic record using the AI\n",
    "        synthetic_record_content = generate_synthetic_record(client, example_content, record_index)\n",
    "\n",
    "        if synthetic_record_content:\n",
    "            # Optional: Basic validation (check for \"FINISH\" marker)\n",
    "            if synthetic_record_content.strip().endswith(\"FINISH\"):\n",
    "                 # Remove the FINISH marker from the saved file if desired, or keep it.\n",
    "                 # Let's keep it for now as per prompt instruction.\n",
    "                 pass # synthetic_record_content = synthetic_record_content.strip()[:-len(\"FINISH\")].strip()\n",
    "            else:\n",
    "                 print(f\"Warning: Generated record {record_index} does not end with 'FINISH'. Content might be incomplete or malformed.\")\n",
    "\n",
    "\n",
    "            # Save the generated record to a file\n",
    "            save_synthetic_record(synthetic_record_content, SYNTHETIC_OUTPUT_DIR, record_index)\n",
    "        else:\n",
    "            print(f\"Skipping save for synthetic record {record_index} due to generation failure.\")\n",
    "\n",
    "        # Optional: Add a small delay between generation calls to avoid hitting rate limits\n",
    "        # time.sleep(1) # Example: 1 second delay\n",
    "\n",
    "    print(\"\\nSynthetic data generation complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azure-openai-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
