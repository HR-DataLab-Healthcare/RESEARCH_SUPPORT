{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e466d56",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "0c55bff5",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "pip install pdfminer.six\n",
    "pip install langdetect\n",
    "=============================>\n",
    "pip install langdetect zlib pdfminer.six pdfplumber numpy\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b7ba6c09",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "==============================================================================================\n",
    "==============================================================================================\n",
    "==============================================================================================\n",
    "==============================================================================================\n",
    "==============================================================================================\n",
    "==============================================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ecf8a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:37: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:37: SyntaxWarning: invalid escape sequence '\\.'\n",
      "C:\\Users\\PROMET02\\AppData\\Local\\Temp\\ipykernel_24344\\2333497156.py:37: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  warnings.filterwarnings(\"ignore\", module='pdfminer\\..*')\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\PROMET02\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing PDF files in directory: D:\\Dataset\\Lagerugpijn\\LR_EPDs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- Pandas DataFrame Summary ---\n",
      "   Filename  Size (MB)  Word Count  Unique Words  Doc Length (Chars) Language  Tables  Figures  Annotations  Char Entropy  Word Entropy   Avg PMI   JS Dist\n",
      "0  ...59037   0.200723        1766           453               12081       nl       3        0            0      4.889979      8.153941  6.968305  0.256047\n",
      "1  ...59684   0.186488        1589           543               11178       nl       3        0            0      4.895385      8.541276  6.937004  0.235412\n",
      "2  ...60038   0.171431        1452           493               10095       nl       3        0            0      4.846837      8.389399  6.711273  0.197682\n",
      "3  ...60384   0.167994        1667           472               10761       nl       2        0            0      4.827849      8.138346  6.517369  0.190041\n",
      "4  ...60818   0.171666        1344           474                9412       nl       3        0            0      4.926047      8.349449  6.273874  0.240325\n",
      "DataFrame shape: (13, 13)\n",
      "\n",
      "\n",
      "--- Numerical Data DataFrame Summary ---\n",
      "   Size (MB)  Word Count  Unique Words  Doc Length (Chars)  Char Entropy  Word Entropy   Avg PMI   JS Dist\n",
      "0   0.200723        1766           453               12081      4.889979      8.153941  6.968305  0.256047\n",
      "1   0.186488        1589           543               11178      4.895385      8.541276  6.937004  0.235412\n",
      "2   0.171431        1452           493               10095      4.846837      8.389399  6.711273  0.197682\n",
      "3   0.167994        1667           472               10761      4.827849      8.138346  6.517369  0.190041\n",
      "4   0.171666        1344           474                9412      4.926047      8.349449  6.273874  0.240325\n",
      "Numerical DataFrame shape: (13, 8)\n",
      "Size (MB)             float64\n",
      "Word Count              int64\n",
      "Unique Words            int64\n",
      "Doc Length (Chars)      int64\n",
      "Char Entropy          float64\n",
      "Word Entropy          float64\n",
      "Avg PMI               float64\n",
      "JS Dist               float64\n",
      "dtype: object\n",
      "\n",
      "Numerical data saved to PDF_numerical_stats.csv\n",
      "\n",
      "============================== PDF Analysis Report ==============================\n",
      "\n",
      "*Note: Markdown rendering may vary depending on the viewer. Multi-column layout and font size control are not standard Markdown features.*\n",
      "\n",
      "\n",
      "## Per-File Analysis Results\n",
      "\n",
      "| Filename | Size (MB) | Word Count | Unique Words | doc length | Language | Tables | Figures | Annotations | Char Entropy | Word Entropy | Avg PMI | JS Dist |\n",
      "|---|---|---|---|---|---|---|---|---|---|---|---|---|\n",
      "| ...59037 | 0.20 | 1766 | 453 | 12081 | nl | 3 | 0 | 0 | 4.89 | 8.15 | 6.9683 | 0.2560 |\n",
      "| ...59684 | 0.19 | 1589 | 543 | 11178 | nl | 3 | 0 | 0 | 4.90 | 8.54 | 6.9370 | 0.2354 |\n",
      "| ...60038 | 0.17 | 1452 | 493 | 10095 | nl | 3 | 0 | 0 | 4.85 | 8.39 | 6.7113 | 0.1977 |\n",
      "| ...60384 | 0.17 | 1667 | 472 | 10761 | nl | 2 | 0 | 0 | 4.83 | 8.14 | 6.5174 | 0.1900 |\n",
      "| ...60818 | 0.17 | 1344 | 474 | 9412 | nl | 3 | 0 | 0 | 4.93 | 8.35 | 6.2739 | 0.2403 |\n",
      "| ...61014 | 0.20 | 1605 | 507 | 11118 | nl | 4 | 0 | 0 | 4.88 | 8.43 | 7.1602 | 0.2260 |\n",
      "| ...61368 | 0.19 | 1888 | 532 | 12109 | nl | 3 | 0 | 0 | 4.89 | 8.32 | 6.7119 | 0.1923 |\n",
      "| ...61665 | 0.16 | 1554 | 446 | 10287 | nl | 2 | 0 | 0 | 4.81 | 8.03 | 6.2304 | 0.2113 |\n",
      "| ...61810 | 0.19 | 2176 | 651 | 14554 | nl | 2 | 0 | 0 | 4.83 | 8.49 | 6.5144 | 0.1927 |\n",
      "| ...62117 | 0.16 | 1360 | 427 | 8873 | nl | 3 | 0 | 0 | 4.87 | 8.08 | 6.2708 | 0.1999 |\n",
      "| ...62177 | 0.17 | 1367 | 480 | 9544 | nl | 3 | 0 | 0 | 4.97 | 8.39 | 6.7884 | 0.2311 |\n",
      "| ...62655 | 0.16 | 1581 | 454 | 10316 | nl | 2 | 0 | 0 | 4.79 | 8.04 | 6.3726 | 0.2073 |\n",
      "| ...63175 | 0.15 | 1566 | 501 | 10358 | nl | 2 | 0 | 0 | 4.82 | 8.26 | 6.2312 | 0.1991 |\n",
      "\n",
      "## Overall Dataset Summary\n",
      "\n",
      "### 1. Number of PDF Files: 13\n",
      "\n",
      "### 2. Storage Size\n",
      "- **Total:** 2.28 MB (2395567 bytes)\n",
      "- **Average:** 0.18 MB\n",
      "- **Range:** (0.15 MB, 0.20 MB)\n",
      "\n",
      "### 3. Textual Content Size\n",
      "- **Total words/tokens:** 20915\n",
      "- **Average words/tokens per document:** 1608.85\n",
      "- **Unique tokens across dataset (vocabulary size):** 1667\n",
      "\n",
      "### 4. Information Content (Estimated)\n",
      "- **Total estimated info content (based on total file size):** 19164536 bits\n",
      "- *Note: This estimate is based on raw file size in bits.*\n",
      "\n",
      "### 4b. Information Theory Metrics (Overall Dataset)\n",
      "- **Overall Character Entropy:** 4.88 bits/character\n",
      "- **Overall Word Entropy:** 9.16 bits/word\n",
      "- **Average Bigram PMI (Threshold=3):** 6.6699\n",
      "  *Note: Average PMI is calculated for bigrams appearing at least 3 times across the dataset.*\n",
      "  *Note: Per-file JSD is calculated against the overall dataset word distribution.*\n",
      "- **Overall Average Document Length (Characters):** 10822.00\n",
      "\n",
      "### 5. Document Length Distribution (in words/tokens)\n",
      "- **Mean length:** 1608.85\n",
      "- **Median length:** 1581\n",
      "- **Standard deviation:** 232.53\n",
      "\n",
      "### 6. Language Distribution\n",
      "- **Counts:**\n",
      "   - nl: 13 files\n",
      "\n",
      "### 7. Structural Elements Summary\n",
      "- **Total Tables found:** 35\n",
      "- **Total Figures/Images found:** 0\n",
      "- **Total Annotations found:** 0\n",
      "- **Files with Tables:** 13 (100.00%)\n",
      "- **Files with Figures/Images:** 0 (0.00%)\n",
      "- **Files with Annotations:** 0 (0.00%)\n",
      "\n",
      "===============================================================================\n",
      "\n",
      "Analysis report also saved to pdf_analysis_report.md\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import statistics\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import re # For tokenization\n",
    "from langdetect import detect, DetectorFactory # For language detection\n",
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "import pdfplumber # Added: For advanced PDF parsing\n",
    "from io import StringIO # To capture print output\n",
    "import string # For character entropy\n",
    "import warnings # Added: To manage warnings\n",
    "# Removed: from pdfminer.pdfparser import PDFSyntaxWarning # Removed: Specific import causing error\n",
    "from scipy.stats import entropy # Added: For KL divergence calculation\n",
    "\n",
    "import nltk\n",
    "# Ensure necessary NLTK resources are downloaded\n",
    "nltk.download('punkt_tab') \n",
    "from nltk.tokenize import word_tokenize # For word tokenization\n",
    "\n",
    "import pandas as pd # For DataFrame handling (if needed later)\n",
    "\n",
    "# Ensure consistent language detection results\n",
    "DetectorFactory.seed = 0\n",
    "\n",
    "# --- Configuration ---\n",
    "# Directory containing the PDF files\n",
    "pdf_directory = r'D:\\Dataset\\Lagerugpijn\\LR_EPDs' # Use a raw string for the path\n",
    "output_markdown_file = 'pdf_analysis_report.md' # Name of the output Markdown file\n",
    "PMI_BIGRAM_FREQ_THRESHOLD = 3 # Minimum frequency for a bigram to be included in Average PMI calculation\n",
    "# Removed: JSD_EPSILON = 1e-9 # Small value used in old JSD calculation\n",
    "\n",
    "# --- Suppress Warnings ---\n",
    "# Suppress warnings originating from any pdfminer module (including PDFSyntaxWarning)\n",
    "# This is a more general approach than filtering by a specific warning class name\n",
    "warnings.filterwarnings(\"ignore\", module='pdfminer\\..*')\n",
    "# You could add other filters here if other specific warnings are bothersable, e.g.,\n",
    "# warnings.filterwarnings(\"ignore\", message=\"some specific message\")\n",
    "\n",
    "# --- Data Storage ---\n",
    "file_data = [] # List to store dictionaries, each containing data for one file\n",
    "all_extracted_text_for_vocab = \"\" # String to accumulate all text for vocabulary analysis (for overall vocab and PMI)\n",
    "all_extracted_chars = \"\" # String to accumulate all characters (for overall char entropy)\n",
    "all_char_counts = [] # List to store character counts for each file (for overall average)\n",
    "\n",
    "report_output = StringIO() # Use StringIO to capture print statements\n",
    "\n",
    "# Redirect stdout to capture print output\n",
    "original_stdout = sys.stdout\n",
    "sys.stdout = report_output\n",
    "\n",
    "# --- Helper Function for Text Extraction and Structural Element Detection ---\n",
    "def analyze_pdf_content(pdf_path):\n",
    "    \"\"\"\n",
    "    Extracts text and detects structural elements (tables, figures, annotations)\n",
    "    from a PDF file using pdfplumber.\n",
    "    Returns extracted text, table count, figure count, annotation count.\n",
    "    \"\"\"\n",
    "    extracted_text = \"\"\n",
    "    table_count = 0\n",
    "    figure_count = 0\n",
    "    annotation_count = 0\n",
    "\n",
    "    try:\n",
    "        # Use warnings.catch_warnings to temporarily manage warnings within this function if needed,\n",
    "        # but filtering globally at the start is simpler for this case.\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for page in pdf.pages:\n",
    "                # Extract text\n",
    "                extracted_text += page.extract_text() or \"\" # Add text, handle None\n",
    "\n",
    "                # Detect tables\n",
    "                tables = page.extract_tables()\n",
    "                table_count += len(tables)\n",
    "\n",
    "                # Detect figures (images)\n",
    "                figure_count += len(page.images)\n",
    "\n",
    "                # Detect annotations\n",
    "                annotation_count += len(page.annots)\n",
    "\n",
    "    except pdfplumber.PDFSyntaxError as e:\n",
    "         # Print warnings directly to original stdout so they appear during processing\n",
    "         original_stdout.write(f\"Warning: PDF Syntax Error in {os.path.basename(pdf_path)}: {e}\\n\")\n",
    "         # Return empty data for this file if there's a syntax error\n",
    "         return \"\", 0, 0, 0\n",
    "    except Exception as e:\n",
    "        # Print warnings directly to original stdout\n",
    "        original_stdout.write(f\"Warning: Error processing {os.path.basename(pdf_path)} with pdfplumber: {e}\\n\")\n",
    "        # Return empty data for this file if there's any other error\n",
    "        return \"\", 0, 0, 0\n",
    "\n",
    "    return extracted_text, table_count, figure_count, annotation_count\n",
    "\n",
    "\n",
    "# --- Helper Function for Language Detection ---\n",
    "def detect_language(text):\n",
    "    \"\"\"Detects the language of the input text.\"\"\"\n",
    "    if not text.strip():\n",
    "        return \"N/A (No text)\"\n",
    "    try:\n",
    "        # langdetect works best on larger text samples.\n",
    "        # Use a sample if the text is very long to speed things up,\n",
    "        # but for typical documents, using the full text is fine.\n",
    "        # Limit text length for detection to avoid potential issues with very large texts\n",
    "        sample_text = text[:5000] if len(text) > 5000 else text\n",
    "        if not sample_text.strip():\n",
    "             return \"N/A (No text in sample)\"\n",
    "        # Use warnings.catch_warnings to temporarily manage warnings within this function if needed\n",
    "        with warnings.catch_warnings():\n",
    "             # langdetect can sometimes issue warnings, e.g., about language probabilities\n",
    "             warnings.filterwarnings(\"ignore\", category=UserWarning) # Example: suppress UserWarnings from langdetect\n",
    "             return detect(sample_text)\n",
    "    except LangDetectException:\n",
    "        return \"Undetected\"\n",
    "    except Exception as e:\n",
    "        # Print warnings directly to original stdout\n",
    "        original_stdout.write(f\"Warning: Error during language detection: {e}\\n\")\n",
    "        return \"Error\"\n",
    "\n",
    "# --- Helper Functions for Information Theory Metrics ---\n",
    "\n",
    "def calculate_shannon_entropy(items):\n",
    "    \"\"\"Calculates Shannon entropy for a list of items (chars or words).\"\"\"\n",
    "    if not items:\n",
    "        return 0.0\n",
    "    counts = Counter(items)\n",
    "    total_items = len(items)\n",
    "    entropy = 0.0\n",
    "    for count in counts.values():\n",
    "        probability = count / total_items\n",
    "        # Add a small epsilon to probability to avoid log(0) if needed, though standard formula handles p > 0\n",
    "        if probability > 0:\n",
    "             entropy -= probability * math.log2(probability)\n",
    "    return entropy\n",
    "\n",
    "# --- Helper Function for canonical Divergence and JSD Calculation ---\n",
    "def calculate_ca_divergence(text1, text2, unit='word', smoothing=1e-9, base=2):\n",
    "    \"\"\"\n",
    "    Calculates the canonical Jensen-Shannon Divergence (JSD) between two texts.\n",
    "    Uses NLTK's word_tokenize for robust word-level tokenization.\n",
    "    \n",
    "    Args:\n",
    "        text1, text2: Input strings to compare.\n",
    "        unit: 'word' (default, recommended) or 'char' for character-level comparison.\n",
    "        smoothing: Small value to avoid zero probabilities.\n",
    "        base: Logarithm base for entropy (2 = bits, e = nats).\n",
    "    Returns:\n",
    "        jsd: Jensen-Shannon divergence value (0 = identical, up to 1 for maximally different, when base=2).\n",
    "    \"\"\"\n",
    "    if not text1 or not text2:\n",
    "        return np.nan\n",
    "\n",
    "    # Tokenize\n",
    "    if unit == 'char':\n",
    "        tokens1 = list(text1)\n",
    "        tokens2 = list(text2)\n",
    "    elif unit == 'word':\n",
    "        tokens1 = word_tokenize(text1)\n",
    "        tokens2 = word_tokenize(text2)\n",
    "    else:\n",
    "        raise ValueError(\"unit must be 'char' or 'word'\")\n",
    "\n",
    "    if not tokens1 or not tokens2:\n",
    "        return np.nan\n",
    "\n",
    "    # Build combined vocabulary\n",
    "    vocab = list(set(tokens1 + tokens2))\n",
    "    counts1 = Counter(tokens1)\n",
    "    counts2 = Counter(tokens2)\n",
    "\n",
    "    # Create probability distributions with smoothing\n",
    "    p1 = np.array([counts1.get(token, 0) + smoothing for token in vocab], dtype=np.float64)\n",
    "    p2 = np.array([counts2.get(token, 0) + smoothing for token in vocab], dtype=np.float64)\n",
    "    p1 /= p1.sum()\n",
    "    p2 /= p2.sum()\n",
    "\n",
    "    # Canonical JSD: KL(P||M) + KL(Q||M), M = 0.5*(P+Q)\n",
    "    M = 0.5 * (p1 + p2)\n",
    "    jsd = 0.5 * entropy(p1, M, base=base) + 0.5 * entropy(p2, M, base=base)\n",
    "    return jsd\n",
    "\n",
    "# --- Helper Function for Bigram Calculation ---\n",
    "def calculate_avg_bigram_pmi(text, min_freq=3):\n",
    "    \"\"\"\n",
    "    Calculates the average Pointwise Mutual Information (PMI) for word bigrams\n",
    "    that occur at least min_freq times.\n",
    "    A proxy metric related to Mutual Information, measuring word association strength.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return 0.0\n",
    "\n",
    "    # Simple word tokenization and lowercase\n",
    "    # Use the same tokenization as the main script for consistency\n",
    "    words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "    if len(words) < 2:\n",
    "        return 0.0\n",
    "\n",
    "    word_counts = Counter(words)\n",
    "    bigram_counts = Counter(zip(words[:-1], words[1:])) # Count occurrences of bigrams\n",
    "\n",
    "    total_words = len(words)\n",
    "    # total_bigrams = len(list(zip(words[:-1], words[1:]))) # Count actual bigram instances\n",
    "\n",
    "    pmi_values = []\n",
    "    for bigram, bigram_count in bigram_counts.items():\n",
    "        # Only consider bigrams that meet the minimum frequency threshold\n",
    "        if bigram_count >= min_freq:\n",
    "            word1, word2 = bigram\n",
    "\n",
    "            # Calculate probabilities (using total_words for marginals is common)\n",
    "            p_w1 = word_counts[word1] / total_words if total_words > 0 else 0\n",
    "            p_w2 = word_counts[word2] / total_words if total_words > 0 else 0\n",
    "            # Use total words as normalization for bigram probability as well for PMI formula\n",
    "            p_w1_w2 = bigram_count / total_words if total_words > 0 else 0\n",
    "\n",
    "            # Avoid log(0) - check if probabilities are positive\n",
    "            if p_w1 > 0 and p_w2 > 0 and p_w1_w2 > 0:\n",
    "                 # PMI formula: log2( P(w1,w2) / (P(w1) * P(w2)) )\n",
    "                 pmi = math.log2(p_w1_w2 / (p_w1 * p_w2))\n",
    "                 pmi_values.append(pmi)\n",
    "            # Note: Bigrams that never appear together with positive marginals would have PMI -infinity.\n",
    "            # We only average over bigrams that *do* appear (with >= min_freq).\n",
    "\n",
    "    if not pmi_values:\n",
    "        return 0.0 # Return 0 if no bigrams meet min_freq or text was empty/too short\n",
    "\n",
    "    return np.mean(pmi_values)\n",
    "\n",
    "\n",
    "# --- Analysis ---\n",
    "# Print initial message to original stdout\n",
    "original_stdout.write(f\"Analyzing PDF files in directory: {pdf_directory}\\n\")\n",
    "\n",
    "\n",
    "if not os.path.isdir(pdf_directory):\n",
    "    # Print error to original stdout\n",
    "    original_stdout.write(f\"Error: Directory not found at {pdf_directory}\\n\")\n",
    "else:\n",
    "    # Iterate through all entries in the directory\n",
    "    for entry_name in os.listdir(pdf_directory):\n",
    "        entry_path = os.path.join(pdf_directory, entry_name)\n",
    "\n",
    "        # Check if the entry is a file and ends with .pdf (case-insensitive)\n",
    "        if os.path.isfile(entry_path) and entry_name.lower().endswith('.pdf'):\n",
    "\n",
    "            file_info = {} # Dictionary to store data for the current file\n",
    "            file_info['filename'] = entry_name\n",
    "            file_info['filepath'] = entry_path\n",
    "\n",
    "            # 1. & 2. Storage Size\n",
    "            try:\n",
    "                file_size_bytes = os.path.getsize(entry_path) # size in bytes\n",
    "                file_info['storage_size_bytes'] = file_size_bytes\n",
    "                file_info['storage_size_mb'] = file_size_bytes / (1024 * 1024)\n",
    "            except Exception as e:\n",
    "                # Print warnings directly to original stdout\n",
    "                original_stdout.write(f\"Warning: Could not get size for {entry_name}: {e}\\n\")\n",
    "                file_info['storage_size_bytes'] = 0\n",
    "                file_info['storage_size_mb'] = 0\n",
    "\n",
    "\n",
    "            # 3. Textual Content Size & Extraction + 7. Structural Elements\n",
    "            text, table_count, figure_count, annotation_count = analyze_pdf_content(entry_path)\n",
    "\n",
    "            file_info['extracted_text'] = text # Store text for language\n",
    "            # Calculate character count for the current file\n",
    "            file_info['char_count'] = len(text)\n",
    "            all_char_counts.append(file_info['char_count']) # Add to list for overall average\n",
    "\n",
    "            # Simple word tokenization and lowercase\n",
    "            tokens = re.findall(r'\\b\\w+\\b', text.lower()) # Convert to lower case for vocabulary size\n",
    "            word_count = len(tokens)\n",
    "            file_info['word_count'] = word_count\n",
    "            file_info['tokens'] = tokens # Store tokens for JSD calculation\n",
    "\n",
    "\n",
    "            # Calculate unique words for the current file\n",
    "            unique_words_in_file = set(tokens)\n",
    "            file_info['unique_word_count'] = len(unique_words_in_file)\n",
    "\n",
    "            # Calculate per-file Shannon Entropy\n",
    "            file_info['char_entropy'] = calculate_shannon_entropy(list(text)) # Character entropy\n",
    "            file_info['word_entropy'] = calculate_shannon_entropy(tokens) # Word entropy\n",
    "\n",
    "            # Calculate per-file Average Bigram PMI\n",
    "            file_info['average_pmi'] = calculate_avg_bigram_pmi(text, min_freq=PMI_BIGRAM_FREQ_THRESHOLD)\n",
    "\n",
    "\n",
    "            # Accumulate text and characters for overall vocabulary, PMI, and char entropy later\n",
    "            all_extracted_text_for_vocab += text + \" \" # Add a space to ensure separation\n",
    "            all_extracted_chars += text # Accumulate all characters\n",
    "\n",
    "\n",
    "            # Store structural element counts\n",
    "            file_info['table_count'] = table_count\n",
    "            file_info['figure_count'] = figure_count\n",
    "            file_info['annotation_count'] = annotation_count\n",
    "            # Add boolean flags for easy checking\n",
    "            file_info['has_tables'] = table_count > 0\n",
    "            file_info['has_figures'] = figure_count > 0\n",
    "            file_info['has_annotations'] = annotation_count > 0\n",
    "\n",
    "\n",
    "            # 6. Language Detection\n",
    "            file_info['language'] = detect_language(text)\n",
    "\n",
    "            # 4. Information Content (Estimated)\n",
    "            # Simple estimation based on file size in bits\n",
    "            file_info['estimated_info_content_bits_filesize'] = file_size_bytes * 8\n",
    "\n",
    "\n",
    "            file_data.append(file_info) # Add the file's data to the list\n",
    "\n",
    "\n",
    "    # --- Calculate Overall Dataset Metrics (needed for per-file JSD) ---\n",
    "    overall_char_entropy = calculate_shannon_entropy(list(all_extracted_chars))\n",
    "\n",
    "    overall_tokens = re.findall(r'\\b\\w+\\b', all_extracted_text_for_vocab.lower())\n",
    "    overall_word_counts = Counter(overall_tokens)\n",
    "    total_words_overall = len(overall_tokens)\n",
    "    overall_word_distribution = {word: count / total_words_overall for word, count in overall_word_counts.items()} if total_words_overall > 0 else {}\n",
    "    overall_vocabulary = set(overall_tokens)\n",
    "\n",
    "\n",
    "    overall_word_entropy = calculate_shannon_entropy(overall_tokens)\n",
    "\n",
    "    # Calculate Average Bigram PMI for the overall dataset\n",
    "    overall_average_pmi = calculate_avg_bigram_pmi(all_extracted_text_for_vocab, min_freq=PMI_BIGRAM_FREQ_THRESHOLD)\n",
    "\n",
    "    # Calculate Overall Average Document Length (Characters)\n",
    "    overall_avg_doc_length_chars = np.mean(all_char_counts) if all_char_counts else 0\n",
    "\n",
    "    # --- Calculate Per-File JSD (compared to overall dataset distribution) ---\n",
    "    if file_data and overall_word_distribution:\n",
    "        for file_info in file_data:\n",
    "            file_tokens = file_info['tokens']\n",
    "            file_word_counts = Counter(file_tokens)\n",
    "            total_file_words = len(file_tokens)\n",
    "            file_word_distribution = {word: count / total_file_words for word, count in file_word_counts.items()} if total_file_words > 0 else {}\n",
    "\n",
    "            # Calculate JSD between file distribution and overall distribution\n",
    "            # Use the provided calculate_ca_divergence function for JSD\n",
    "            jsd_word = calculate_ca_divergence(file_info['extracted_text'], all_extracted_text_for_vocab, unit='word', smoothing=1e-9, base=2)\n",
    "            file_info['js_dist'] = jsd_word\n",
    "            # Note: JSD is symmetric, so we can use either order of texts\n",
    "    else:\n",
    "        # Set JSD to NaN if no data or no overall distribution (consistent with calculate_kl_divergence)\n",
    "        for file_info in file_data:\n",
    "             file_info['js_dist'] = np.nan\n",
    "\n",
    "    # --- Create Pandas DataFrame from file_data ---\n",
    "    if file_data:\n",
    "        # Create a list of dictionaries for the DataFrame, applying transformations\n",
    "        df_data_list = []\n",
    "        for item in file_data:\n",
    "            # Apply display_filename logic\n",
    "            digits_in_filename = re.findall(r'\\d+', item['filename'])\n",
    "            last_five_digits = \"\".join(digits_in_filename)[-5:] if digits_in_filename else \"\"\n",
    "            display_name = last_five_digits\n",
    "            if len(item['filename'].replace('.', '').replace('_', '').replace('-', '').replace(' ', '')) > len(last_five_digits):\n",
    "                display_name = \"...\" + display_name\n",
    "            \n",
    "            df_data_list.append({\n",
    "                'Filename': display_name,\n",
    "                'Size (MB)': item['storage_size_mb'],\n",
    "                'Word Count': item['word_count'],\n",
    "                'Unique Words': item['unique_word_count'],\n",
    "                'Doc Length (Chars)': item['char_count'], # Renamed for clarity vs. word count\n",
    "                'Language': item['language'],\n",
    "                'Tables': item['table_count'],\n",
    "                'Figures': item['figure_count'],\n",
    "                'Annotations': item['annotation_count'],\n",
    "                'Char Entropy': item['char_entropy'],\n",
    "                'Word Entropy': item['word_entropy'],\n",
    "                'Avg PMI': item['average_pmi'],\n",
    "                'JS Dist': item['js_dist']\n",
    "            })\n",
    "\n",
    "        df = pd.DataFrame(df_data_list)\n",
    "\n",
    "\n",
    "\n",
    "        # Optional: Display the DataFrame (or parts of it) to the original console\n",
    "        original_stdout.write(\"\\n\\n--- Pandas DataFrame Summary ---\\n\")\n",
    "        with pd.option_context('display.max_rows', None, 'display.max_columns', None, 'display.width', 1000):\n",
    "            original_stdout.write(str(df.head()) + \"\\n\") # Print head to console\n",
    "        original_stdout.write(f\"DataFrame shape: {df.shape}\\n\")\n",
    "        # Note: If you want to save the DataFrame to a CSV or Excel file, you can do so here\n",
    "\n",
    "        # --- Create numerical_data DataFrame ---\n",
    "        # Columns based on the df DataFrame:\n",
    "        # 'Size (MB)', 'Word Count', 'Unique Words', 'Doc Length (Chars)', \n",
    "        # 'Char Entropy', 'Word Entropy', 'Avg PMI', 'JS Dist'\n",
    "        numerical_column_names = [\n",
    "            'Size (MB)', 'Word Count', 'Unique Words', 'Doc Length (Chars)',\n",
    "            'Char Entropy', 'Word Entropy', 'Avg PMI', 'JS Dist'\n",
    "        ]\n",
    "        numerical_data = df[numerical_column_names].copy() # Use .copy() to avoid SettingWithCopyWarning\n",
    "\n",
    "        original_stdout.write(\"\\n\\n--- Numerical Data DataFrame Summary ---\\n\")\n",
    "        with pd.option_context('display.max_rows', None, 'display.max_columns', None, 'display.width', 1000):\n",
    "            original_stdout.write(str(numerical_data.head()) + \"\\n\")\n",
    "        original_stdout.write(f\"Numerical DataFrame shape: {numerical_data.shape}\\n\")\n",
    "        original_stdout.write(str(numerical_data.dtypes) + \"\\n\")\n",
    "\n",
    "        # --- Save numerical_data to CSV ---\n",
    "        try:\n",
    "            numerical_data_csv_file = 'PDF_numerical_stats.csv'\n",
    "            numerical_data.to_csv(numerical_data_csv_file, index=False)\n",
    "            original_stdout.write(f\"\\nNumerical data saved to {numerical_data_csv_file}\\n\")\n",
    "        except Exception as e:\n",
    "            original_stdout.write(f\"\\nError saving numerical data to CSV: {e}\\n\")\n",
    "\n",
    "    else:\n",
    "        original_stdout.write(\"\\n\\n--- Pandas DataFrame Summary ---\\n\")\n",
    "        original_stdout.write(\"No data to create DataFrame.\\n\")\n",
    "    # --- End of Data Collection ---\n",
    "\n",
    "    # --- Captured Output for Markdown Report ---\n",
    "    # Print Markdown report header\n",
    "    # --- Per-File Reporting (Captured for Markdown) ---\n",
    "    print(\"\\n## Per-File Analysis Results\") # Markdown Heading\n",
    "    if file_data:\n",
    "        # Create a Markdown table for per-file data\n",
    "        # Added 'JS Dist' column header\n",
    "        print(\"\\n| Filename | Size (MB) | Word Count | Unique Words | doc length | Language | Tables | Figures | Annotations | Char Entropy | Word Entropy | Avg PMI | JS Dist |\")\n",
    "        # Updated separator line to match the new column count\n",
    "        print(\"|---|---|---|---|---|---|---|---|---|---|---|---|---|\")\n",
    "        for file_info in file_data:\n",
    "            # Extract only the digits from the filename\n",
    "            digits_in_filename = re.findall(r'\\d+', file_info['filename'])\n",
    "            last_five_digits = \"\".join(digits_in_filename)[-5:] if digits_in_filename else \"\"\n",
    "\n",
    "            # Determine display filename\n",
    "            display_filename = last_five_digits\n",
    "            # Only add \"...\" if there were other characters besides the last 5 digits\n",
    "            if len(file_info['filename'].replace('.', '').replace('_', '').replace('-', '').replace(' ', '')) > len(last_five_digits):\n",
    "                 display_filename = \"...\" + display_filename\n",
    "\n",
    "            # Format JSD value, handle NaN\n",
    "            js_dist_formatted = f\"{file_info['js_dist']:.4f}\" if not np.isnan(file_info['js_dist']) else \"NaN\"\n",
    "\n",
    "            # Added file_info['js_dist'] to the row\n",
    "            print(f\"| {display_filename} | {file_info['storage_size_mb']:.2f} | {file_info['word_count']} | {file_info['unique_word_count']} | {file_info['char_count']} | {file_info['language']} | {file_info['table_count']} | {file_info['figure_count']} | {file_info['annotation_count']} | {file_info['char_entropy']:.2f} | {file_info['word_entropy']:.2f} | {file_info['average_pmi']:.4f} | {js_dist_formatted} |\")\n",
    "\n",
    "    else:\n",
    "        print(\"No PDF files found in the specified directory.\")\n",
    "\n",
    "\n",
    "    # --- Overall Dataset Summary (Captured for Markdown) ---\n",
    "    print(\"\\n## Overall Dataset Summary\") # Markdown Heading\n",
    "\n",
    "    total_files = len(file_data)\n",
    "    print(f\"\\n### 1. Number of PDF Files: {total_files}\") # Markdown Subheading\n",
    "\n",
    "    if total_files > 0:\n",
    "        # 2. Storage Size\n",
    "        all_storage_sizes_bytes = [f['storage_size_bytes'] for f in file_data]\n",
    "        total_storage_size_bytes = sum(all_storage_sizes_bytes)\n",
    "        total_storage_size_mb = total_storage_size_bytes / (1024 * 1024)\n",
    "        print(f\"\\n### 2. Storage Size\") # Markdown Subheading\n",
    "        print(f\"- **Total:** {total_storage_size_mb:.2f} MB ({total_storage_size_bytes} bytes)\")\n",
    "        if all_storage_sizes_bytes:\n",
    "            avg_file_size_mb = statistics.mean(all_storage_sizes_bytes) / (1024 * 1024)\n",
    "            min_file_size_mb = min(all_storage_sizes_bytes) / (1024 * 1024)\n",
    "            max_file_size_mb = max(all_storage_sizes_bytes) / (1024 * 1024)\n",
    "            print(f\"- **Average:** {avg_file_size_mb:.2f} MB\")\n",
    "            print(f\"- **Range:** ({min_file_size_mb:.2f} MB, {max_file_size_mb:.2f} MB)\")\n",
    "\n",
    "        # 3. Textual Content Size\n",
    "        all_word_counts = [f['word_count'] for f in file_data]\n",
    "        total_word_count = sum(all_word_counts)\n",
    "        print(f\"\\n### 3. Textual Content Size\") # Markdown Subheading\n",
    "        print(f\"- **Total words/tokens:** {total_word_count}\")\n",
    "        if all_word_counts:\n",
    "            avg_word_count = statistics.mean(all_word_counts)\n",
    "            print(f\"- **Average words/tokens per document:** {avg_word_count:.2f}\")\n",
    "\n",
    "            # Calculate unique tokens (vocabulary size) from accumulated text\n",
    "            all_tokens = re.findall(r'\\b\\w+\\b', all_extracted_text_for_vocab.lower())\n",
    "            unique_tokens = set(all_tokens)\n",
    "            vocabulary_size = len(unique_tokens)\n",
    "            print(f\"- **Unique tokens across dataset (vocabulary size):** {vocabulary_size}\") # Clarified label\n",
    "        else:\n",
    "             print(f\"- **Average words/tokens per document:** 0\")\n",
    "             print(f\"- **Unique tokens across dataset (vocabulary size):** 0\")\n",
    "\n",
    "        # 4. Information Content (Estimated)\n",
    "        total_estimated_info_content_bits_filesize = total_storage_size_bytes * 8\n",
    "        print(f\"\\n### 4. Information Content (Estimated)\") # Markdown Subheading\n",
    "        print(f\"- **Total estimated info content (based on total file size):** {total_estimated_info_content_bits_filesize} bits\")\n",
    "        print(f\"- *Note: This estimate is based on raw file size in bits.*\")\n",
    "\n",
    "        # 4b. Information Theory Metrics (Overall Dataset)\n",
    "        print(f\"\\n### 4b. Information Theory Metrics (Overall Dataset)\") # Markdown Subheading\n",
    "        print(f\"- **Overall Character Entropy:** {overall_char_entropy:.2f} bits/character\")\n",
    "        print(f\"- **Overall Word Entropy:** {overall_word_entropy:.2f} bits/word\")\n",
    "        print(f\"- **Average Bigram PMI (Threshold={PMI_BIGRAM_FREQ_THRESHOLD}):** {overall_average_pmi:.4f}\")\n",
    "        print(f\"  *Note: Average PMI is calculated for bigrams appearing at least {PMI_BIGRAM_FREQ_THRESHOLD} times across the dataset.*\")\n",
    "        print(f\"  *Note: Per-file JSD is calculated against the overall dataset word distribution.*\")\n",
    "\n",
    "\n",
    "        # Added Overall Average Document Length (Characters)\n",
    "        print(f\"- **Overall Average Document Length (Characters):** {overall_avg_doc_length_chars:.2f}\")\n",
    "\n",
    "\n",
    "        # 5. Document Length Distribution\n",
    "        print(f\"\\n### 5. Document Length Distribution (in words/tokens)\") # Markdown Subheading\n",
    "        if all_word_counts:\n",
    "            mean_len = statistics.mean(all_word_counts)\n",
    "            median_len = statistics.median(all_word_counts)\n",
    "            # Ensure std dev calculation is valid for >1 data points\n",
    "            std_dev_len = statistics.stdev(all_word_counts) if len(all_word_counts) > 1 else 0\n",
    "            print(f\"- **Mean length:** {mean_len:.2f}\")\n",
    "            print(f\"- **Median length:** {median_len}\")\n",
    "            print(f\"- **Standard deviation:** {std_dev_len:.2f}\")\n",
    "\n",
    "\n",
    "        # 6. Language Distribution\n",
    "        print(f\"\\n### 6. Language Distribution\") # Markdown Subheading\n",
    "        all_languages = [f['language'] for f in file_data]\n",
    "        language_counts = Counter(all_languages)\n",
    "        print(\"- **Counts:**\")\n",
    "        for lang, count in language_counts.most_common():\n",
    "            print(f\"   - {lang}: {count} files\")\n",
    "\n",
    "\n",
    "        # 7. Structural Elements Summary\n",
    "        print(f\"\\n### 7. Structural Elements Summary\") # Markdown Subheading\n",
    "        total_tables_found = sum([f['table_count'] for f in file_data])\n",
    "        total_figures_found = sum([f['figure_count'] for f in file_data])\n",
    "        total_annotations_found = sum([f['annotation_count'] for f in file_data])\n",
    "\n",
    "        files_with_tables = sum([f['has_tables'] for f in file_data])\n",
    "        files_with_figures = sum([f['has_figures'] for f in file_data])\n",
    "        files_with_annotations = sum([f['has_annotations'] for f in file_data])\n",
    "\n",
    "        print(f\"- **Total Tables found:** {total_tables_found}\")\n",
    "        print(f\"- **Total Figures/Images found:** {total_figures_found}\")\n",
    "        print(f\"- **Total Annotations found:** {total_annotations_found}\")\n",
    "        print(f\"- **Files with Tables:** {files_with_tables} ({files_with_tables/total_files*100:.2f}%)\")\n",
    "        print(f\"- **Files with Figures/Images:** {files_with_figures} ({files_with_figures/total_files*100:.2f}%)\")\n",
    "        print(f\"- **Files with Annotations:** {files_with_annotations} ({files_with_annotations/total_files*100:.2f}%)\")\n",
    "\n",
    "\n",
    "    else:\n",
    "        print(\"No data collected from PDF files.\")\n",
    "\n",
    "\n",
    "# --- End of Captured Output ---\n",
    "sys.stdout = original_stdout # Restore stdout\n",
    "sys.stdout.flush() # Explicitly flush the buffer\n",
    "\n",
    "\n",
    "# --- Display Report in Console ---\n",
    "markdown_content = report_output.getvalue() # Get the captured output\n",
    "print(\"\\n\" + \"=\"*30 + \" PDF Analysis Report \" + \"=\"*30) # Separator\n",
    "print(\"\\n*Note: Markdown rendering may vary depending on the viewer. Multi-column layout and font size control are not standard Markdown features.*\\n\")\n",
    "print(markdown_content) # Print the captured report content to console\n",
    "print(\"=\"*79) # Separator\n",
    "\n",
    "\n",
    "# --- Save Output to Markdown File ---\n",
    "try:\n",
    "    with open(output_markdown_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(markdown_content)\n",
    "    print(f\"\\nAnalysis report also saved to {output_markdown_file}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nError saving report to {output_markdown_file}: {e}\")\n",
    "\n",
    "report_output.close() # Close the StringIO object\n",
    "# --- End of Script ---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azure-openai-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
