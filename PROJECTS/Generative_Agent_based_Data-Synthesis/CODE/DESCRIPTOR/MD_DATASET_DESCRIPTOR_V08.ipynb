{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e466d56",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "0c55bff5",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "pip install pdfminer.six\n",
    "pip install langdetect\n",
    "=============================>\n",
    "pip install langdetect zlib pdfminer.six pdfplumber numpy\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b7ba6c09",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "==============================================================================================\n",
    "==============================================================================================\n",
    "==============================================================================================\n",
    "==============================================================================================\n",
    "==============================================================================================\n",
    "==============================================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecf8a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:41: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:41: SyntaxWarning: invalid escape sequence '\\.'\n",
      "C:\\Users\\PROMET02\\AppData\\Local\\Temp\\ipykernel_62572\\2337180538.py:41: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  warnings.filterwarnings(\"ignore\", module='pdfminer\\..*')\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\PROMET02\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing Markdown files in directory: D:\\Dataset\\Lagerugpijn\\pseudonymized-epds\n",
      "\n",
      "\n",
      "--- Pandas DataFrame Summary ---\n",
      "   Filename  Size (MB)  Word Count  Unique Words  Doc Length (Chars) Language  Tables  Figures  Annotations  Char Entropy  Word Entropy   Avg PMI   JS Dist\n",
      "0  ...59037   0.014140        1836           484               14386       nl       0        0            0      4.916925      8.207334  6.930547  0.169945\n",
      "1  ...59684   0.013328        1657           569               13533       nl       0        0            0      4.914231      8.583490  6.922197  0.150735\n",
      "2  ...60038   0.012178        1512           525               12388       nl       0        0            0      4.835315      8.450317  6.661283  0.128479\n",
      "3  ...60384   0.013181        1717           497               13375       nl       0        0            0      4.812887      8.202289  6.542843  0.121215\n",
      "4  ...60818   0.011514        1408           500               11675       nl       0        0            0      4.903854      8.409912  6.331947  0.153860\n",
      "DataFrame shape: (13, 13)\n",
      "\n",
      "\n",
      "--- Numerical Data DataFrame Summary ---\n",
      "   Size (MB)  Word Count  Unique Words  Doc Length (Chars)  Char Entropy  Word Entropy   Avg PMI   JS Dist\n",
      "0   0.014140        1836           484               14386      4.916925      8.207334  6.930547  0.169945\n",
      "1   0.013328        1657           569               13533      4.914231      8.583490  6.922197  0.150735\n",
      "2   0.012178        1512           525               12388      4.835315      8.450317  6.661283  0.128479\n",
      "3   0.013181        1717           497               13375      4.812887      8.202289  6.542843  0.121215\n",
      "4   0.011514        1408           500               11675      4.903854      8.409912  6.331947  0.153860\n",
      "Numerical DataFrame shape: (13, 8)\n",
      "Size (MB)             float64\n",
      "Word Count              int64\n",
      "Unique Words            int64\n",
      "Doc Length (Chars)      int64\n",
      "Char Entropy          float64\n",
      "Word Entropy          float64\n",
      "Avg PMI               float64\n",
      "JS Dist               float64\n",
      "dtype: object\n",
      "\n",
      "Numerical data saved to pseudo_MD_numerical_stats.csv\n",
      "\n",
      "============================== MD File Analysis Report ==============================\n",
      "\n",
      "*Note: Markdown rendering may vary depending on the viewer.*\n",
      "\n",
      "\n",
      "## Per-File Analysis Results\n",
      "\n",
      "| Filename | Size (MB) | Word Count | Unique Words | doc length | Language | Tables | Figures | Annotations | Char Entropy | Word Entropy | Avg PMI | JS Dist |\n",
      "|---|---|---|---|---|---|---|---|---|---|---|---|---|\n",
      "| ...59037 | 0.01 | 1836 | 484 | 14386 | nl | 0 | 0 | 0 | 4.92 | 8.21 | 6.9305 | 0.1699 |\n",
      "| ...59684 | 0.01 | 1657 | 569 | 13533 | nl | 0 | 0 | 0 | 4.91 | 8.58 | 6.9222 | 0.1507 |\n",
      "| ...60038 | 0.01 | 1512 | 525 | 12388 | nl | 0 | 0 | 0 | 4.84 | 8.45 | 6.6613 | 0.1285 |\n",
      "| ...60384 | 0.01 | 1717 | 497 | 13375 | nl | 0 | 0 | 0 | 4.81 | 8.20 | 6.5428 | 0.1212 |\n",
      "| ...60818 | 0.01 | 1408 | 500 | 11675 | nl | 0 | 0 | 0 | 4.90 | 8.41 | 6.3319 | 0.1539 |\n",
      "| ...61014 | 0.01 | 1667 | 539 | 14283 | nl | 0 | 0 | 0 | 4.86 | 8.49 | 7.1609 | 0.1404 |\n",
      "| ...61368 | 0.01 | 1944 | 559 | 14984 | nl | 0 | 0 | 0 | 4.85 | 8.38 | 6.6850 | 0.1253 |\n",
      "| ...61665 | 0.01 | 1603 | 474 | 12678 | nl | 0 | 0 | 0 | 4.79 | 8.10 | 6.2565 | 0.1401 |\n",
      "| ...61810 | 0.02 | 2241 | 676 | 17648 | nl | 0 | 0 | 0 | 4.79 | 8.51 | 6.4934 | 0.1348 |\n",
      "| ...62117 | 0.01 | 1411 | 453 | 11242 | nl | 0 | 0 | 0 | 4.83 | 8.14 | 6.3147 | 0.1279 |\n",
      "| ...62177 | 0.01 | 1433 | 511 | 12413 | nl | 1 | 0 | 0 | 4.86 | 8.44 | 6.7774 | 0.1457 |\n",
      "| ...62655 | 0.01 | 1634 | 485 | 12624 | nl | 0 | 0 | 0 | 4.80 | 8.12 | 6.4240 | 0.1355 |\n",
      "| ...63175 | 0.01 | 1621 | 531 | 12583 | nl | 0 | 0 | 0 | 4.82 | 8.32 | 6.2663 | 0.1320 |\n",
      "\n",
      "## Overall Dataset Summary\n",
      "\n",
      "### 1. Number of MD Files: 13\n",
      "\n",
      "### 2. Storage Size\n",
      "- **Total:** 0.17 MB (179547 bytes)\n",
      "- **Average:** 0.01 MB\n",
      "- **Range:** (0.01 MB, 0.02 MB)\n",
      "\n",
      "### 3. Textual Content Size\n",
      "- **Total words/tokens:** 21684\n",
      "- **Average words/tokens per document:** 1668.00\n",
      "- **Unique tokens across dataset (vocabulary size):** 1642\n",
      "\n",
      "### 4. Information Content (Estimated)\n",
      "- **Total estimated info content (based on total file size):** 1436376 bits\n",
      "- *Note: This estimate is based on raw file size in bits.*\n",
      "\n",
      "### 4b. Information Theory Metrics (Overall Dataset)\n",
      "- **Overall Character Entropy:** 4.86 bits/character\n",
      "- **Overall Word Entropy:** 9.18 bits/word\n",
      "- **Average Bigram PMI (Threshold=3):** 6.7342\n",
      "  *Note: Average PMI is calculated for bigrams appearing at least 3 times across the dataset.*\n",
      "  *Note: Per-file JSD is calculated against the overall dataset word distribution.*\n",
      "- **Overall Average Document Length (Characters):** 13370.15\n",
      "\n",
      "### 5. Document Length Distribution (in words/tokens)\n",
      "- **Mean length:** 1668.00\n",
      "- **Median length:** 1634\n",
      "- **Standard deviation:** 233.82\n",
      "\n",
      "### 6. Language Distribution\n",
      "- **Counts:**\n",
      "   - nl: 13 files\n",
      "\n",
      "### 7. Structural Elements Summary\n",
      "- **Total Tables found (heuristic):** 1\n",
      "- **Total Figures/Images found:** 0\n",
      "- **Total Annotations found:** 0\n",
      "- **Files with Tables (heuristic):** 1 (7.69% if total_files > 0 else 0.00%)\n",
      "- **Files with Figures/Images:** 0 (0.00% if total_files > 0 else 0.00%)\n",
      "- **Files with Annotations:** 0 (0.00% if total_files > 0 else 0.00%)\n",
      "\n",
      "===============================================================================\n",
      "\n",
      "Analysis report also saved to pseudo_md_analysis_report.md\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import statistics\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import re # For tokenization\n",
    "from langdetect import detect, DetectorFactory # For language detection\n",
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "# import pdfplumber # Commented out: No longer used for MD file processing\n",
    "from io import StringIO # To capture print output\n",
    "import string # For character entropy\n",
    "import warnings # Added: To manage warnings\n",
    "# Removed: from pdfminer.pdfparser import PDFSyntaxWarning # Removed: Specific import causing error\n",
    "from scipy.stats import entropy # Added: For KL divergence calculation\n",
    "\n",
    "import nltk\n",
    "# Ensure necessary NLTK resources are downloaded\n",
    "nltk.download('punkt') # Corrected from 'punkt_tab'\n",
    "from nltk.tokenize import word_tokenize # For word tokenization\n",
    "\n",
    "import pandas as pd # For DataFrame handling (if needed later)\n",
    "\n",
    "# Ensure consistent language detection results\n",
    "DetectorFactory.seed = 0\n",
    "\n",
    "# --- Configuration ---\n",
    "# Directory containing the MD files\n",
    "########### source_directory = r'D:\\Dataset\\Lagerugpijn\\LR_EPDs' # Use a raw string for the path, renamed from pdf_directory\n",
    "source_directory = r'D:\\Dataset\\Lagerugpijn\\synthetic_epds_ORG'\n",
    "output_markdown_file = 'synth_md_analysis_report.md' \n",
    "numerical_data_csv_file = 'synth_MD_numerical_stats.csv'\n",
    "#source_directory = r'D:\\Dataset\\Lagerugpijn\\pseudonymized-epds' # Updated to point to the correct directory for MD files\n",
    "#output_markdown_file = 'pseudo_md_analysis_report.md' # Name of the output Markdown file, changed from pdf_analysis_report.md\n",
    "#numerical_data_csv_file = 'pseudo_MD_numerical_stats.csv'\n",
    "\n",
    "PMI_BIGRAM_FREQ_THRESHOLD = 3 # Minimum frequency for a bigram to be included in Average PMI calculation\n",
    "\n",
    "# --- Suppress Warnings ---\n",
    "# Suppress warnings originating from any pdfminer module (still potentially relevant if other PDF tools are used indirectly or in future)\n",
    "warnings.filterwarnings(\"ignore\", module='pdfminer\\..*')\n",
    "# You could add other filters here if other specific warnings are bothersable, e.g.,\n",
    "# warnings.filterwarnings(\"ignore\", message=\"some specific message\")\n",
    "\n",
    "# --- Data Storage ---\n",
    "file_data = [] # List to store dictionaries, each containing data for one file\n",
    "all_extracted_text_for_vocab = \"\" # String to accumulate all text for vocabulary analysis (for overall vocab and PMI)\n",
    "all_extracted_chars = \"\" # String to accumulate all characters (for overall char entropy)\n",
    "all_char_counts = [] # List to store character counts for each file (for overall average)\n",
    "\n",
    "report_output = StringIO() # Use StringIO to capture print statements\n",
    "\n",
    "# Redirect stdout to capture print output\n",
    "original_stdout = sys.stdout\n",
    "sys.stdout = report_output\n",
    "\n",
    "# --- Helper Function for MD Text Extraction and Basic Structural Element Detection ---\n",
    "def analyze_md_content(md_path):\n",
    "    \"\"\"\n",
    "    Extracts text from a Markdown file and performs basic detection of figures and tables.\n",
    "    Returns extracted text, table count, figure count, and 0 for annotation count.\n",
    "    \"\"\"\n",
    "    extracted_text = \"\"\n",
    "    table_count = 0\n",
    "    figure_count = 0\n",
    "    annotation_count = 0 # Markdown files don't have a direct equivalent to PDF annotations\n",
    "\n",
    "    try:\n",
    "        with open(md_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            extracted_text = f.read()\n",
    "\n",
    "        # Count figures (Markdown image syntax: ![alt text](image_url))\n",
    "        figure_count = len(re.findall(r'!\\[.*?\\]\\(.*?\\)', extracted_text))\n",
    "\n",
    "        # Count tables: A heuristic looking for Markdown table header separator patterns.\n",
    "        # This counts lines that look like |---|---| or similar.\n",
    "        # This is a rough estimate of the number of tables.\n",
    "        lines = extracted_text.splitlines()\n",
    "        for line in lines:\n",
    "            s_line = line.strip()\n",
    "            # Heuristic for Markdown table separator line:\n",
    "            # 1. Contains '---'\n",
    "            # 2. Contains '|'\n",
    "            # 3. Consists primarily of '-', '|', ':', and whitespace.\n",
    "            if '---' in s_line and '|' in s_line:\n",
    "                is_separator_candidate = True\n",
    "                for char_in_line in s_line:\n",
    "                    if char_in_line not in ['-', '|', ':', ' ', '\\t']: # Allow tabs as well\n",
    "                        is_separator_candidate = False\n",
    "                        break\n",
    "                if is_separator_candidate:\n",
    "                    # Avoid counting simple horizontal rules like \"---\" if they somehow pass\n",
    "                    if not re.fullmatch(r'-{3,}', s_line.replace('|','').replace(':','').strip()):\n",
    "                        table_count += 1\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        original_stdout.write(f\"Warning: File not found: {os.path.basename(md_path)}\\n\")\n",
    "        return \"\", 0, 0, 0 # Return empty data for this file\n",
    "    except Exception as e:\n",
    "        original_stdout.write(f\"Warning: Error reading or processing MD file {os.path.basename(md_path)}: {e}\\n\")\n",
    "        return \"\", 0, 0, 0 # Return empty data for this file\n",
    "\n",
    "    return extracted_text, table_count, figure_count, annotation_count\n",
    "\n",
    "\n",
    "# --- Helper Function for Language Detection ---\n",
    "def detect_language(text):\n",
    "    \"\"\"Detects the language of the input text.\"\"\"\n",
    "    if not text.strip():\n",
    "        return \"N/A (No text)\"\n",
    "    try:\n",
    "        sample_text = text[:5000] if len(text) > 5000 else text\n",
    "        if not sample_text.strip():\n",
    "             return \"N/A (No text in sample)\"\n",
    "        with warnings.catch_warnings():\n",
    "             warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "             return detect(sample_text)\n",
    "    except LangDetectException:\n",
    "        return \"Undetected\"\n",
    "    except Exception as e:\n",
    "        original_stdout.write(f\"Warning: Error during language detection: {e}\\n\")\n",
    "        return \"Error\"\n",
    "\n",
    "# --- Helper Functions for Information Theory Metrics ---\n",
    "def calculate_shannon_entropy(items):\n",
    "    \"\"\"Calculates Shannon entropy for a list of items (chars or words).\"\"\"\n",
    "    if not items:\n",
    "        return 0.0\n",
    "    counts = Counter(items)\n",
    "    total_items = len(items)\n",
    "    entropy_val = 0.0 # Renamed to avoid conflict with scipy.stats.entropy\n",
    "    for count in counts.values():\n",
    "        probability = count / total_items\n",
    "        if probability > 0:\n",
    "             entropy_val -= probability * math.log2(probability)\n",
    "    return entropy_val\n",
    "\n",
    "# --- Helper Function for canonical Divergence and JSD Calculation ---\n",
    "def calculate_ca_divergence(text1, text2, unit='word', smoothing=1e-9, base=2):\n",
    "    \"\"\"\n",
    "    Calculates the canonical Jensen-Shannon Divergence (JSD) between two texts.\n",
    "    Uses NLTK's word_tokenize for robust word-level tokenization.\n",
    "    \"\"\"\n",
    "    if not text1 or not text2:\n",
    "        return np.nan\n",
    "\n",
    "    if unit == 'char':\n",
    "        tokens1 = list(text1)\n",
    "        tokens2 = list(text2)\n",
    "    elif unit == 'word':\n",
    "        tokens1 = word_tokenize(text1)\n",
    "        tokens2 = word_tokenize(text2)\n",
    "    else:\n",
    "        raise ValueError(\"unit must be 'char' or 'word'\")\n",
    "\n",
    "    if not tokens1 or not tokens2:\n",
    "        return np.nan\n",
    "\n",
    "    vocab = list(set(tokens1 + tokens2))\n",
    "    counts1 = Counter(tokens1)\n",
    "    counts2 = Counter(tokens2)\n",
    "\n",
    "    p1 = np.array([counts1.get(token, 0) + smoothing for token in vocab], dtype=np.float64)\n",
    "    p2 = np.array([counts2.get(token, 0) + smoothing for token in vocab], dtype=np.float64)\n",
    "    p1 /= p1.sum()\n",
    "    p2 /= p2.sum()\n",
    "\n",
    "    M = 0.5 * (p1 + p2)\n",
    "    # Using scipy.stats.entropy for KL divergence part of JSD\n",
    "    jsd = 0.5 * entropy(p1, M, base=base) + 0.5 * entropy(p2, M, base=base)\n",
    "    return jsd\n",
    "\n",
    "# --- Helper Function for Bigram Calculation ---\n",
    "def calculate_avg_bigram_pmi(text, min_freq=3):\n",
    "    \"\"\"\n",
    "    Calculates the average Pointwise Mutual Information (PMI) for word bigrams\n",
    "    that occur at least min_freq times.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return 0.0\n",
    "    words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "    if len(words) < 2:\n",
    "        return 0.0\n",
    "\n",
    "    word_counts = Counter(words)\n",
    "    bigram_counts = Counter(zip(words[:-1], words[1:]))\n",
    "\n",
    "    total_words = len(words)\n",
    "    pmi_values = []\n",
    "    for bigram, bigram_count in bigram_counts.items():\n",
    "        if bigram_count >= min_freq:\n",
    "            word1, word2 = bigram\n",
    "            p_w1 = word_counts[word1] / total_words if total_words > 0 else 0\n",
    "            p_w2 = word_counts[word2] / total_words if total_words > 0 else 0\n",
    "            p_w1_w2 = bigram_count / total_words if total_words > 0 else 0\n",
    "            if p_w1 > 0 and p_w2 > 0 and p_w1_w2 > 0:\n",
    "                 pmi = math.log2(p_w1_w2 / (p_w1 * p_w2))\n",
    "                 pmi_values.append(pmi)\n",
    "    if not pmi_values:\n",
    "        return 0.0\n",
    "    return np.mean(pmi_values)\n",
    "\n",
    "\n",
    "# --- Analysis ---\n",
    "original_stdout.write(f\"Analyzing Markdown files in directory: {source_directory}\\n\") # Updated message\n",
    "\n",
    "if not os.path.isdir(source_directory):\n",
    "    original_stdout.write(f\"Error: Directory not found at {source_directory}\\n\")\n",
    "else:\n",
    "    for entry_name in os.listdir(source_directory):\n",
    "        entry_path = os.path.join(source_directory, entry_name)\n",
    "\n",
    "        # Check if the entry is a file and ends with .md (case-insensitive)\n",
    "        if os.path.isfile(entry_path) and entry_name.lower().endswith('.md'): # Changed from .pdf to .md\n",
    "            file_info = {}\n",
    "            file_info['filename'] = entry_name\n",
    "            file_info['filepath'] = entry_path\n",
    "\n",
    "            try:\n",
    "                file_size_bytes = os.path.getsize(entry_path)\n",
    "                file_info['storage_size_bytes'] = file_size_bytes\n",
    "                file_info['storage_size_mb'] = file_size_bytes / (1024 * 1024)\n",
    "            except Exception as e:\n",
    "                original_stdout.write(f\"Warning: Could not get size for {entry_name}: {e}\\n\")\n",
    "                file_info['storage_size_bytes'] = 0\n",
    "                file_info['storage_size_mb'] = 0\n",
    "\n",
    "            # Use analyze_md_content for .md files\n",
    "            text, table_count, figure_count, annotation_count = analyze_md_content(entry_path)\n",
    "\n",
    "            file_info['extracted_text'] = text\n",
    "            file_info['char_count'] = len(text)\n",
    "            all_char_counts.append(file_info['char_count'])\n",
    "\n",
    "            tokens = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "            word_count = len(tokens)\n",
    "            file_info['word_count'] = word_count\n",
    "            file_info['tokens'] = tokens\n",
    "\n",
    "            unique_words_in_file = set(tokens)\n",
    "            file_info['unique_word_count'] = len(unique_words_in_file)\n",
    "\n",
    "            file_info['char_entropy'] = calculate_shannon_entropy(list(text))\n",
    "            file_info['word_entropy'] = calculate_shannon_entropy(tokens)\n",
    "            file_info['average_pmi'] = calculate_avg_bigram_pmi(text, min_freq=PMI_BIGRAM_FREQ_THRESHOLD)\n",
    "\n",
    "            all_extracted_text_for_vocab += text + \" \"\n",
    "            all_extracted_chars += text\n",
    "\n",
    "            file_info['table_count'] = table_count\n",
    "            file_info['figure_count'] = figure_count\n",
    "            file_info['annotation_count'] = annotation_count # Will be 0 for MD files from analyze_md_content\n",
    "            file_info['has_tables'] = table_count > 0\n",
    "            file_info['has_figures'] = figure_count > 0\n",
    "            file_info['has_annotations'] = annotation_count > 0\n",
    "\n",
    "            file_info['language'] = detect_language(text)\n",
    "            file_info['estimated_info_content_bits_filesize'] = file_info.get('storage_size_bytes', 0) * 8 # Use .get for safety\n",
    "\n",
    "            file_data.append(file_info)\n",
    "\n",
    "    overall_char_entropy = calculate_shannon_entropy(list(all_extracted_chars))\n",
    "    overall_tokens = re.findall(r'\\b\\w+\\b', all_extracted_text_for_vocab.lower())\n",
    "    overall_word_counts = Counter(overall_tokens)\n",
    "    total_words_overall = len(overall_tokens)\n",
    "    overall_word_distribution = {word: count / total_words_overall for word, count in overall_word_counts.items()} if total_words_overall > 0 else {}\n",
    "    overall_vocabulary = set(overall_tokens)\n",
    "    overall_word_entropy = calculate_shannon_entropy(overall_tokens)\n",
    "    overall_average_pmi = calculate_avg_bigram_pmi(all_extracted_text_for_vocab, min_freq=PMI_BIGRAM_FREQ_THRESHOLD)\n",
    "    overall_avg_doc_length_chars = np.mean(all_char_counts) if all_char_counts else 0\n",
    "\n",
    "    if file_data and overall_word_distribution: # Check overall_word_distribution as well\n",
    "        for file_info in file_data:\n",
    "            # Ensure 'extracted_text' is present before trying to use it for JSD\n",
    "            if 'extracted_text' in file_info and file_info['extracted_text']:\n",
    "                 jsd_word = calculate_ca_divergence(file_info['extracted_text'], all_extracted_text_for_vocab, unit='word', smoothing=1e-9, base=2)\n",
    "                 file_info['js_dist'] = jsd_word\n",
    "            else:\n",
    "                 file_info['js_dist'] = np.nan # Set to NaN if no text\n",
    "    else:\n",
    "        for file_info in file_data:\n",
    "             file_info['js_dist'] = np.nan\n",
    "\n",
    "    if file_data:\n",
    "        df_data_list = []\n",
    "        for item in file_data:\n",
    "            digits_in_filename = re.findall(r'\\d+', item['filename'])\n",
    "            last_five_digits = \"\".join(digits_in_filename)[-5:] if digits_in_filename else \"\"\n",
    "            display_name = last_five_digits\n",
    "            if len(item['filename'].replace('.', '').replace('_', '').replace('-', '').replace(' ', '')) > len(last_five_digits):\n",
    "                display_name = \"...\" + display_name\n",
    "            \n",
    "            df_data_list.append({\n",
    "                'Filename': display_name,\n",
    "                'Size (MB)': item.get('storage_size_mb', 0.0), # Use .get for safety\n",
    "                'Word Count': item.get('word_count', 0),\n",
    "                'Unique Words': item.get('unique_word_count', 0),\n",
    "                'Doc Length (Chars)': item.get('char_count', 0),\n",
    "                'Language': item.get('language', 'N/A'),\n",
    "                'Tables': item.get('table_count', 0),\n",
    "                'Figures': item.get('figure_count', 0),\n",
    "                'Annotations': item.get('annotation_count', 0),\n",
    "                'Char Entropy': item.get('char_entropy', 0.0),\n",
    "                'Word Entropy': item.get('word_entropy', 0.0),\n",
    "                'Avg PMI': item.get('average_pmi', 0.0),\n",
    "                'JS Dist': item.get('js_dist', np.nan)\n",
    "            })\n",
    "        df = pd.DataFrame(df_data_list)\n",
    "\n",
    "        original_stdout.write(\"\\n\\n--- Pandas DataFrame Summary ---\\n\")\n",
    "        with pd.option_context('display.max_rows', None, 'display.max_columns', None, 'display.width', 1000):\n",
    "            original_stdout.write(str(df.head()) + \"\\n\")\n",
    "        original_stdout.write(f\"DataFrame shape: {df.shape}\\n\")\n",
    "\n",
    "        numerical_column_names = [\n",
    "            'Size (MB)', 'Word Count', 'Unique Words', 'Doc Length (Chars)',\n",
    "            'Char Entropy', 'Word Entropy', 'Avg PMI', 'JS Dist'\n",
    "        ]\n",
    "        numerical_data = df[numerical_column_names].copy()\n",
    "\n",
    "        original_stdout.write(\"\\n\\n--- Numerical Data DataFrame Summary ---\\n\")\n",
    "        with pd.option_context('display.max_rows', None, 'display.max_columns', None, 'display.width', 1000):\n",
    "            original_stdout.write(str(numerical_data.head()) + \"\\n\")\n",
    "        original_stdout.write(f\"Numerical DataFrame shape: {numerical_data.shape}\\n\")\n",
    "        original_stdout.write(str(numerical_data.dtypes) + \"\\n\")\n",
    "\n",
    "        try:\n",
    "            ## numerical_data_csv_file = 'MD_numerical_stats.csv' # Changed filename\n",
    "            numerical_data.to_csv(numerical_data_csv_file, index=False)\n",
    "            original_stdout.write(f\"\\nNumerical data saved to {numerical_data_csv_file}\\n\")\n",
    "        except Exception as e:\n",
    "            original_stdout.write(f\"\\nError saving numerical data to CSV: {e}\\n\")\n",
    "    else:\n",
    "        original_stdout.write(\"\\n\\n--- Pandas DataFrame Summary ---\\n\")\n",
    "        original_stdout.write(\"No data to create DataFrame.\\n\")\n",
    "\n",
    "    print(\"\\n## Per-File Analysis Results\")\n",
    "    if file_data:\n",
    "        print(\"\\n| Filename | Size (MB) | Word Count | Unique Words | doc length | Language | Tables | Figures | Annotations | Char Entropy | Word Entropy | Avg PMI | JS Dist |\")\n",
    "        print(\"|---|---|---|---|---|---|---|---|---|---|---|---|---|\")\n",
    "        for file_info in file_data:\n",
    "            digits_in_filename = re.findall(r'\\d+', file_info['filename'])\n",
    "            last_five_digits = \"\".join(digits_in_filename)[-5:] if digits_in_filename else \"\"\n",
    "            display_filename = last_five_digits\n",
    "            if len(file_info['filename'].replace('.', '').replace('_', '').replace('-', '').replace(' ', '')) > len(last_five_digits):\n",
    "                 display_filename = \"...\" + display_filename\n",
    "            js_dist_formatted = f\"{file_info.get('js_dist', np.nan):.4f}\" if not np.isnan(file_info.get('js_dist', np.nan)) else \"NaN\"\n",
    "            print(f\"| {display_filename} | {file_info.get('storage_size_mb', 0.0):.2f} | {file_info.get('word_count',0)} | {file_info.get('unique_word_count',0)} | {file_info.get('char_count',0)} | {file_info.get('language','N/A')} | {file_info.get('table_count',0)} | {file_info.get('figure_count',0)} | {file_info.get('annotation_count',0)} | {file_info.get('char_entropy',0.0):.2f} | {file_info.get('word_entropy',0.0):.2f} | {file_info.get('average_pmi',0.0):.4f} | {js_dist_formatted} |\")\n",
    "    else:\n",
    "        print(f\"No Markdown files found in the specified directory: {source_directory}\") # Updated message\n",
    "\n",
    "    print(\"\\n## Overall Dataset Summary\")\n",
    "    total_files = len(file_data)\n",
    "    print(f\"\\n### 1. Number of MD Files: {total_files}\") # Updated message\n",
    "\n",
    "    if total_files > 0:\n",
    "        all_storage_sizes_bytes = [f.get('storage_size_bytes', 0) for f in file_data]\n",
    "        total_storage_size_bytes = sum(all_storage_sizes_bytes)\n",
    "        total_storage_size_mb = total_storage_size_bytes / (1024 * 1024) if total_storage_size_bytes > 0 else 0\n",
    "        print(f\"\\n### 2. Storage Size\")\n",
    "        print(f\"- **Total:** {total_storage_size_mb:.2f} MB ({total_storage_size_bytes} bytes)\")\n",
    "        if all_storage_sizes_bytes: # Check if list is not empty before calculating mean/min/max\n",
    "            avg_file_size_mb = (statistics.mean(all_storage_sizes_bytes) / (1024 * 1024)) if all_storage_sizes_bytes else 0\n",
    "            min_file_size_mb = (min(all_storage_sizes_bytes) / (1024 * 1024)) if all_storage_sizes_bytes else 0\n",
    "            max_file_size_mb = (max(all_storage_sizes_bytes) / (1024 * 1024)) if all_storage_sizes_bytes else 0\n",
    "            print(f\"- **Average:** {avg_file_size_mb:.2f} MB\")\n",
    "            print(f\"- **Range:** ({min_file_size_mb:.2f} MB, {max_file_size_mb:.2f} MB)\")\n",
    "\n",
    "        all_word_counts = [f.get('word_count', 0) for f in file_data]\n",
    "        total_word_count = sum(all_word_counts)\n",
    "        print(f\"\\n### 3. Textual Content Size\")\n",
    "        print(f\"- **Total words/tokens:** {total_word_count}\")\n",
    "        if all_word_counts:\n",
    "            avg_word_count = statistics.mean(all_word_counts) if all_word_counts else 0\n",
    "            print(f\"- **Average words/tokens per document:** {avg_word_count:.2f}\")\n",
    "            vocabulary_size = len(overall_vocabulary) # overall_vocabulary already calculated\n",
    "            print(f\"- **Unique tokens across dataset (vocabulary size):** {vocabulary_size}\")\n",
    "        else:\n",
    "             print(f\"- **Average words/tokens per document:** 0\")\n",
    "             print(f\"- **Unique tokens across dataset (vocabulary size):** 0\")\n",
    "\n",
    "        total_estimated_info_content_bits_filesize = total_storage_size_bytes * 8\n",
    "        print(f\"\\n### 4. Information Content (Estimated)\")\n",
    "        print(f\"- **Total estimated info content (based on total file size):** {total_estimated_info_content_bits_filesize} bits\")\n",
    "        print(f\"- *Note: This estimate is based on raw file size in bits.*\")\n",
    "\n",
    "        print(f\"\\n### 4b. Information Theory Metrics (Overall Dataset)\")\n",
    "        print(f\"- **Overall Character Entropy:** {overall_char_entropy:.2f} bits/character\")\n",
    "        print(f\"- **Overall Word Entropy:** {overall_word_entropy:.2f} bits/word\")\n",
    "        print(f\"- **Average Bigram PMI (Threshold={PMI_BIGRAM_FREQ_THRESHOLD}):** {overall_average_pmi:.4f}\")\n",
    "        print(f\"  *Note: Average PMI is calculated for bigrams appearing at least {PMI_BIGRAM_FREQ_THRESHOLD} times across the dataset.*\")\n",
    "        print(f\"  *Note: Per-file JSD is calculated against the overall dataset word distribution.*\")\n",
    "        print(f\"- **Overall Average Document Length (Characters):** {overall_avg_doc_length_chars:.2f}\")\n",
    "\n",
    "        print(f\"\\n### 5. Document Length Distribution (in words/tokens)\")\n",
    "        if all_word_counts:\n",
    "            mean_len = statistics.mean(all_word_counts) if all_word_counts else 0\n",
    "            median_len = statistics.median(all_word_counts) if all_word_counts else 0\n",
    "            std_dev_len = statistics.stdev(all_word_counts) if len(all_word_counts) > 1 else 0\n",
    "            print(f\"- **Mean length:** {mean_len:.2f}\")\n",
    "            print(f\"- **Median length:** {median_len}\")\n",
    "            print(f\"- **Standard deviation:** {std_dev_len:.2f}\")\n",
    "\n",
    "        print(f\"\\n### 6. Language Distribution\")\n",
    "        all_languages = [f.get('language', 'N/A') for f in file_data]\n",
    "        language_counts = Counter(all_languages)\n",
    "        print(\"- **Counts:**\")\n",
    "        for lang, count in language_counts.most_common():\n",
    "            print(f\"   - {lang}: {count} files\")\n",
    "\n",
    "        print(f\"\\n### 7. Structural Elements Summary\")\n",
    "        total_tables_found = sum([f.get('table_count', 0) for f in file_data])\n",
    "        total_figures_found = sum([f.get('figure_count', 0) for f in file_data])\n",
    "        total_annotations_found = sum([f.get('annotation_count', 0) for f in file_data]) # Will be 0 for MD\n",
    "        files_with_tables = sum([f.get('has_tables', False) for f in file_data])\n",
    "        files_with_figures = sum([f.get('has_figures', False) for f in file_data])\n",
    "        files_with_annotations = sum([f.get('has_annotations', False) for f in file_data]) # Will be 0 for MD\n",
    "\n",
    "        print(f\"- **Total Tables found (heuristic):** {total_tables_found}\")\n",
    "        print(f\"- **Total Figures/Images found:** {total_figures_found}\")\n",
    "        print(f\"- **Total Annotations found:** {total_annotations_found}\") # Will be 0\n",
    "        print(f\"- **Files with Tables (heuristic):** {files_with_tables} ({files_with_tables/total_files*100:.2f}% if total_files > 0 else 0.00%)\")\n",
    "        print(f\"- **Files with Figures/Images:** {files_with_figures} ({files_with_figures/total_files*100:.2f}% if total_files > 0 else 0.00%)\")\n",
    "        print(f\"- **Files with Annotations:** {files_with_annotations} ({files_with_annotations/total_files*100:.2f}% if total_files > 0 else 0.00%)\")\n",
    "    else:\n",
    "        print(\"No data collected from MD files.\")\n",
    "\n",
    "sys.stdout = original_stdout\n",
    "sys.stdout.flush()\n",
    "\n",
    "markdown_content = report_output.getvalue()\n",
    "print(\"\\n\" + \"=\"*30 + \" MD File Analysis Report \" + \"=\"*30) # Updated title\n",
    "print(\"\\n*Note: Markdown rendering may vary depending on the viewer.*\\n\")\n",
    "print(markdown_content)\n",
    "print(\"=\"*79)\n",
    "\n",
    "try:\n",
    "    with open(output_markdown_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(markdown_content)\n",
    "    print(f\"\\nAnalysis report also saved to {output_markdown_file}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nError saving report to {output_markdown_file}: {e}\")\n",
    "\n",
    "report_output.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc51b3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azure-openai-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
