{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60b53ba9",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "<span style=\"font-size: 12px;\">\n",
    "\n",
    "\n",
    "Given:\n",
    "\n",
    "Key Parameters to Describe a PDF Dataset Parametrically\n",
    "\n",
    "1. Number of PDF Files\n",
    "\n",
    "Total count of PDF documents included in the dataset.\n",
    "\n",
    "2. Storage Size\n",
    "\n",
    "Total size on disk (e.g., in megabytes or gigabytes).\n",
    "\n",
    "Average file size and range (min, max) to indicate variability.\n",
    "\n",
    "3. Textual Content Size\n",
    "\n",
    "Total number of words or tokens extracted from all PDFs combined.\n",
    "\n",
    "Average number of words or tokens per document.\n",
    "\n",
    "If applicable, number of unique tokens (vocabulary size).\n",
    "\n",
    "4. Information Content (in Bits)\n",
    "\n",
    "Approximate information content can be estimated by the file size in bytes multiplied by 8 (bits per byte).\n",
    "\n",
    "For extracted text, entropy-based measures or compression size can be used as proxies for information content.\n",
    "\n",
    "If the dataset includes scanned PDFs requiring OCR, note the quality and potential noise affecting information content.\n",
    "\n",
    "5. Document Length Distribution\n",
    "\n",
    "Statistical summary of document lengths (e.g., mean, median, standard deviation of word counts or token counts).\n",
    "\n",
    "Distribution shape (e.g., skewness) if relevant.\n",
    "\n",
    "6. Language and Encoding\n",
    "\n",
    "Language(s) of the documents.\n",
    "\n",
    "Text encoding used (e.g., UTF-8).\n",
    "\n",
    "7. Structural Elements (Optional)\n",
    "\n",
    "Number or proportion of documents containing tables, figures, or annotations.\n",
    "\n",
    "\n",
    "\n",
    "Directory: D:\\Dataset\\Lagerugpijn\\LR_EPDs\n",
    "\n",
    "contains a batch of EHR PDF files\n",
    "\n",
    "\n",
    "\n",
    "Provide a Pyhton based Jupyter notebook code that provides the above stated parameters about all PDF files stored in D:\\Dataset\\Lagerugpijn\\LR_EPDs\n",
    "\n",
    "\n",
    "============================================================================================================\n",
    "\n",
    "now make sure it can detect the language that is used in the PDF files\n",
    "\n",
    "add sophisticated measures (entropy, compression) would require deeper analysis.\n",
    "\n",
    "\n",
    "\n",
    "make sure that the output reports on all the required parameters for each file separately and that it concludes with a general overview report that best describes the entire dataset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "===========================================================================================================\n",
    "\n",
    "\n",
    "\n",
    "now make sure the code:\n",
    "\n",
    "can Identifying structural elements like tables, figures, or annotations requires more advanced PDF parsing libraries and logic\n",
    "\n",
    "+ reports on these elements in the Per-File Analysis Results ouput\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e466d56",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "0c55bff5",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "pip install pdfminer.six\n",
    "pip install langdetect\n",
    "=============================>\n",
    "pip install langdetect zlib pdfminer.six pdfplumber numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4623ac27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing PDF files in directory: D:\\Dataset\\Lagerugpijn\\LR_EPDs\n",
      "\n",
      "============================== PDF Analysis Report ==============================\n",
      "\n",
      "*Note: Markdown rendering may vary depending on the viewer. Multi-column layout and font size control are not standard Markdown features.*\n",
      "\n",
      "\n",
      "## Per-File Analysis Results\n",
      "\n",
      "| Filename | Size (MB) | Word Count | Unique Words | Language | Tables | Figures | Annotations |\n",
      "|---|---|---|---|---|---|---|---|\n",
      "| EPDAfdruk_897_59037.pdf | 0.20 | 1766 | 453 | nl | 3 | 0 | 0 |\n",
      "| EPDAfdruk_897_59684.pdf | 0.19 | 1589 | 543 | nl | 3 | 0 | 0 |\n",
      "| EPDAfdruk_897_60038.pdf | 0.17 | 1452 | 493 | nl | 3 | 0 | 0 |\n",
      "| EPDAfdruk_897_60384.pdf | 0.17 | 1667 | 472 | nl | 2 | 0 | 0 |\n",
      "| EPDAfdruk_897_60818.pdf | 0.17 | 1344 | 474 | nl | 3 | 0 | 0 |\n",
      "| EPDAfdruk_897_61014.pdf | 0.20 | 1605 | 507 | nl | 4 | 0 | 0 |\n",
      "| EPDAfdruk_897_61368.pdf | 0.19 | 1888 | 532 | nl | 3 | 0 | 0 |\n",
      "| EPDAfdruk_897_61665.pdf | 0.16 | 1554 | 446 | nl | 2 | 0 | 0 |\n",
      "| EPDAfdruk_897_61810.pdf | 0.19 | 2176 | 651 | nl | 2 | 0 | 0 |\n",
      "| EPDAfdruk_897_62117.pdf | 0.16 | 1360 | 427 | nl | 3 | 0 | 0 |\n",
      "| EPDAfdruk_897_62177.pdf | 0.17 | 1367 | 480 | nl | 3 | 0 | 0 |\n",
      "| EPDAfdruk_897_62655.pdf | 0.16 | 1581 | 454 | nl | 2 | 0 | 0 |\n",
      "| EPDAfdruk_897_63175.pdf | 0.15 | 1566 | 501 | nl | 2 | 0 | 0 |\n",
      "\n",
      "## Overall Dataset Summary\n",
      "\n",
      "### 1. Number of PDF Files: 13\n",
      "\n",
      "### 2. Storage Size\n",
      "- **Total:** 2.28 MB (2395567 bytes)\n",
      "- **Average:** 0.18 MB\n",
      "- **Range:** (0.15 MB, 0.20 MB)\n",
      "\n",
      "### 3. Textual Content Size\n",
      "- **Total words/tokens:** 20915\n",
      "- **Average words/tokens per document:** 1608.85\n",
      "- **Unique tokens across dataset (vocabulary size):** 1667\n",
      "\n",
      "### 4. Information Content (Estimated)\n",
      "- **Total estimated info content (based on total file size):** 19164536 bits\n",
      "- *Note: This estimate is based on raw file size in bits.*\n",
      "\n",
      "### 5. Document Length Distribution (in words/tokens)\n",
      "- **Mean length:** 1608.85\n",
      "- **Median length:** 1581\n",
      "- **Standard deviation:** 232.53\n",
      "\n",
      "### 6. Language Distribution\n",
      "- **Counts:**\n",
      "   - nl: 13 files\n",
      "\n",
      "### 7. Structural Elements Summary\n",
      "- **Total Tables found:** 35\n",
      "- **Total Figures/Images found:** 0\n",
      "- **Total Annotations found:** 0\n",
      "- **Files with Tables:** 13 (100.00%)\n",
      "- **Files with Figures/Images:** 0 (0.00%)\n",
      "- **Files with Annotations:** 0 (0.00%)\n",
      "\n",
      "===============================================================================\n",
      "\n",
      "Analysis report also saved to pdf_analysis_report.md\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import statistics\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import re # For tokenization\n",
    "from langdetect import detect, DetectorFactory # For language detection\n",
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "import pdfplumber # Added: For advanced PDF parsing\n",
    "from io import StringIO # To capture print output\n",
    "\n",
    "# Ensure consistent language detection results\n",
    "DetectorFactory.seed = 0\n",
    "\n",
    "# --- Configuration ---\n",
    "# Directory containing the PDF files\n",
    "pdf_directory = r'D:\\Dataset\\Lagerugpijn\\LR_EPDs' # Use a raw string for the path\n",
    "output_markdown_file = 'pdf_analysis_report.md' # Name of the output Markdown file\n",
    "\n",
    "# --- Data Storage ---\n",
    "file_data = [] # List to store dictionaries, each containing data for one file\n",
    "all_extracted_text_for_vocab = \"\" # String to accumulate all text for vocabulary analysis\n",
    "report_output = StringIO() # Use StringIO to capture print statements\n",
    "\n",
    "# Redirect stdout to capture print output\n",
    "original_stdout = sys.stdout\n",
    "sys.stdout = report_output\n",
    "\n",
    "# --- Helper Function for Text Extraction and Structural Element Detection ---\n",
    "def analyze_pdf_content(pdf_path):\n",
    "    \"\"\"\n",
    "    Extracts text and detects structural elements (tables, figures, annotations)\n",
    "    from a PDF file using pdfplumber.\n",
    "    Returns extracted text, table count, figure count, annotation count.\n",
    "    \"\"\"\n",
    "    extracted_text = \"\"\n",
    "    table_count = 0\n",
    "    figure_count = 0\n",
    "    annotation_count = 0\n",
    "\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for page in pdf.pages:\n",
    "                # Extract text\n",
    "                extracted_text += page.extract_text() or \"\" # Add text, handle None\n",
    "\n",
    "                # Detect tables\n",
    "                tables = page.extract_tables()\n",
    "                table_count += len(tables)\n",
    "\n",
    "                # Detect figures (images)\n",
    "                figure_count += len(page.images)\n",
    "\n",
    "                # Detect annotations\n",
    "                annotation_count += len(page.annots)\n",
    "\n",
    "    except pdfplumber.PDFSyntaxError as e:\n",
    "         # Print warnings directly to original stdout so they appear during processing\n",
    "         original_stdout.write(f\"Warning: PDF Syntax Error in {os.path.basename(pdf_path)}: {e}\\n\")\n",
    "         # Return empty data for this file if there's a syntax error\n",
    "         return \"\", 0, 0, 0\n",
    "    except Exception as e:\n",
    "        # Print warnings directly to original stdout\n",
    "        original_stdout.write(f\"Warning: Error processing {os.path.basename(pdf_path)} with pdfplumber: {e}\\n\")\n",
    "        # Return empty data for this file if there's any other error\n",
    "        return \"\", 0, 0, 0\n",
    "\n",
    "    return extracted_text, table_count, figure_count, annotation_count\n",
    "\n",
    "\n",
    "# --- Helper Function for Language Detection ---\n",
    "def detect_language(text):\n",
    "    \"\"\"Detects the language of the input text.\"\"\"\n",
    "    if not text.strip():\n",
    "        return \"N/A (No text)\"\n",
    "    try:\n",
    "        # langdetect works best on larger text samples.\n",
    "        # Use a sample if the text is very long to speed things up,\n",
    "        # but for typical documents, using the full text is fine.\n",
    "        # Limit text length for detection to avoid potential issues with very large texts\n",
    "        sample_text = text[:5000] if len(text) > 5000 else text\n",
    "        if not sample_text.strip():\n",
    "             return \"N/A (No text in sample)\"\n",
    "        return detect(sample_text)\n",
    "    except LangDetectException:\n",
    "        return \"Undetected\"\n",
    "    except Exception as e:\n",
    "        # Print warnings directly to original stdout\n",
    "        original_stdout.write(f\"Warning: Error during language detection: {e}\\n\")\n",
    "        return \"Error\"\n",
    "\n",
    "# --- Analysis ---\n",
    "# Print initial message to original stdout\n",
    "original_stdout.write(f\"Analyzing PDF files in directory: {pdf_directory}\\n\")\n",
    "\n",
    "\n",
    "if not os.path.isdir(pdf_directory):\n",
    "    # Print error to original stdout\n",
    "    original_stdout.write(f\"Error: Directory not found at {pdf_directory}\\n\")\n",
    "else:\n",
    "    # Iterate through all entries in the directory\n",
    "    for entry_name in os.listdir(pdf_directory):\n",
    "        entry_path = os.path.join(pdf_directory, entry_name)\n",
    "\n",
    "        # Check if the entry is a file and ends with .pdf (case-insensitive)\n",
    "        if os.path.isfile(entry_path) and entry_name.lower().endswith('.pdf'):\n",
    "\n",
    "            file_info = {} # Dictionary to store data for the current file\n",
    "            file_info['filename'] = entry_name\n",
    "            file_info['filepath'] = entry_path\n",
    "\n",
    "            # 1. & 2. Storage Size\n",
    "            try:\n",
    "                file_size_bytes = os.path.getsize(entry_path) # size in bytes\n",
    "                file_info['storage_size_bytes'] = file_size_bytes\n",
    "                file_info['storage_size_mb'] = file_size_bytes / (1024 * 1024)\n",
    "            except Exception as e:\n",
    "                # Print warnings directly to original stdout\n",
    "                original_stdout.write(f\"Warning: Could not get size for {entry_name}: {e}\\n\")\n",
    "                file_info['storage_size_bytes'] = 0\n",
    "                file_info['storage_size_mb'] = 0\n",
    "\n",
    "\n",
    "            # 3. Textual Content Size & Extraction + 7. Structural Elements\n",
    "            text, table_count, figure_count, annotation_count = analyze_pdf_content(entry_path)\n",
    "\n",
    "            file_info['extracted_text'] = text # Store text for language\n",
    "            # Simple tokenization: split by whitespace and punctuation\n",
    "            tokens = re.findall(r'\\b\\w+\\b', text.lower()) # Convert to lower case for vocabulary size\n",
    "            word_count = len(tokens)\n",
    "            file_info['word_count'] = word_count\n",
    "\n",
    "            # Calculate unique words for the current file\n",
    "            unique_words_in_file = set(tokens)\n",
    "            file_info['unique_word_count'] = len(unique_words_in_file)\n",
    "\n",
    "\n",
    "            # Accumulate text for overall vocabulary later\n",
    "            all_extracted_text_for_vocab += text + \" \" # Add a space to ensure separation\n",
    "\n",
    "            # Store structural element counts\n",
    "            file_info['table_count'] = table_count\n",
    "            file_info['figure_count'] = figure_count\n",
    "            file_info['annotation_count'] = annotation_count\n",
    "            # Add boolean flags for easy checking\n",
    "            file_info['has_tables'] = table_count > 0\n",
    "            file_info['has_figures'] = figure_count > 0\n",
    "            file_info['has_annotations'] = annotation_count > 0\n",
    "\n",
    "\n",
    "            # 6. Language Detection\n",
    "            file_info['language'] = detect_language(text)\n",
    "\n",
    "            # 4. Information Content (Estimated)\n",
    "            # Simple estimation based on file size in bits\n",
    "            file_info['estimated_info_content_bits_filesize'] = file_size_bytes * 8\n",
    "\n",
    "\n",
    "            file_data.append(file_info) # Add the file's data to the list\n",
    "\n",
    "\n",
    "    # --- Per-File Reporting (Captured for Markdown) ---\n",
    "    print(\"\\n## Per-File Analysis Results\") # Markdown Heading\n",
    "    if file_data:\n",
    "        # Create a Markdown table for per-file data\n",
    "        # Added 'Unique Words' column header\n",
    "        print(\"\\n| Filename | Size (MB) | Word Count | Unique Words | Language | Tables | Figures | Annotations |\")\n",
    "        print(\"|---|---|---|---|---|---|---|---|\") # Updated separator line\n",
    "        for file_info in file_data:\n",
    "            # Added file_info['unique_word_count'] to the row\n",
    "            print(f\"| {file_info['filename']} | {file_info['storage_size_mb']:.2f} | {file_info['word_count']} | {file_info['unique_word_count']} | {file_info['language']} | {file_info['table_count']} | {file_info['figure_count']} | {file_info['annotation_count']} |\")\n",
    "\n",
    "    else:\n",
    "        print(\"No PDF files found in the specified directory.\")\n",
    "\n",
    "\n",
    "    # --- Overall Dataset Summary (Captured for Markdown) ---\n",
    "    print(\"\\n## Overall Dataset Summary\") # Markdown Heading\n",
    "\n",
    "    total_files = len(file_data)\n",
    "    print(f\"\\n### 1. Number of PDF Files: {total_files}\") # Markdown Subheading\n",
    "\n",
    "    if total_files > 0:\n",
    "        # 2. Storage Size\n",
    "        all_storage_sizes_bytes = [f['storage_size_bytes'] for f in file_data]\n",
    "        total_storage_size_bytes = sum(all_storage_sizes_bytes)\n",
    "        total_storage_size_mb = total_storage_size_bytes / (1024 * 1024)\n",
    "        print(f\"\\n### 2. Storage Size\") # Markdown Subheading\n",
    "        print(f\"- **Total:** {total_storage_size_mb:.2f} MB ({total_storage_size_bytes} bytes)\")\n",
    "        if all_storage_sizes_bytes:\n",
    "            avg_file_size_mb = statistics.mean(all_storage_sizes_bytes) / (1024 * 1024)\n",
    "            min_file_size_mb = min(all_storage_sizes_bytes) / (1024 * 1024)\n",
    "            max_file_size_mb = max(all_storage_sizes_bytes) / (1024 * 1024)\n",
    "            print(f\"- **Average:** {avg_file_size_mb:.2f} MB\")\n",
    "            print(f\"- **Range:** ({min_file_size_mb:.2f} MB, {max_file_size_mb:.2f} MB)\")\n",
    "\n",
    "        # 3. Textual Content Size\n",
    "        all_word_counts = [f['word_count'] for f in file_data]\n",
    "        total_word_count = sum(all_word_counts)\n",
    "        print(f\"\\n### 3. Textual Content Size\") # Markdown Subheading\n",
    "        print(f\"- **Total words/tokens:** {total_word_count}\")\n",
    "        if all_word_counts:\n",
    "            avg_word_count = statistics.mean(all_word_counts)\n",
    "            print(f\"- **Average words/tokens per document:** {avg_word_count:.2f}\")\n",
    "\n",
    "            # Calculate unique tokens (vocabulary size) from accumulated text\n",
    "            all_tokens = re.findall(r'\\b\\w+\\b', all_extracted_text_for_vocab.lower())\n",
    "            unique_tokens = set(all_tokens)\n",
    "            vocabulary_size = len(unique_tokens)\n",
    "            print(f\"- **Unique tokens across dataset (vocabulary size):** {vocabulary_size}\") # Clarified label\n",
    "        else:\n",
    "             print(f\"- **Average words/tokens per document:** 0\")\n",
    "             print(f\"- **Unique tokens across dataset (vocabulary size):** 0\")\n",
    "\n",
    "        # 4. Information Content (Estimated)\n",
    "        total_estimated_info_content_bits_filesize = total_storage_size_bytes * 8\n",
    "        print(f\"\\n### 4. Information Content (Estimated)\") # Markdown Subheading\n",
    "        print(f\"- **Total estimated info content (based on total file size):** {total_estimated_info_content_bits_filesize} bits\")\n",
    "        print(f\"- *Note: This estimate is based on raw file size in bits.*\")\n",
    "\n",
    "\n",
    "        # 5. Document Length Distribution\n",
    "        print(f\"\\n### 5. Document Length Distribution (in words/tokens)\") # Markdown Subheading\n",
    "        if all_word_counts:\n",
    "            mean_len = statistics.mean(all_word_counts)\n",
    "            median_len = statistics.median(all_word_counts)\n",
    "            # Ensure std dev calculation is valid for >1 data points\n",
    "            std_dev_len = statistics.stdev(all_word_counts) if len(all_word_counts) > 1 else 0\n",
    "            print(f\"- **Mean length:** {mean_len:.2f}\")\n",
    "            print(f\"- **Median length:** {median_len}\")\n",
    "            print(f\"- **Standard deviation:** {std_dev_len:.2f}\")\n",
    "\n",
    "\n",
    "        # 6. Language Distribution\n",
    "        print(f\"\\n### 6. Language Distribution\") # Markdown Subheading\n",
    "        all_languages = [f['language'] for f in file_data]\n",
    "        language_counts = Counter(all_languages)\n",
    "        print(\"- **Counts:**\")\n",
    "        for lang, count in language_counts.most_common():\n",
    "            print(f\"   - {lang}: {count} files\")\n",
    "\n",
    "\n",
    "        # 7. Structural Elements Summary\n",
    "        print(f\"\\n### 7. Structural Elements Summary\") # Markdown Subheading\n",
    "        total_tables_found = sum([f['table_count'] for f in file_data])\n",
    "        total_figures_found = sum([f['figure_count'] for f in file_data])\n",
    "        total_annotations_found = sum([f['annotation_count'] for f in file_data])\n",
    "\n",
    "        files_with_tables = sum([f['has_tables'] for f in file_data])\n",
    "        files_with_figures = sum([f['has_figures'] for f in file_data])\n",
    "        files_with_annotations = sum([f['has_annotations'] for f in file_data])\n",
    "\n",
    "        print(f\"- **Total Tables found:** {total_tables_found}\")\n",
    "        print(f\"- **Total Figures/Images found:** {total_figures_found}\")\n",
    "        print(f\"- **Total Annotations found:** {total_annotations_found}\")\n",
    "        print(f\"- **Files with Tables:** {files_with_tables} ({files_with_tables/total_files*100:.2f}%)\")\n",
    "        print(f\"- **Files with Figures/Images:** {files_with_figures} ({files_with_figures/total_files*100:.2f}%)\")\n",
    "        print(f\"- **Files with Annotations:** {files_with_annotations} ({files_with_annotations/total_files*100:.2f}%)\")\n",
    "\n",
    "\n",
    "    else:\n",
    "        print(\"No data collected from PDF files.\")\n",
    "\n",
    "\n",
    "# --- End of Captured Output ---\n",
    "sys.stdout = original_stdout # Restore stdout\n",
    "\n",
    "# --- Display Report in Console ---\n",
    "markdown_content = report_output.getvalue() # Get the captured output\n",
    "print(\"\\n\" + \"=\"*30 + \" PDF Analysis Report \" + \"=\"*30) # Separator\n",
    "print(\"\\n*Note: Markdown rendering may vary depending on the viewer. Multi-column layout and font size control are not standard Markdown features.*\\n\")\n",
    "print(markdown_content) # Print the captured report content to console\n",
    "print(\"=\"*79) # Separator\n",
    "\n",
    "\n",
    "# --- Save Output to Markdown File ---\n",
    "try:\n",
    "    with open(output_markdown_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(markdown_content)\n",
    "    print(f\"\\nAnalysis report also saved to {output_markdown_file}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nError saving report to {output_markdown_file}: {e}\")\n",
    "\n",
    "report_output.close() # Close the StringIO object\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b7ba6c09",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "==============================================================================================\n",
    "==============================================================================================\n",
    "==============================================================================================\n",
    "==============================================================================================\n",
    "==============================================================================================\n",
    "==============================================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b199c9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing PDF files in directory: D:\\Dataset\\Lagerugpijn\\LR_EPDs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================== PDF Analysis Report ==============================\n",
      "\n",
      "*Note: Markdown rendering may vary depending on the viewer. Multi-column layout and font size control are not standard Markdown features.*\n",
      "\n",
      "\n",
      "## Per-File Analysis Results\n",
      "\n",
      "| Filename | Size (MB) | Word Count | Unique Words | Language | Tables | Figures | Annotations | Char Entropy | Word Entropy | Avg PMI |\n",
      "|---|---|---|---|---|---|---|---|---|---|---|\n",
      "| ...59037 | 0.20 | 1766 | 453 | nl | 3 | 0 | 0 | 4.89 | 8.15 | 6.9683 |\n",
      "| ...59684 | 0.19 | 1589 | 543 | nl | 3 | 0 | 0 | 4.90 | 8.54 | 6.9370 |\n",
      "| ...60038 | 0.17 | 1452 | 493 | nl | 3 | 0 | 0 | 4.85 | 8.39 | 6.7113 |\n",
      "| ...60384 | 0.17 | 1667 | 472 | nl | 2 | 0 | 0 | 4.83 | 8.14 | 6.5174 |\n",
      "| ...60818 | 0.17 | 1344 | 474 | nl | 3 | 0 | 0 | 4.93 | 8.35 | 6.2739 |\n",
      "| ...61014 | 0.20 | 1605 | 507 | nl | 4 | 0 | 0 | 4.88 | 8.43 | 7.1602 |\n",
      "| ...61368 | 0.19 | 1888 | 532 | nl | 3 | 0 | 0 | 4.89 | 8.32 | 6.7119 |\n",
      "| ...61665 | 0.16 | 1554 | 446 | nl | 2 | 0 | 0 | 4.81 | 8.03 | 6.2304 |\n",
      "| ...61810 | 0.19 | 2176 | 651 | nl | 2 | 0 | 0 | 4.83 | 8.49 | 6.5144 |\n",
      "| ...62117 | 0.16 | 1360 | 427 | nl | 3 | 0 | 0 | 4.87 | 8.08 | 6.2708 |\n",
      "| ...62177 | 0.17 | 1367 | 480 | nl | 3 | 0 | 0 | 4.97 | 8.39 | 6.7884 |\n",
      "| ...62655 | 0.16 | 1581 | 454 | nl | 2 | 0 | 0 | 4.79 | 8.04 | 6.3726 |\n",
      "| ...63175 | 0.15 | 1566 | 501 | nl | 2 | 0 | 0 | 4.82 | 8.26 | 6.2312 |\n",
      "\n",
      "## Overall Dataset Summary\n",
      "\n",
      "### 1. Number of PDF Files: 13\n",
      "\n",
      "### 2. Storage Size\n",
      "- **Total:** 2.28 MB (2395567 bytes)\n",
      "- **Average:** 0.18 MB\n",
      "- **Range:** (0.15 MB, 0.20 MB)\n",
      "\n",
      "### 3. Textual Content Size\n",
      "- **Total words/tokens:** 20915\n",
      "- **Average words/tokens per document:** 1608.85\n",
      "- **Unique tokens across dataset (vocabulary size):** 1667\n",
      "\n",
      "### 4. Information Content (Estimated)\n",
      "- **Total estimated info content (based on total file size):** 19164536 bits\n",
      "- *Note: This estimate is based on raw file size in bits.*\n",
      "\n",
      "### 4b. Information Theory Metrics (Overall Dataset)\n",
      "- **Overall Character Entropy:** 4.88 bits/character\n",
      "- **Overall Word Entropy:** 9.16 bits/word\n",
      "- **Average Bigram PMI (Threshold=3):** 6.6699\n",
      "  *Note: Average PMI is calculated for bigrams appearing at least 3 times across the dataset.*\n",
      "\n",
      "### 5. Document Length Distribution (in words/tokens)\n",
      "- **Mean length:** 1608.85\n",
      "- **Median length:** 1581\n",
      "- **Standard deviation:** 232.53\n",
      "\n",
      "### 6. Language Distribution\n",
      "- **Counts:**\n",
      "   - nl: 13 files\n",
      "\n",
      "### 7. Structural Elements Summary\n",
      "- **Total Tables found:** 35\n",
      "- **Total Figures/Images found:** 0\n",
      "- **Total Annotations found:** 0\n",
      "- **Files with Tables:** 13 (100.00%)\n",
      "- **Files with Figures/Images:** 0 (0.00%)\n",
      "- **Files with Annotations:** 0 (0.00%)\n",
      "\n",
      "===============================================================================\n",
      "\n",
      "Analysis report also saved to pdf_analysis_report.md\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import statistics\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import re # For tokenization\n",
    "from langdetect import detect, DetectorFactory # For language detection\n",
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "import pdfplumber # Added: For advanced PDF parsing\n",
    "from io import StringIO # To capture print output\n",
    "import string # For character entropy\n",
    "\n",
    "# Ensure consistent language detection results\n",
    "DetectorFactory.seed = 0\n",
    "\n",
    "# --- Configuration ---\n",
    "# Directory containing the PDF files\n",
    "pdf_directory = r'D:\\Dataset\\Lagerugpijn\\LR_EPDs' # Use a raw string for the path\n",
    "output_markdown_file = 'pdf_analysis_report.md' # Name of the output Markdown file\n",
    "PMI_BIGRAM_FREQ_THRESHOLD = 3 # Minimum frequency for a bigram to be included in Average PMI calculation\n",
    "# Removed: JSD_EPSILON = 1e-9 # Small value to prevent log(0) in JSD calculation\n",
    "\n",
    "# --- Suppress Warnings ---\n",
    "# Suppress warnings originating from any pdfminer module (including PDFSyntaxWarning)\n",
    "# This is a more general approach than filtering by a specific warning class name\n",
    "# warnings.filterwarnings(\"ignore\", module='pdfminer\\\\..*')\n",
    "# You could add other filters here if other specific warnings are bothersome, e.g.,\n",
    "# warnings.filterwarnings(\"ignore\", message=\"some specific message\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"CropBox missing from /Page, defaulting to MediaBox\", category=UserWarning)\n",
    "# If the above doesn't catch it, the warning might be a different category or you can remove category\n",
    "# warnings.filterwarnings(\"ignore\", message=\"CropBox missing from /Page, defaulting to MediaBox\")\n",
    "\n",
    "\n",
    "# --- Data Storage ---\n",
    "file_data = [] # List to store dictionaries, each containing data for one file\n",
    "all_extracted_text_for_vocab = \"\" # String to accumulate all text for vocabulary analysis (for overall vocab and PMI)\n",
    "all_extracted_chars = \"\" # String to accumulate all characters (for overall char entropy)\n",
    "\n",
    "report_output = StringIO() # Use StringIO to capture print statements\n",
    "\n",
    "# Redirect stdout to capture print output\n",
    "original_stdout = sys.stdout\n",
    "sys.stdout = report_output\n",
    "\n",
    "# --- Helper Function for Text Extraction and Structural Element Detection ---\n",
    "def analyze_pdf_content(pdf_path):\n",
    "    \"\"\"\n",
    "    Extracts text and detects structural elements (tables, figures, annotations)\n",
    "    from a PDF file using pdfplumber.\n",
    "    Returns extracted text, table count, figure count, annotation count.\n",
    "    \"\"\"\n",
    "    extracted_text = \"\"\n",
    "    table_count = 0\n",
    "    figure_count = 0\n",
    "    annotation_count = 0\n",
    "\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for page in pdf.pages:\n",
    "                # Extract text\n",
    "                extracted_text += page.extract_text() or \"\" # Add text, handle None\n",
    "\n",
    "                # Detect tables\n",
    "                tables = page.extract_tables()\n",
    "                table_count += len(tables)\n",
    "\n",
    "                # Detect figures (images)\n",
    "                figure_count += len(page.images)\n",
    "\n",
    "                # Detect annotations\n",
    "                annotation_count += len(page.annots)\n",
    "\n",
    "    except pdfplumber.PDFSyntaxError as e:\n",
    "         # Print warnings directly to original stdout so they appear during processing\n",
    "         original_stdout.write(f\"Warning: PDF Syntax Error in {os.path.basename(pdf_path)}: {e}\\n\")\n",
    "         # Return empty data for this file if there's a syntax error\n",
    "         return \"\", 0, 0, 0\n",
    "    except Exception as e:\n",
    "        # Print warnings directly to original stdout\n",
    "        original_stdout.write(f\"Warning: Error processing {os.path.basename(pdf_path)} with pdfplumber: {e}\\n\")\n",
    "        # Return empty data for this file if there's any other error\n",
    "        return \"\", 0, 0, 0\n",
    "\n",
    "    return extracted_text, table_count, figure_count, annotation_count\n",
    "\n",
    "\n",
    "# --- Helper Function for Language Detection ---\n",
    "def detect_language(text):\n",
    "    \"\"\"Detects the language of the input text.\"\"\"\n",
    "    if not text.strip():\n",
    "        return \"N/A (No text)\"\n",
    "    try:\n",
    "        # langdetect works best on larger text samples.\n",
    "        # Use a sample if the text is very long to speed things up,\n",
    "        # but for typical documents, using the full text is fine.\n",
    "        # Limit text length for detection to avoid potential issues with very large texts\n",
    "        sample_text = text[:5000] if len(text) > 5000 else text\n",
    "        if not sample_text.strip():\n",
    "             return \"N/A (No text in sample)\"\n",
    "        return detect(sample_text)\n",
    "    except LangDetectException:\n",
    "        return \"Undetected\"\n",
    "    except Exception as e:\n",
    "        # Print warnings directly to original stdout\n",
    "        original_stdout.write(f\"Warning: Error during language detection: {e}\\n\")\n",
    "        return \"Error\"\n",
    "\n",
    "# --- Helper Functions for Information Theory Metrics ---\n",
    "\n",
    "def calculate_shannon_entropy(items):\n",
    "    \"\"\"Calculates Shannon entropy for a list of items (chars or words).\"\"\"\n",
    "    if not items:\n",
    "        return 0.0\n",
    "    counts = Counter(items)\n",
    "    total_items = len(items)\n",
    "    entropy = 0.0\n",
    "    for count in counts.values():\n",
    "        probability = count / total_items\n",
    "        # Add a small epsilon to probability to avoid log(0) if needed, though standard formula handles p > 0\n",
    "        if probability > 0:\n",
    "             entropy -= probability * math.log2(probability)\n",
    "    return entropy\n",
    "\n",
    "# Removed: calculate_kullback_leibler_divergence function\n",
    "# Removed: calculate_jensen_shannon_divergence function\n",
    "\n",
    "def calculate_avg_bigram_pmi(text, min_freq=3):\n",
    "    \"\"\"\n",
    "    Calculates the average Pointwise Mutual Information (PMI) for word bigrams\n",
    "    that occur at least min_freq times.\n",
    "    A proxy metric related to Mutual Information, measuring word association strength.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return 0.0\n",
    "\n",
    "    # Simple word tokenization and lowercase\n",
    "    # Use the same tokenization as the main script for consistency\n",
    "    words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "    if len(words) < 2:\n",
    "        return 0.0\n",
    "\n",
    "    word_counts = Counter(words)\n",
    "    bigram_counts = Counter(zip(words[:-1], words[1:])) # Count occurrences of bigrams\n",
    "\n",
    "    total_words = len(words)\n",
    "    # total_bigrams = len(list(zip(words[:-1], words[1:]))) # Count actual bigram instances\n",
    "\n",
    "    pmi_values = []\n",
    "    for bigram, bigram_count in bigram_counts.items():\n",
    "        # Only consider bigrams that meet the minimum frequency threshold\n",
    "        if bigram_count >= min_freq:\n",
    "            word1, word2 = bigram\n",
    "\n",
    "            # Calculate probabilities (using total_words for marginals is common)\n",
    "            p_w1 = word_counts[word1] / total_words if total_words > 0 else 0\n",
    "            p_w2 = word_counts[word2] / total_words if total_words > 0 else 0\n",
    "            # Use total words as normalization for bigram probability as well for PMI formula\n",
    "            p_w1_w2 = bigram_count / total_words if total_words > 0 else 0\n",
    "\n",
    "            # Avoid log(0) - check if probabilities are positive\n",
    "            if p_w1 > 0 and p_w2 > 0 and p_w1_w2 > 0:\n",
    "                 # PMI formula: log2( P(w1,w2) / (P(w1) * P(w2)) )\n",
    "                 pmi = math.log2(p_w1_w2 / (p_w1 * p_w2))\n",
    "                 pmi_values.append(pmi)\n",
    "            # Note: Bigrams that never appear together with positive marginals would have PMI -infinity.\n",
    "            # We only average over bigrams that *do* appear (with >= min_freq).\n",
    "\n",
    "    if not pmi_values:\n",
    "        return 0.0 # Return 0 if no bigrams meet min_freq or text was empty/too short\n",
    "\n",
    "    return np.mean(pmi_values)\n",
    "\n",
    "\n",
    "# --- Analysis ---\n",
    "# Print initial message to original stdout\n",
    "original_stdout.write(f\"Analyzing PDF files in directory: {pdf_directory}\\n\")\n",
    "\n",
    "\n",
    "if not os.path.isdir(pdf_directory):\n",
    "    # Print error to original stdout\n",
    "    original_stdout.write(f\"Error: Directory not found at {pdf_directory}\\n\")\n",
    "else:\n",
    "    # Iterate through all entries in the directory\n",
    "    for entry_name in os.listdir(pdf_directory):\n",
    "        entry_path = os.path.join(pdf_directory, entry_name)\n",
    "\n",
    "        # Check if the entry is a file and ends with .pdf (case-insensitive)\n",
    "        if os.path.isfile(entry_path) and entry_name.lower().endswith('.pdf'):\n",
    "\n",
    "            file_info = {} # Dictionary to store data for the current file\n",
    "            file_info['filename'] = entry_name\n",
    "            file_info['filepath'] = entry_path\n",
    "\n",
    "            # 1. & 2. Storage Size\n",
    "            try:\n",
    "                file_size_bytes = os.path.getsize(entry_path) # size in bytes\n",
    "                file_info['storage_size_bytes'] = file_size_bytes\n",
    "                file_info['storage_size_mb'] = file_size_bytes / (1024 * 1024)\n",
    "            except Exception as e:\n",
    "                # Print warnings directly to original stdout\n",
    "                original_stdout.write(f\"Warning: Could not get size for {entry_name}: {e}\\n\")\n",
    "                file_info['storage_size_bytes'] = 0\n",
    "                file_info['storage_size_mb'] = 0\n",
    "\n",
    "\n",
    "            # 3. Textual Content Size & Extraction + 7. Structural Elements\n",
    "            text, table_count, figure_count, annotation_count = analyze_pdf_content(entry_path)\n",
    "\n",
    "            file_info['extracted_text'] = text # Store text for language\n",
    "            # Simple tokenization: split by whitespace and punctuation\n",
    "            tokens = re.findall(r'\\b\\w+\\b', text.lower()) # Convert to lower case for vocabulary size\n",
    "            word_count = len(tokens)\n",
    "            file_info['word_count'] = word_count\n",
    "\n",
    "            # Calculate unique words for the current file\n",
    "            unique_words_in_file = set(tokens)\n",
    "            file_info['unique_word_count'] = len(unique_words_in_file)\n",
    "\n",
    "            # Calculate per-file Shannon Entropy\n",
    "            file_info['char_entropy'] = calculate_shannon_entropy(list(text)) # Character entropy\n",
    "            file_info['word_entropy'] = calculate_shannon_entropy(tokens) # Word entropy\n",
    "\n",
    "            # Calculate per-file Average Bigram PMI\n",
    "            file_info['average_pmi'] = calculate_avg_bigram_pmi(text, min_freq=PMI_BIGRAM_FREQ_THRESHOLD)\n",
    "\n",
    "\n",
    "            # Accumulate text and characters for overall vocabulary, PMI, and char entropy later\n",
    "            all_extracted_text_for_vocab += text + \" \" # Add a space to ensure separation\n",
    "            all_extracted_chars += text # Accumulate all characters\n",
    "\n",
    "\n",
    "            # Store structural element counts\n",
    "            file_info['table_count'] = table_count\n",
    "            file_info['figure_count'] = figure_count\n",
    "            file_info['annotation_count'] = annotation_count\n",
    "            # Add boolean flags for easy checking\n",
    "            file_info['has_tables'] = table_count > 0\n",
    "            file_info['has_figures'] = figure_count > 0\n",
    "            file_info['has_annotations'] = annotation_count > 0\n",
    "\n",
    "\n",
    "            # 6. Language Detection\n",
    "            file_info['language'] = detect_language(text)\n",
    "\n",
    "            # 4. Information Content (Estimated)\n",
    "            # Simple estimation based on file size in bits\n",
    "            file_info['estimated_info_content_bits_filesize'] = file_size_bytes * 8\n",
    "\n",
    "\n",
    "            file_data.append(file_info) # Add the file's data to the list\n",
    "\n",
    "\n",
    "    # Removed: --- Calculate Pairwise JSD and Average JSD per file --- section\n",
    "\n",
    "\n",
    "    # --- Calculate Overall Dataset Metrics ---\n",
    "    overall_char_entropy = calculate_shannon_entropy(list(all_extracted_chars))\n",
    "\n",
    "    overall_tokens = re.findall(r'\\b\\w+\\b', all_extracted_text_for_vocab.lower())\n",
    "    overall_word_entropy = calculate_shannon_entropy(overall_tokens)\n",
    "\n",
    "    # Calculate Average Bigram PMI for the overall dataset\n",
    "    # Re-using the calculate_avg_bigram_pmi function for consistency\n",
    "    overall_average_pmi = calculate_avg_bigram_pmi(all_extracted_text_for_vocab, min_freq=PMI_BIGRAM_FREQ_THRESHOLD)\n",
    "\n",
    "\n",
    "    # --- Per-File Reporting (Captured for Markdown) ---\n",
    "    print(\"\\n## Per-File Analysis Results\") # Markdown Heading\n",
    "    if file_data:\n",
    "        # Create a Markdown table for per-file data\n",
    "        # Replaced 'Avg JSD' with 'Avg PMI' column header\n",
    "        print(\"\\n| Filename | Size (MB) | Word Count | Unique Words | Language | Tables | Figures | Annotations | Char Entropy | Word Entropy | Avg PMI |\")\n",
    "        print(\"|---|---|---|---|---|---|---|---|---|---|---|\") # Updated separator line\n",
    "        for file_info in file_data:\n",
    "            # Extract only the digits from the filename\n",
    "            digits_in_filename = re.findall(r'\\d+', file_info['filename'])\n",
    "            last_five_digits = \"\".join(digits_in_filename)[-5:] if digits_in_filename else \"\"\n",
    "\n",
    "            # Determine display filename\n",
    "            display_filename = last_five_digits\n",
    "            # Only add \"...\" if there were other characters besides the last 5 digits\n",
    "            if len(file_info['filename'].replace('.', '').replace('_', '').replace('-', '').replace(' ', '')) > len(last_five_digits):\n",
    "                 display_filename = \"...\" + display_filename\n",
    "\n",
    "\n",
    "            # Replaced file_info['average_jsd'] with file_info['average_pmi'] in the row\n",
    "            print(f\"| {display_filename} | {file_info['storage_size_mb']:.2f} | {file_info['word_count']} | {file_info['unique_word_count']} | {file_info['language']} | {file_info['table_count']} | {file_info['figure_count']} | {file_info['annotation_count']} | {file_info['char_entropy']:.2f} | {file_info['word_entropy']:.2f} | {file_info['average_pmi']:.4f} |\")\n",
    "\n",
    "    else:\n",
    "        print(\"No PDF files found in the specified directory.\")\n",
    "\n",
    "\n",
    "    # --- Overall Dataset Summary (Captured for Markdown) ---\n",
    "    print(\"\\n## Overall Dataset Summary\") # Markdown Heading\n",
    "\n",
    "    total_files = len(file_data)\n",
    "    print(f\"\\n### 1. Number of PDF Files: {total_files}\") # Markdown Subheading\n",
    "\n",
    "    if total_files > 0:\n",
    "        # 2. Storage Size\n",
    "        all_storage_sizes_bytes = [f['storage_size_bytes'] for f in file_data]\n",
    "        total_storage_size_bytes = sum(all_storage_sizes_bytes)\n",
    "        total_storage_size_mb = total_storage_size_bytes / (1024 * 1024)\n",
    "        print(f\"\\n### 2. Storage Size\") # Markdown Subheading\n",
    "        print(f\"- **Total:** {total_storage_size_mb:.2f} MB ({total_storage_size_bytes} bytes)\")\n",
    "        if all_storage_sizes_bytes:\n",
    "            avg_file_size_mb = statistics.mean(all_storage_sizes_bytes) / (1024 * 1024)\n",
    "            min_file_size_mb = min(all_storage_sizes_bytes) / (1024 * 1024)\n",
    "            max_file_size_mb = max(all_storage_sizes_bytes) / (1024 * 1024)\n",
    "            print(f\"- **Average:** {avg_file_size_mb:.2f} MB\")\n",
    "            print(f\"- **Range:** ({min_file_size_mb:.2f} MB, {max_file_size_mb:.2f} MB)\")\n",
    "\n",
    "        # 3. Textual Content Size\n",
    "        all_word_counts = [f['word_count'] for f in file_data]\n",
    "        total_word_count = sum(all_word_counts)\n",
    "        print(f\"\\n### 3. Textual Content Size\") # Markdown Subheading\n",
    "        print(f\"- **Total words/tokens:** {total_word_count}\")\n",
    "        if all_word_counts:\n",
    "            avg_word_count = statistics.mean(all_word_counts)\n",
    "            print(f\"- **Average words/tokens per document:** {avg_word_count:.2f}\")\n",
    "\n",
    "            # Calculate unique tokens (vocabulary size) from accumulated text\n",
    "            all_tokens = re.findall(r'\\b\\w+\\b', all_extracted_text_for_vocab.lower())\n",
    "            unique_tokens = set(all_tokens)\n",
    "            vocabulary_size = len(unique_tokens)\n",
    "            print(f\"- **Unique tokens across dataset (vocabulary size):** {vocabulary_size}\") # Clarified label\n",
    "        else:\n",
    "             print(f\"- **Average words/tokens per document:** 0\")\n",
    "             print(f\"- **Unique tokens across dataset (vocabulary size):** 0\")\n",
    "\n",
    "        # 4. Information Content (Estimated)\n",
    "        total_estimated_info_content_bits_filesize = total_storage_size_bytes * 8\n",
    "        print(f\"\\n### 4. Information Content (Estimated)\") # Markdown Subheading\n",
    "        print(f\"- **Total estimated info content (based on total file size):** {total_estimated_info_content_bits_filesize} bits\")\n",
    "        print(f\"- *Note: This estimate is based on raw file size in bits.*\")\n",
    "\n",
    "        # 4b. Information Theory Metrics (Overall Dataset)\n",
    "        print(f\"\\n### 4b. Information Theory Metrics (Overall Dataset)\") # Markdown Subheading\n",
    "        print(f\"- **Overall Character Entropy:** {overall_char_entropy:.2f} bits/character\")\n",
    "        print(f\"- **Overall Word Entropy:** {overall_word_entropy:.2f} bits/word\")\n",
    "        # Replaced overall average PMI calculation logic with a call to the new function\n",
    "        print(f\"- **Average Bigram PMI (Threshold={PMI_BIGRAM_FREQ_THRESHOLD}):** {overall_average_pmi:.4f}\")\n",
    "        print(f\"  *Note: Average PMI is calculated for bigrams appearing at least {PMI_BIGRAM_FREQ_THRESHOLD} times across the dataset.*\")\n",
    "\n",
    "\n",
    "        # 5. Document Length Distribution\n",
    "        print(f\"\\n### 5. Document Length Distribution (in words/tokens)\") # Markdown Subheading\n",
    "        if all_word_counts:\n",
    "            mean_len = statistics.mean(all_word_counts)\n",
    "            median_len = statistics.median(all_word_counts)\n",
    "            # Ensure std dev calculation is valid for >1 data points\n",
    "            std_dev_len = statistics.stdev(all_word_counts) if len(all_word_counts) > 1 else 0\n",
    "            print(f\"- **Mean length:** {mean_len:.2f}\")\n",
    "            print(f\"- **Median length:** {median_len}\")\n",
    "            print(f\"- **Standard deviation:** {std_dev_len:.2f}\")\n",
    "\n",
    "\n",
    "        # 6. Language Distribution\n",
    "        print(f\"\\n### 6. Language Distribution\") # Markdown Subheading\n",
    "        all_languages = [f['language'] for f in file_data]\n",
    "        language_counts = Counter(all_languages)\n",
    "        print(\"- **Counts:**\")\n",
    "        for lang, count in language_counts.most_common():\n",
    "            print(f\"   - {lang}: {count} files\")\n",
    "\n",
    "\n",
    "        # 7. Structural Elements Summary\n",
    "        print(f\"\\n### 7. Structural Elements Summary\") # Markdown Subheading\n",
    "        total_tables_found = sum([f['table_count'] for f in file_data])\n",
    "        total_figures_found = sum([f['figure_count'] for f in file_data])\n",
    "        total_annotations_found = sum([f['annotation_count'] for f in file_data])\n",
    "\n",
    "        files_with_tables = sum([f['has_tables'] for f in file_data])\n",
    "        files_with_figures = sum([f['has_figures'] for f in file_data])\n",
    "        files_with_annotations = sum([f['has_annotations'] for f in file_data])\n",
    "\n",
    "        print(f\"- **Total Tables found:** {total_tables_found}\")\n",
    "        print(f\"- **Total Figures/Images found:** {total_figures_found}\")\n",
    "        print(f\"- **Total Annotations found:** {total_annotations_found}\")\n",
    "        print(f\"- **Files with Tables:** {files_with_tables} ({files_with_tables/total_files*100:.2f}%)\")\n",
    "        print(f\"- **Files with Figures/Images:** {files_with_figures} ({files_with_figures/total_files*100:.2f}%)\")\n",
    "        print(f\"- **Files with Annotations:** {files_with_annotations} ({files_with_annotations/total_files*100:.2f}%)\")\n",
    "\n",
    "\n",
    "    else:\n",
    "        print(\"No data collected from PDF files.\")\n",
    "\n",
    "\n",
    "# --- End of Captured Output ---\n",
    "sys.stdout = original_stdout # Restore stdout\n",
    "sys.stdout.flush() # Explicitly flush the buffer\n",
    "\n",
    "\n",
    "# --- Display Report in Console ---\n",
    "markdown_content = report_output.getvalue() # Get the captured output\n",
    "print(\"\\n\" + \"=\"*30 + \" PDF Analysis Report \" + \"=\"*30) # Separator\n",
    "print(\"\\n*Note: Markdown rendering may vary depending on the viewer. Multi-column layout and font size control are not standard Markdown features.*\\n\")\n",
    "print(markdown_content) # Print the captured report content to console\n",
    "print(\"=\"*79) # Separator\n",
    "\n",
    "\n",
    "# --- Save Output to Markdown File ---\n",
    "try:\n",
    "    with open(output_markdown_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(markdown_content)\n",
    "    print(f\"\\nAnalysis report also saved to {output_markdown_file}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nError saving report to {output_markdown_file}: {e}\")\n",
    "\n",
    "report_output.close() # Close the StringIO object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "76a1973a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:28: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:28: SyntaxWarning: invalid escape sequence '\\.'\n",
      "C:\\Users\\PROMET02\\AppData\\Local\\Temp\\ipykernel_66240\\1956331218.py:28: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  warnings.filterwarnings(\"ignore\", module='pdfminer\\..*')\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing PDF files in directory: D:\\Dataset\\Lagerugpijn\\LR_EPDs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================== PDF Analysis Report ==============================\n",
      "\n",
      "*Note: Markdown rendering may vary depending on the viewer. Multi-column layout and font size control are not standard Markdown features.*\n",
      "\n",
      "\n",
      "## Per-File Analysis Results\n",
      "\n",
      "| Filename | Size (MB) | Word Count | Unique Words | doc length | Language | Tables | Figures | Annotations | Char Entropy | Word Entropy | Avg PMI |\n",
      "|---|---|---|---|---|---|---|---|---|---|---|---|\n",
      "| ...59037 | 0.20 | 1766 | 453 | 12081 | nl | 3 | 0 | 0 | 4.89 | 8.15 | 6.9683 |\n",
      "| ...59684 | 0.19 | 1589 | 543 | 11178 | nl | 3 | 0 | 0 | 4.90 | 8.54 | 6.9370 |\n",
      "| ...60038 | 0.17 | 1452 | 493 | 10095 | nl | 3 | 0 | 0 | 4.85 | 8.39 | 6.7113 |\n",
      "| ...60384 | 0.17 | 1667 | 472 | 10761 | nl | 2 | 0 | 0 | 4.83 | 8.14 | 6.5174 |\n",
      "| ...60818 | 0.17 | 1344 | 474 | 9412 | nl | 3 | 0 | 0 | 4.93 | 8.35 | 6.2739 |\n",
      "| ...61014 | 0.20 | 1605 | 507 | 11118 | nl | 4 | 0 | 0 | 4.88 | 8.43 | 7.1602 |\n",
      "| ...61368 | 0.19 | 1888 | 532 | 12109 | nl | 3 | 0 | 0 | 4.89 | 8.32 | 6.7119 |\n",
      "| ...61665 | 0.16 | 1554 | 446 | 10287 | nl | 2 | 0 | 0 | 4.81 | 8.03 | 6.2304 |\n",
      "| ...61810 | 0.19 | 2176 | 651 | 14554 | nl | 2 | 0 | 0 | 4.83 | 8.49 | 6.5144 |\n",
      "| ...62117 | 0.16 | 1360 | 427 | 8873 | nl | 3 | 0 | 0 | 4.87 | 8.08 | 6.2708 |\n",
      "| ...62177 | 0.17 | 1367 | 480 | 9544 | nl | 3 | 0 | 0 | 4.97 | 8.39 | 6.7884 |\n",
      "| ...62655 | 0.16 | 1581 | 454 | 10316 | nl | 2 | 0 | 0 | 4.79 | 8.04 | 6.3726 |\n",
      "| ...63175 | 0.15 | 1566 | 501 | 10358 | nl | 2 | 0 | 0 | 4.82 | 8.26 | 6.2312 |\n",
      "\n",
      "## Overall Dataset Summary\n",
      "\n",
      "### 1. Number of PDF Files: 13\n",
      "\n",
      "### 2. Storage Size\n",
      "- **Total:** 2.28 MB (2395567 bytes)\n",
      "- **Average:** 0.18 MB\n",
      "- **Range:** (0.15 MB, 0.20 MB)\n",
      "\n",
      "### 3. Textual Content Size\n",
      "- **Total words/tokens:** 20915\n",
      "- **Average words/tokens per document:** 1608.85\n",
      "- **Unique tokens across dataset (vocabulary size):** 1667\n",
      "\n",
      "### 4. Information Content (Estimated)\n",
      "- **Total estimated info content (based on total file size):** 19164536 bits\n",
      "- *Note: This estimate is based on raw file size in bits.*\n",
      "\n",
      "### 4b. Information Theory Metrics (Overall Dataset)\n",
      "- **Overall Character Entropy:** 4.88 bits/character\n",
      "- **Overall Word Entropy:** 9.16 bits/word\n",
      "- **Average Bigram PMI (Threshold=3):** 6.6699\n",
      "  *Note: Average PMI is calculated for bigrams appearing at least 3 times across the dataset.*\n",
      "- **Overall Average Document Length (Characters):** 10822.00\n",
      "\n",
      "### 5. Document Length Distribution (in words/tokens)\n",
      "- **Mean length:** 1608.85\n",
      "- **Median length:** 1581\n",
      "- **Standard deviation:** 232.53\n",
      "\n",
      "### 6. Language Distribution\n",
      "- **Counts:**\n",
      "   - nl: 13 files\n",
      "\n",
      "### 7. Structural Elements Summary\n",
      "- **Total Tables found:** 35\n",
      "- **Total Figures/Images found:** 0\n",
      "- **Total Annotations found:** 0\n",
      "- **Files with Tables:** 13 (100.00%)\n",
      "- **Files with Figures/Images:** 0 (0.00%)\n",
      "- **Files with Annotations:** 0 (0.00%)\n",
      "\n",
      "===============================================================================\n",
      "\n",
      "Analysis report also saved to pdf_analysis_report.md\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import statistics\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import re # For tokenization\n",
    "from langdetect import detect, DetectorFactory # For language detection\n",
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "import pdfplumber # Added: For advanced PDF parsing\n",
    "from io import StringIO # To capture print output\n",
    "import string # For character entropy\n",
    "import warnings # Added: To manage warnings\n",
    "# Removed: from pdfminer.pdfparser import PDFSyntaxWarning # Removed: Specific import causing error\n",
    "\n",
    "# Ensure consistent language detection results\n",
    "DetectorFactory.seed = 0\n",
    "\n",
    "# --- Configuration ---\n",
    "# Directory containing the PDF files\n",
    "pdf_directory = r'D:\\Dataset\\Lagerugpijn\\LR_EPDs' # Use a raw string for the path\n",
    "output_markdown_file = 'pdf_analysis_report.md' # Name of the output Markdown file\n",
    "PMI_BIGRAM_FREQ_THRESHOLD = 3 # Minimum frequency for a bigram to be included in Average PMI calculation\n",
    "\n",
    "# --- Suppress Warnings ---\n",
    "# Suppress warnings originating from any pdfminer module (including PDFSyntaxWarning)\n",
    "# This is a more general approach than filtering by a specific warning class name\n",
    "warnings.filterwarnings(\"ignore\", module='pdfminer\\..*')\n",
    "# You could add other filters here if other specific warnings are bothersome, e.g.,\n",
    "# warnings.filterwarnings(\"ignore\", message=\"some specific message\")\n",
    "\n",
    "# --- Data Storage ---\n",
    "file_data = [] # List to store dictionaries, each containing data for one file\n",
    "all_extracted_text_for_vocab = \"\" # String to accumulate all text for vocabulary analysis (for overall vocab and PMI)\n",
    "all_extracted_chars = \"\" # String to accumulate all characters (for overall char entropy)\n",
    "all_char_counts = [] # List to store character counts for each file (for overall average)\n",
    "\n",
    "report_output = StringIO() # Use StringIO to capture print statements\n",
    "\n",
    "# Redirect stdout to capture print output\n",
    "original_stdout = sys.stdout\n",
    "sys.stdout = report_output\n",
    "\n",
    "# --- Helper Function for Text Extraction and Structural Element Detection ---\n",
    "def analyze_pdf_content(pdf_path):\n",
    "    \"\"\"\n",
    "    Extracts text and detects structural elements (tables, figures, annotations)\n",
    "    from a PDF file using pdfplumber.\n",
    "    Returns extracted text, table count, figure count, annotation count.\n",
    "    \"\"\"\n",
    "    extracted_text = \"\"\n",
    "    table_count = 0\n",
    "    figure_count = 0\n",
    "    annotation_count = 0\n",
    "\n",
    "    try:\n",
    "        # Use warnings.catch_warnings to temporarily manage warnings within this function if needed,\n",
    "        # but filtering globally at the start is simpler for this case.\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for page in pdf.pages:\n",
    "                # Extract text\n",
    "                extracted_text += page.extract_text() or \"\" # Add text, handle None\n",
    "\n",
    "                # Detect tables\n",
    "                tables = page.extract_tables()\n",
    "                table_count += len(tables)\n",
    "\n",
    "                # Detect figures (images)\n",
    "                figure_count += len(page.images)\n",
    "\n",
    "                # Detect annotations\n",
    "                annotation_count += len(page.annots)\n",
    "\n",
    "    except pdfplumber.PDFSyntaxError as e:\n",
    "         # Print warnings directly to original stdout so they appear during processing\n",
    "         original_stdout.write(f\"Warning: PDF Syntax Error in {os.path.basename(pdf_path)}: {e}\\n\")\n",
    "         # Return empty data for this file if there's a syntax error\n",
    "         return \"\", 0, 0, 0\n",
    "    except Exception as e:\n",
    "        # Print warnings directly to original stdout\n",
    "        original_stdout.write(f\"Warning: Error processing {os.path.basename(pdf_path)} with pdfplumber: {e}\\n\")\n",
    "        # Return empty data for this file if there's any other error\n",
    "        return \"\", 0, 0, 0\n",
    "\n",
    "    return extracted_text, table_count, figure_count, annotation_count\n",
    "\n",
    "\n",
    "# --- Helper Function for Language Detection ---\n",
    "def detect_language(text):\n",
    "    \"\"\"Detects the language of the input text.\"\"\"\n",
    "    if not text.strip():\n",
    "        return \"N/A (No text)\"\n",
    "    try:\n",
    "        # langdetect works best on larger text samples.\n",
    "        # Use a sample if the text is very long to speed things up,\n",
    "        # but for typical documents, using the full text is fine.\n",
    "        # Limit text length for detection to avoid potential issues with very large texts\n",
    "        sample_text = text[:5000] if len(text) > 5000 else text\n",
    "        if not sample_text.strip():\n",
    "             return \"N/A (No text in sample)\"\n",
    "        # Use warnings.catch_warnings to temporarily manage warnings within this function if needed\n",
    "        with warnings.catch_warnings():\n",
    "             # langdetect can sometimes issue warnings, e.g., about language probabilities\n",
    "             warnings.filterwarnings(\"ignore\", category=UserWarning) # Example: suppress UserWarnings from langdetect\n",
    "             return detect(sample_text)\n",
    "    except LangDetectException:\n",
    "        return \"Undetected\"\n",
    "    except Exception as e:\n",
    "        # Print warnings directly to original stdout\n",
    "        original_stdout.write(f\"Warning: Error during language detection: {e}\\n\")\n",
    "        return \"Error\"\n",
    "\n",
    "# --- Helper Functions for Information Theory Metrics ---\n",
    "\n",
    "def calculate_shannon_entropy(items):\n",
    "    \"\"\"Calculates Shannon entropy for a list of items (chars or words).\"\"\"\n",
    "    if not items:\n",
    "        return 0.0\n",
    "    counts = Counter(items)\n",
    "    total_items = len(items)\n",
    "    entropy = 0.0\n",
    "    for count in counts.values():\n",
    "        probability = count / total_items\n",
    "        # Add a small epsilon to probability to avoid log(0) if needed, though standard formula handles p > 0\n",
    "        if probability > 0:\n",
    "             entropy -= probability * math.log2(probability)\n",
    "    return entropy\n",
    "\n",
    "# Removed: calculate_kullback_leibler_divergence function\n",
    "# Removed: calculate_jensen_shannon_divergence function\n",
    "\n",
    "def calculate_avg_bigram_pmi(text, min_freq=3):\n",
    "    \"\"\"\n",
    "    Calculates the average Pointwise Mutual Information (PMI) for word bigrams\n",
    "    that occur at least min_freq times.\n",
    "    A proxy metric related to Mutual Information, measuring word association strength.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return 0.0\n",
    "\n",
    "    # Simple word tokenization and lowercase\n",
    "    # Use the same tokenization as the main script for consistency\n",
    "    words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "    if len(words) < 2:\n",
    "        return 0.0\n",
    "\n",
    "    word_counts = Counter(words)\n",
    "    bigram_counts = Counter(zip(words[:-1], words[1:])) # Count occurrences of bigrams\n",
    "\n",
    "    total_words = len(words)\n",
    "    # total_bigrams = len(list(zip(words[:-1], words[1:]))) # Count actual bigram instances\n",
    "\n",
    "    pmi_values = []\n",
    "    for bigram, bigram_count in bigram_counts.items():\n",
    "        # Only consider bigrams that meet the minimum frequency threshold\n",
    "        if bigram_count >= min_freq:\n",
    "            word1, word2 = bigram\n",
    "\n",
    "            # Calculate probabilities (using total_words for marginals is common)\n",
    "            p_w1 = word_counts[word1] / total_words if total_words > 0 else 0\n",
    "            p_w2 = word_counts[word2] / total_words if total_words > 0 else 0\n",
    "            # Use total words as normalization for bigram probability as well for PMI formula\n",
    "            p_w1_w2 = bigram_count / total_words if total_words > 0 else 0\n",
    "\n",
    "            # Avoid log(0) - check if probabilities are positive\n",
    "            if p_w1 > 0 and p_w2 > 0 and p_w1_w2 > 0:\n",
    "                 # PMI formula: log2( P(w1,w2) / (P(w1) * P(w2)) )\n",
    "                 pmi = math.log2(p_w1_w2 / (p_w1 * p_w2))\n",
    "                 pmi_values.append(pmi)\n",
    "            # Note: Bigrams that never appear together with positive marginals would have PMI -infinity.\n",
    "            # We only average over bigrams that *do* appear (with >= min_freq).\n",
    "\n",
    "    if not pmi_values:\n",
    "        return 0.0 # Return 0 if no bigrams meet min_freq or text was empty/too short\n",
    "\n",
    "    return np.mean(pmi_values)\n",
    "\n",
    "\n",
    "# --- Analysis ---\n",
    "# Print initial message to original stdout\n",
    "original_stdout.write(f\"Analyzing PDF files in directory: {pdf_directory}\\n\")\n",
    "\n",
    "\n",
    "if not os.path.isdir(pdf_directory):\n",
    "    # Print error to original stdout\n",
    "    original_stdout.write(f\"Error: Directory not found at {pdf_directory}\\n\")\n",
    "else:\n",
    "    # Iterate through all entries in the directory\n",
    "    for entry_name in os.listdir(pdf_directory):\n",
    "        entry_path = os.path.join(pdf_directory, entry_name)\n",
    "\n",
    "        # Check if the entry is a file and ends with .pdf (case-insensitive)\n",
    "        if os.path.isfile(entry_path) and entry_name.lower().endswith('.pdf'):\n",
    "\n",
    "            file_info = {} # Dictionary to store data for the current file\n",
    "            file_info['filename'] = entry_name\n",
    "            file_info['filepath'] = entry_path\n",
    "\n",
    "            # 1. & 2. Storage Size\n",
    "            try:\n",
    "                file_size_bytes = os.path.getsize(entry_path) # size in bytes\n",
    "                file_info['storage_size_bytes'] = file_size_bytes\n",
    "                file_info['storage_size_mb'] = file_size_bytes / (1024 * 1024)\n",
    "            except Exception as e:\n",
    "                # Print warnings directly to original stdout\n",
    "                original_stdout.write(f\"Warning: Could not get size for {entry_name}: {e}\\n\")\n",
    "                file_info['storage_size_bytes'] = 0\n",
    "                file_info['storage_size_mb'] = 0\n",
    "\n",
    "\n",
    "            # 3. Textual Content Size & Extraction + 7. Structural Elements\n",
    "            text, table_count, figure_count, annotation_count = analyze_pdf_content(entry_path)\n",
    "\n",
    "            file_info['extracted_text'] = text # Store text for language\n",
    "            # Calculate character count for the current file\n",
    "            file_info['char_count'] = len(text)\n",
    "            all_char_counts.append(file_info['char_count']) # Add to list for overall average\n",
    "\n",
    "            # Simple word tokenization and lowercase\n",
    "            tokens = re.findall(r'\\b\\w+\\b', text.lower()) # Convert to lower case for vocabulary size\n",
    "            word_count = len(tokens)\n",
    "            file_info['word_count'] = word_count\n",
    "\n",
    "            # Calculate unique words for the current file\n",
    "            unique_words_in_file = set(tokens)\n",
    "            file_info['unique_word_count'] = len(unique_words_in_file)\n",
    "\n",
    "            # Calculate per-file Shannon Entropy\n",
    "            file_info['char_entropy'] = calculate_shannon_entropy(list(text)) # Character entropy\n",
    "            file_info['word_entropy'] = calculate_shannon_entropy(tokens) # Word entropy\n",
    "\n",
    "            # Calculate per-file Average Bigram PMI\n",
    "            file_info['average_pmi'] = calculate_avg_bigram_pmi(text, min_freq=PMI_BIGRAM_FREQ_THRESHOLD)\n",
    "\n",
    "\n",
    "            # Accumulate text and characters for overall vocabulary, PMI, and char entropy later\n",
    "            all_extracted_text_for_vocab += text + \" \" # Add a space to ensure separation\n",
    "            all_extracted_chars += text # Accumulate all characters\n",
    "\n",
    "\n",
    "            # Store structural element counts\n",
    "            file_info['table_count'] = table_count\n",
    "            file_info['figure_count'] = figure_count\n",
    "            file_info['annotation_count'] = annotation_count\n",
    "            # Add boolean flags for easy checking\n",
    "            file_info['has_tables'] = table_count > 0\n",
    "            file_info['has_figures'] = figure_count > 0\n",
    "            file_info['has_annotations'] = annotation_count > 0\n",
    "\n",
    "\n",
    "            # 6. Language Detection\n",
    "            file_info['language'] = detect_language(text)\n",
    "\n",
    "            # 4. Information Content (Estimated)\n",
    "            # Simple estimation based on file size in bits\n",
    "            file_info['estimated_info_content_bits_filesize'] = file_size_bytes * 8\n",
    "\n",
    "\n",
    "            file_data.append(file_info) # Add the file's data to the list\n",
    "\n",
    "\n",
    "    # Removed: --- Calculate Pairwise JSD and Average JSD per file --- section\n",
    "\n",
    "\n",
    "    # --- Calculate Overall Dataset Metrics ---\n",
    "    overall_char_entropy = calculate_shannon_entropy(list(all_extracted_chars))\n",
    "\n",
    "    overall_tokens = re.findall(r'\\b\\w+\\b', all_extracted_text_for_vocab.lower())\n",
    "    overall_word_entropy = calculate_shannon_entropy(overall_tokens)\n",
    "\n",
    "    # Calculate Average Bigram PMI for the overall dataset\n",
    "    # Re-using the calculate_avg_bigram_pmi function for consistency\n",
    "    overall_average_pmi = calculate_avg_bigram_pmi(all_extracted_text_for_vocab, min_freq=PMI_BIGRAM_FREQ_THRESHOLD)\n",
    "\n",
    "    # Calculate Overall Average Document Length (Characters)\n",
    "    overall_avg_doc_length_chars = np.mean(all_char_counts) if all_char_counts else 0\n",
    "\n",
    "\n",
    "    # --- Per-File Reporting (Captured for Markdown) ---\n",
    "    print(\"\\n## Per-File Analysis Results\") # Markdown Heading\n",
    "    if file_data:\n",
    "        # Create a Markdown table for per-file data\n",
    "        # Added 'doc length' column header\n",
    "        print(\"\\n| Filename | Size (MB) | Word Count | Unique Words | doc length | Language | Tables | Figures | Annotations | Char Entropy | Word Entropy | Avg PMI |\")\n",
    "        # Updated separator line to match the new column count\n",
    "        print(\"|---|---|---|---|---|---|---|---|---|---|---|---|\")\n",
    "        for file_info in file_data:\n",
    "            # Extract only the digits from the filename\n",
    "            digits_in_filename = re.findall(r'\\d+', file_info['filename'])\n",
    "            last_five_digits = \"\".join(digits_in_filename)[-5:] if digits_in_filename else \"\"\n",
    "\n",
    "            # Determine display filename\n",
    "            display_filename = last_five_digits\n",
    "            # Only add \"...\" if there were other characters besides the last 5 digits\n",
    "            if len(file_info['filename'].replace('.', '').replace('_', '').replace('-', '').replace(' ', '')) > len(last_five_digits):\n",
    "                 display_filename = \"...\" + display_filename\n",
    "\n",
    "\n",
    "            # Added file_info['char_count'] to the row\n",
    "            print(f\"| {display_filename} | {file_info['storage_size_mb']:.2f} | {file_info['word_count']} | {file_info['unique_word_count']} | {file_info['char_count']} | {file_info['language']} | {file_info['table_count']} | {file_info['figure_count']} | {file_info['annotation_count']} | {file_info['char_entropy']:.2f} | {file_info['word_entropy']:.2f} | {file_info['average_pmi']:.4f} |\")\n",
    "\n",
    "    else:\n",
    "        print(\"No PDF files found in the specified directory.\")\n",
    "\n",
    "\n",
    "    # --- Overall Dataset Summary (Captured for Markdown) ---\n",
    "    print(\"\\n## Overall Dataset Summary\") # Markdown Heading\n",
    "\n",
    "    total_files = len(file_data)\n",
    "    print(f\"\\n### 1. Number of PDF Files: {total_files}\") # Markdown Subheading\n",
    "\n",
    "    if total_files > 0:\n",
    "        # 2. Storage Size\n",
    "        all_storage_sizes_bytes = [f['storage_size_bytes'] for f in file_data]\n",
    "        total_storage_size_bytes = sum(all_storage_sizes_bytes)\n",
    "        total_storage_size_mb = total_storage_size_bytes / (1024 * 1024)\n",
    "        print(f\"\\n### 2. Storage Size\") # Markdown Subheading\n",
    "        print(f\"- **Total:** {total_storage_size_mb:.2f} MB ({total_storage_size_bytes} bytes)\")\n",
    "        if all_storage_sizes_bytes:\n",
    "            avg_file_size_mb = statistics.mean(all_storage_sizes_bytes) / (1024 * 1024)\n",
    "            min_file_size_mb = min(all_storage_sizes_bytes) / (1024 * 1024)\n",
    "            max_file_size_mb = max(all_storage_sizes_bytes) / (1024 * 1024)\n",
    "            print(f\"- **Average:** {avg_file_size_mb:.2f} MB\")\n",
    "            print(f\"- **Range:** ({min_file_size_mb:.2f} MB, {max_file_size_mb:.2f} MB)\")\n",
    "\n",
    "        # 3. Textual Content Size\n",
    "        all_word_counts = [f['word_count'] for f in file_data]\n",
    "        total_word_count = sum(all_word_counts)\n",
    "        print(f\"\\n### 3. Textual Content Size\") # Markdown Subheading\n",
    "        print(f\"- **Total words/tokens:** {total_word_count}\")\n",
    "        if all_word_counts:\n",
    "            avg_word_count = statistics.mean(all_word_counts)\n",
    "            print(f\"- **Average words/tokens per document:** {avg_word_count:.2f}\")\n",
    "\n",
    "            # Calculate unique tokens (vocabulary size) from accumulated text\n",
    "            all_tokens = re.findall(r'\\b\\w+\\b', all_extracted_text_for_vocab.lower())\n",
    "            unique_tokens = set(all_tokens)\n",
    "            vocabulary_size = len(unique_tokens)\n",
    "            print(f\"- **Unique tokens across dataset (vocabulary size):** {vocabulary_size}\") # Clarified label\n",
    "        else:\n",
    "             print(f\"- **Average words/tokens per document:** 0\")\n",
    "             print(f\"- **Unique tokens across dataset (vocabulary size):** 0\")\n",
    "\n",
    "        # 4. Information Content (Estimated)\n",
    "        total_estimated_info_content_bits_filesize = total_storage_size_bytes * 8\n",
    "        print(f\"\\n### 4. Information Content (Estimated)\") # Markdown Subheading\n",
    "        print(f\"- **Total estimated info content (based on total file size):** {total_estimated_info_content_bits_filesize} bits\")\n",
    "        print(f\"- *Note: This estimate is based on raw file size in bits.*\")\n",
    "\n",
    "        # 4b. Information Theory Metrics (Overall Dataset)\n",
    "        print(f\"\\n### 4b. Information Theory Metrics (Overall Dataset)\") # Markdown Subheading\n",
    "        print(f\"- **Overall Character Entropy:** {overall_char_entropy:.2f} bits/character\")\n",
    "        print(f\"- **Overall Word Entropy:** {overall_word_entropy:.2f} bits/word\")\n",
    "        # Replaced overall average PMI calculation logic with a call to the new function\n",
    "        print(f\"- **Average Bigram PMI (Threshold={PMI_BIGRAM_FREQ_THRESHOLD}):** {overall_average_pmi:.4f}\")\n",
    "        print(f\"  *Note: Average PMI is calculated for bigrams appearing at least {PMI_BIGRAM_FREQ_THRESHOLD} times across the dataset.*\")\n",
    "\n",
    "        # Added Overall Average Document Length (Characters)\n",
    "        print(f\"- **Overall Average Document Length (Characters):** {overall_avg_doc_length_chars:.2f}\")\n",
    "\n",
    "\n",
    "        # 5. Document Length Distribution\n",
    "        print(f\"\\n### 5. Document Length Distribution (in words/tokens)\") # Markdown Subheading\n",
    "        if all_word_counts:\n",
    "            mean_len = statistics.mean(all_word_counts)\n",
    "            median_len = statistics.median(all_word_counts)\n",
    "            # Ensure std dev calculation is valid for >1 data points\n",
    "            std_dev_len = statistics.stdev(all_word_counts) if len(all_word_counts) > 1 else 0\n",
    "            print(f\"- **Mean length:** {mean_len:.2f}\")\n",
    "            print(f\"- **Median length:** {median_len}\")\n",
    "            print(f\"- **Standard deviation:** {std_dev_len:.2f}\")\n",
    "\n",
    "\n",
    "        # 6. Language Distribution\n",
    "        print(f\"\\n### 6. Language Distribution\") # Markdown Subheading\n",
    "        all_languages = [f['language'] for f in file_data]\n",
    "        language_counts = Counter(all_languages)\n",
    "        print(\"- **Counts:**\")\n",
    "        for lang, count in language_counts.most_common():\n",
    "            print(f\"   - {lang}: {count} files\")\n",
    "\n",
    "\n",
    "        # 7. Structural Elements Summary\n",
    "        print(f\"\\n### 7. Structural Elements Summary\") # Markdown Subheading\n",
    "        total_tables_found = sum([f['table_count'] for f in file_data])\n",
    "        total_figures_found = sum([f['figure_count'] for f in file_data])\n",
    "        total_annotations_found = sum([f['annotation_count'] for f in file_data])\n",
    "\n",
    "        files_with_tables = sum([f['has_tables'] for f in file_data])\n",
    "        files_with_figures = sum([f['has_figures'] for f in file_data])\n",
    "        files_with_annotations = sum([f['has_annotations'] for f in file_data])\n",
    "\n",
    "        print(f\"- **Total Tables found:** {total_tables_found}\")\n",
    "        print(f\"- **Total Figures/Images found:** {total_figures_found}\")\n",
    "        print(f\"- **Total Annotations found:** {total_annotations_found}\")\n",
    "        print(f\"- **Files with Tables:** {files_with_tables} ({files_with_tables/total_files*100:.2f}%)\")\n",
    "        print(f\"- **Files with Figures/Images:** {files_with_figures} ({files_with_figures/total_files*100:.2f}%)\")\n",
    "        print(f\"- **Files with Annotations:** {files_with_annotations} ({files_with_annotations/total_files*100:.2f}%)\")\n",
    "\n",
    "\n",
    "    else:\n",
    "        print(\"No data collected from PDF files.\")\n",
    "\n",
    "\n",
    "# --- End of Captured Output ---\n",
    "sys.stdout = original_stdout # Restore stdout\n",
    "sys.stdout.flush() # Explicitly flush the buffer\n",
    "\n",
    "\n",
    "# --- Display Report in Console ---\n",
    "markdown_content = report_output.getvalue() # Get the captured output\n",
    "print(\"\\n\" + \"=\"*30 + \" PDF Analysis Report \" + \"=\"*30) # Separator\n",
    "print(\"\\n*Note: Markdown rendering may vary depending on the viewer. Multi-column layout and font size control are not standard Markdown features.*\\n\")\n",
    "print(markdown_content) # Print the captured report content to console\n",
    "print(\"=\"*79) # Separator\n",
    "\n",
    "\n",
    "# --- Save Output to Markdown File ---\n",
    "try:\n",
    "    with open(output_markdown_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(markdown_content)\n",
    "    print(f\"\\nAnalysis report also saved to {output_markdown_file}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nError saving report to {output_markdown_file}: {e}\")\n",
    "\n",
    "report_output.close() # Close the StringIO object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8c01f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecf8a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import statistics\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import re # For tokenization\n",
    "from langdetect import detect, DetectorFactory # For language detection\n",
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "import pdfplumber # Added: For advanced PDF parsing\n",
    "from io import StringIO # To capture print output\n",
    "import string # For character entropy\n",
    "import warnings # Added: To manage warnings\n",
    "# Removed: from pdfminer.pdfparser import PDFSyntaxWarning # Removed: Specific import causing error\n",
    "from scipy.stats import entropy # Added: For KL divergence calculation\n",
    "\n",
    "# Ensure consistent language detection results\n",
    "DetectorFactory.seed = 0\n",
    "\n",
    "# --- Configuration ---\n",
    "# Directory containing the PDF files\n",
    "pdf_directory = r'D:\\Dataset\\Lagerugpijn\\LR_EPDs' # Use a raw string for the path\n",
    "output_markdown_file = 'pdf_analysis_report.md' # Name of the output Markdown file\n",
    "PMI_BIGRAM_FREQ_THRESHOLD = 3 # Minimum frequency for a bigram to be included in Average PMI calculation\n",
    "# Removed: JSD_EPSILON = 1e-9 # Small value used in old JSD calculation\n",
    "\n",
    "# --- Suppress Warnings ---\n",
    "# Suppress warnings originating from any pdfminer module (including PDFSyntaxWarning)\n",
    "# This is a more general approach than filtering by a specific warning class name\n",
    "warnings.filterwarnings(\"ignore\", module='pdfminer\\..*')\n",
    "# You could add other filters here if other specific warnings are bothersable, e.g.,\n",
    "# warnings.filterwarnings(\"ignore\", message=\"some specific message\")\n",
    "\n",
    "# --- Data Storage ---\n",
    "file_data = [] # List to store dictionaries, each containing data for one file\n",
    "all_extracted_text_for_vocab = \"\" # String to accumulate all text for vocabulary analysis (for overall vocab and PMI)\n",
    "all_extracted_chars = \"\" # String to accumulate all characters (for overall char entropy)\n",
    "all_char_counts = [] # List to store character counts for each file (for overall average)\n",
    "\n",
    "report_output = StringIO() # Use StringIO to capture print statements\n",
    "\n",
    "# Redirect stdout to capture print output\n",
    "original_stdout = sys.stdout\n",
    "sys.stdout = report_output\n",
    "\n",
    "# --- Helper Function for Text Extraction and Structural Element Detection ---\n",
    "def analyze_pdf_content(pdf_path):\n",
    "    \"\"\"\n",
    "    Extracts text and detects structural elements (tables, figures, annotations)\n",
    "    from a PDF file using pdfplumber.\n",
    "    Returns extracted text, table count, figure count, annotation count.\n",
    "    \"\"\"\n",
    "    extracted_text = \"\"\n",
    "    table_count = 0\n",
    "    figure_count = 0\n",
    "    annotation_count = 0\n",
    "\n",
    "    try:\n",
    "        # Use warnings.catch_warnings to temporarily manage warnings within this function if needed,\n",
    "        # but filtering globally at the start is simpler for this case.\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for page in pdf.pages:\n",
    "                # Extract text\n",
    "                extracted_text += page.extract_text() or \"\" # Add text, handle None\n",
    "\n",
    "                # Detect tables\n",
    "                tables = page.extract_tables()\n",
    "                table_count += len(tables)\n",
    "\n",
    "                # Detect figures (images)\n",
    "                figure_count += len(page.images)\n",
    "\n",
    "                # Detect annotations\n",
    "                annotation_count += len(page.annots)\n",
    "\n",
    "    except pdfplumber.PDFSyntaxError as e:\n",
    "         # Print warnings directly to original stdout so they appear during processing\n",
    "         original_stdout.write(f\"Warning: PDF Syntax Error in {os.path.basename(pdf_path)}: {e}\\n\")\n",
    "         # Return empty data for this file if there's a syntax error\n",
    "         return \"\", 0, 0, 0\n",
    "    except Exception as e:\n",
    "        # Print warnings directly to original stdout\n",
    "        original_stdout.write(f\"Warning: Error processing {os.path.basename(pdf_path)} with pdfplumber: {e}\\n\")\n",
    "        # Return empty data for this file if there's any other error\n",
    "        return \"\", 0, 0, 0\n",
    "\n",
    "    return extracted_text, table_count, figure_count, annotation_count\n",
    "\n",
    "\n",
    "# --- Helper Function for Language Detection ---\n",
    "def detect_language(text):\n",
    "    \"\"\"Detects the language of the input text.\"\"\"\n",
    "    if not text.strip():\n",
    "        return \"N/A (No text)\"\n",
    "    try:\n",
    "        # langdetect works best on larger text samples.\n",
    "        # Use a sample if the text is very long to speed things up,\n",
    "        # but for typical documents, using the full text is fine.\n",
    "        # Limit text length for detection to avoid potential issues with very large texts\n",
    "        sample_text = text[:5000] if len(text) > 5000 else text\n",
    "        if not sample_text.strip():\n",
    "             return \"N/A (No text in sample)\"\n",
    "        # Use warnings.catch_warnings to temporarily manage warnings within this function if needed\n",
    "        with warnings.catch_warnings():\n",
    "             # langdetect can sometimes issue warnings, e.g., about language probabilities\n",
    "             warnings.filterwarnings(\"ignore\", category=UserWarning) # Example: suppress UserWarnings from langdetect\n",
    "             return detect(sample_text)\n",
    "    except LangDetectException:\n",
    "        return \"Undetected\"\n",
    "    except Exception as e:\n",
    "        # Print warnings directly to original stdout\n",
    "        original_stdout.write(f\"Warning: Error during language detection: {e}\\n\")\n",
    "        return \"Error\"\n",
    "\n",
    "# --- Helper Functions for Information Theory Metrics ---\n",
    "\n",
    "def calculate_shannon_entropy(items):\n",
    "    \"\"\"Calculates Shannon entropy for a list of items (chars or words).\"\"\"\n",
    "    if not items:\n",
    "        return 0.0\n",
    "    counts = Counter(items)\n",
    "    total_items = len(items)\n",
    "    entropy = 0.0\n",
    "    for count in counts.values():\n",
    "        probability = count / total_items\n",
    "        # Add a small epsilon to probability to avoid log(0) if needed, though standard formula handles p > 0\n",
    "        if probability > 0:\n",
    "             entropy -= probability * math.log2(probability)\n",
    "    return entropy\n",
    "\n",
    "# Removed: calculate_kullback_leibler_divergence function\n",
    "# Removed: calculate_jensen_shannon_divergence function\n",
    "\n",
    "def calculate_kl_divergence(text1, text2, unit='word'):\n",
    "    \"\"\"\n",
    "    Calculates the Jensen-Shannon Divergence (JSD) between the distributions\n",
    "    of tokens (chars or words) in two texts.\n",
    "    Uses scipy.stats.entropy for KL calculation.\n",
    "    \"\"\"\n",
    "    if not text1 or not text2:\n",
    "        return np.nan # Cannot compute divergence with empty text\n",
    "\n",
    "    if unit == 'char':\n",
    "        tokens1 = list(text1)\n",
    "        tokens2 = list(text2)\n",
    "    elif unit == 'word':\n",
    "        # Simple word tokenization and lowercase\n",
    "        tokens1 = re.findall(r'\\b\\w+\\b', text1.lower()) # Use consistent tokenization\n",
    "        tokens2 = re.findall(r'\\b\\w+\\b', text2.lower()) # Use consistent tokenization\n",
    "    else:\n",
    "        raise ValueError(\"Unit must be 'char' or 'word'\")\n",
    "\n",
    "    if not tokens1 or not tokens2:\n",
    "          return np.nan\n",
    "\n",
    "    # Build a combined vocabulary\n",
    "    vocab = list(set(tokens1 + tokens2))\n",
    "    vocab_size = len(vocab)\n",
    "\n",
    "    # Create frequency distributions\n",
    "    counts1 = Counter(tokens1)\n",
    "    counts2 = Counter(tokens2)\n",
    "\n",
    "    # Create probability distributions over the combined vocabulary\n",
    "    # Add a small smoothing value to avoid zero probabilities, which cause log(0) issues\n",
    "    smoothing = 1e-9\n",
    "    p1 = np.array([counts1.get(token, 0) + smoothing for token in vocab])\n",
    "    p2 = np.array([counts2.get(token, 0) + smoothing for token in vocab])\n",
    "\n",
    "    # Normalize to get probability distributions\n",
    "    p1 = p1 / p1.sum()\n",
    "    p2 = p2 / p2.sum()\n",
    "\n",
    "    # Calculate KL Divergence using scipy.stats.entropy\n",
    "    # entropy(pk, qk) calculates KL(pk || qk)\n",
    "    kl_pq = entropy(p1, qk=p2, base=2) # Use base=2 for bits\n",
    "    kl_qp = entropy(p2, qk=p1, base=2) # Use base=2 for bits\n",
    "\n",
    "\n",
    "    # Calculate Jensen-Shannon Divergence (JSD) - symmetric and bounded\n",
    "    # JSD = 0.5 * (KL(P || Q) + KL(Q || P))\n",
    "    jsd = 0.5 * (kl_pq + kl_qp)\n",
    "\n",
    "    return jsd\n",
    "\n",
    "\n",
    "def calculate_avg_bigram_pmi(text, min_freq=3):\n",
    "    \"\"\"\n",
    "    Calculates the average Pointwise Mutual Information (PMI) for word bigrams\n",
    "    that occur at least min_freq times.\n",
    "    A proxy metric related to Mutual Information, measuring word association strength.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return 0.0\n",
    "\n",
    "    # Simple word tokenization and lowercase\n",
    "    # Use the same tokenization as the main script for consistency\n",
    "    words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "    if len(words) < 2:\n",
    "        return 0.0\n",
    "\n",
    "    word_counts = Counter(words)\n",
    "    bigram_counts = Counter(zip(words[:-1], words[1:])) # Count occurrences of bigrams\n",
    "\n",
    "    total_words = len(words)\n",
    "    # total_bigrams = len(list(zip(words[:-1], words[1:]))) # Count actual bigram instances\n",
    "\n",
    "    pmi_values = []\n",
    "    for bigram, bigram_count in bigram_counts.items():\n",
    "        # Only consider bigrams that meet the minimum frequency threshold\n",
    "        if bigram_count >= min_freq:\n",
    "            word1, word2 = bigram\n",
    "\n",
    "            # Calculate probabilities (using total_words for marginals is common)\n",
    "            p_w1 = word_counts[word1] / total_words if total_words > 0 else 0\n",
    "            p_w2 = word_counts[word2] / total_words if total_words > 0 else 0\n",
    "            # Use total words as normalization for bigram probability as well for PMI formula\n",
    "            p_w1_w2 = bigram_count / total_words if total_words > 0 else 0\n",
    "\n",
    "            # Avoid log(0) - check if probabilities are positive\n",
    "            if p_w1 > 0 and p_w2 > 0 and p_w1_w2 > 0:\n",
    "                 # PMI formula: log2( P(w1,w2) / (P(w1) * P(w2)) )\n",
    "                 pmi = math.log2(p_w1_w2 / (p_w1 * p_w2))\n",
    "                 pmi_values.append(pmi)\n",
    "            # Note: Bigrams that never appear together with positive marginals would have PMI -infinity.\n",
    "            # We only average over bigrams that *do* appear (with >= min_freq).\n",
    "\n",
    "    if not pmi_values:\n",
    "        return 0.0 # Return 0 if no bigrams meet min_freq or text was empty/too short\n",
    "\n",
    "    return np.mean(pmi_values)\n",
    "\n",
    "\n",
    "# --- Analysis ---\n",
    "# Print initial message to original stdout\n",
    "original_stdout.write(f\"Analyzing PDF files in directory: {pdf_directory}\\n\")\n",
    "\n",
    "\n",
    "if not os.path.isdir(pdf_directory):\n",
    "    # Print error to original stdout\n",
    "    original_stdout.write(f\"Error: Directory not found at {pdf_directory}\\n\")\n",
    "else:\n",
    "    # Iterate through all entries in the directory\n",
    "    for entry_name in os.listdir(pdf_directory):\n",
    "        entry_path = os.path.join(pdf_directory, entry_name)\n",
    "\n",
    "        # Check if the entry is a file and ends with .pdf (case-insensitive)\n",
    "        if os.path.isfile(entry_path) and entry_name.lower().endswith('.pdf'):\n",
    "\n",
    "            file_info = {} # Dictionary to store data for the current file\n",
    "            file_info['filename'] = entry_name\n",
    "            file_info['filepath'] = entry_path\n",
    "\n",
    "            # 1. & 2. Storage Size\n",
    "            try:\n",
    "                file_size_bytes = os.path.getsize(entry_path) # size in bytes\n",
    "                file_info['storage_size_bytes'] = file_size_bytes\n",
    "                file_info['storage_size_mb'] = file_size_bytes / (1024 * 1024)\n",
    "            except Exception as e:\n",
    "                # Print warnings directly to original stdout\n",
    "                original_stdout.write(f\"Warning: Could not get size for {entry_name}: {e}\\n\")\n",
    "                file_info['storage_size_bytes'] = 0\n",
    "                file_info['storage_size_mb'] = 0\n",
    "\n",
    "\n",
    "            # 3. Textual Content Size & Extraction + 7. Structural Elements\n",
    "            text, table_count, figure_count, annotation_count = analyze_pdf_content(entry_path)\n",
    "\n",
    "            file_info['extracted_text'] = text # Store text for language\n",
    "            # Calculate character count for the current file\n",
    "            file_info['char_count'] = len(text)\n",
    "            all_char_counts.append(file_info['char_count']) # Add to list for overall average\n",
    "\n",
    "            # Simple word tokenization and lowercase\n",
    "            tokens = re.findall(r'\\b\\w+\\b', text.lower()) # Convert to lower case for vocabulary size\n",
    "            word_count = len(tokens)\n",
    "            file_info['word_count'] = word_count\n",
    "            file_info['tokens'] = tokens # Store tokens for JSD calculation\n",
    "\n",
    "\n",
    "            # Calculate unique words for the current file\n",
    "            unique_words_in_file = set(tokens)\n",
    "            file_info['unique_word_count'] = len(unique_words_in_file)\n",
    "\n",
    "            # Calculate per-file Shannon Entropy\n",
    "            file_info['char_entropy'] = calculate_shannon_entropy(list(text)) # Character entropy\n",
    "            file_info['word_entropy'] = calculate_shannon_entropy(tokens) # Word entropy\n",
    "\n",
    "            # Calculate per-file Average Bigram PMI\n",
    "            file_info['average_pmi'] = calculate_avg_bigram_pmi(text, min_freq=PMI_BIGRAM_FREQ_THRESHOLD)\n",
    "\n",
    "\n",
    "            # Accumulate text and characters for overall vocabulary, PMI, and char entropy later\n",
    "            all_extracted_text_for_vocab += text + \" \" # Add a space to ensure separation\n",
    "            all_extracted_chars += text # Accumulate all characters\n",
    "\n",
    "\n",
    "            # Store structural element counts\n",
    "            file_info['table_count'] = table_count\n",
    "            file_info['figure_count'] = figure_count\n",
    "            file_info['annotation_count'] = annotation_count\n",
    "            # Add boolean flags for easy checking\n",
    "            file_info['has_tables'] = table_count > 0\n",
    "            file_info['has_figures'] = figure_count > 0\n",
    "            file_info['has_annotations'] = annotation_count > 0\n",
    "\n",
    "\n",
    "            # 6. Language Detection\n",
    "            file_info['language'] = detect_language(text)\n",
    "\n",
    "            # 4. Information Content (Estimated)\n",
    "            # Simple estimation based on file size in bits\n",
    "            file_info['estimated_info_content_bits_filesize'] = file_size_bytes * 8\n",
    "\n",
    "\n",
    "            file_data.append(file_info) # Add the file's data to the list\n",
    "\n",
    "\n",
    "    # --- Calculate Overall Dataset Metrics (needed for per-file JSD) ---\n",
    "    overall_char_entropy = calculate_shannon_entropy(list(all_extracted_chars))\n",
    "\n",
    "    overall_tokens = re.findall(r'\\b\\w+\\b', all_extracted_text_for_vocab.lower())\n",
    "    overall_word_counts = Counter(overall_tokens)\n",
    "    total_words_overall = len(overall_tokens)\n",
    "    overall_word_distribution = {word: count / total_words_overall for word, count in overall_word_counts.items()} if total_words_overall > 0 else {}\n",
    "    overall_vocabulary = set(overall_tokens)\n",
    "\n",
    "\n",
    "    overall_word_entropy = calculate_shannon_entropy(overall_tokens)\n",
    "\n",
    "    # Calculate Average Bigram PMI for the overall dataset\n",
    "    overall_average_pmi = calculate_avg_bigram_pmi(all_extracted_text_for_vocab, min_freq=PMI_BIGRAM_FREQ_THRESHOLD)\n",
    "\n",
    "    # Calculate Overall Average Document Length (Characters)\n",
    "    overall_avg_doc_length_chars = np.mean(all_char_counts) if all_char_counts else 0\n",
    "\n",
    "    # --- Calculate Per-File JSD (compared to overall dataset distribution) ---\n",
    "    if file_data and overall_word_distribution:\n",
    "        for file_info in file_data:\n",
    "            file_tokens = file_info['tokens']\n",
    "            file_word_counts = Counter(file_tokens)\n",
    "            total_file_words = len(file_tokens)\n",
    "            file_word_distribution = {word: count / total_file_words for word, count in file_word_counts.items()} if total_file_words > 0 else {}\n",
    "\n",
    "            # Calculate JSD between file distribution and overall distribution\n",
    "            # Use the provided calculate_kl_divergence function for JSD\n",
    "            jsd_value = calculate_kl_divergence(file_info['extracted_text'], all_extracted_text_for_vocab, unit='word')\n",
    "            file_info['js_dist'] = jsd_value\n",
    "    else:\n",
    "        # Set JSD to NaN if no data or no overall distribution (consistent with calculate_kl_divergence)\n",
    "        for file_info in file_data:\n",
    "             file_info['js_dist'] = np.nan\n",
    "\n",
    "\n",
    "    # --- Per-File Reporting (Captured for Markdown) ---\n",
    "    print(\"\\n## Per-File Analysis Results\") # Markdown Heading\n",
    "    if file_data:\n",
    "        # Create a Markdown table for per-file data\n",
    "        # Added 'JS Dist' column header\n",
    "        print(\"\\n| Filename | Size (MB) | Word Count | Unique Words | doc length | Language | Tables | Figures | Annotations | Char Entropy | Word Entropy | Avg PMI | JS Dist |\")\n",
    "        # Updated separator line to match the new column count\n",
    "        print(\"|---|---|---|---|---|---|---|---|---|---|---|---|---|\")\n",
    "        for file_info in file_data:\n",
    "            # Extract only the digits from the filename\n",
    "            digits_in_filename = re.findall(r'\\d+', file_info['filename'])\n",
    "            last_five_digits = \"\".join(digits_in_filename)[-5:] if digits_in_filename else \"\"\n",
    "\n",
    "            # Determine display filename\n",
    "            display_filename = last_five_digits\n",
    "            # Only add \"...\" if there were other characters besides the last 5 digits\n",
    "            if len(file_info['filename'].replace('.', '').replace('_', '').replace('-', '').replace(' ', '')) > len(last_five_digits):\n",
    "                 display_filename = \"...\" + display_filename\n",
    "\n",
    "            # Format JSD value, handle NaN\n",
    "            js_dist_formatted = f\"{file_info['js_dist']:.4f}\" if not np.isnan(file_info['js_dist']) else \"NaN\"\n",
    "\n",
    "            # Added file_info['js_dist'] to the row\n",
    "            print(f\"| {display_filename} | {file_info['storage_size_mb']:.2f} | {file_info['word_count']} | {file_info['unique_word_count']} | {file_info['char_count']} | {file_info['language']} | {file_info['table_count']} | {file_info['figure_count']} | {file_info['annotation_count']} | {file_info['char_entropy']:.2f} | {file_info['word_entropy']:.2f} | {file_info['average_pmi']:.4f} | {js_dist_formatted} |\")\n",
    "\n",
    "    else:\n",
    "        print(\"No PDF files found in the specified directory.\")\n",
    "\n",
    "\n",
    "    # --- Overall Dataset Summary (Captured for Markdown) ---\n",
    "    print(\"\\n## Overall Dataset Summary\") # Markdown Heading\n",
    "\n",
    "    total_files = len(file_data)\n",
    "    print(f\"\\n### 1. Number of PDF Files: {total_files}\") # Markdown Subheading\n",
    "\n",
    "    if total_files > 0:\n",
    "        # 2. Storage Size\n",
    "        all_storage_sizes_bytes = [f['storage_size_bytes'] for f in file_data]\n",
    "        total_storage_size_bytes = sum(all_storage_sizes_bytes)\n",
    "        total_storage_size_mb = total_storage_size_bytes / (1024 * 1024)\n",
    "        print(f\"\\n### 2. Storage Size\") # Markdown Subheading\n",
    "        print(f\"- **Total:** {total_storage_size_mb:.2f} MB ({total_storage_size_bytes} bytes)\")\n",
    "        if all_storage_sizes_bytes:\n",
    "            avg_file_size_mb = statistics.mean(all_storage_sizes_bytes) / (1024 * 1024)\n",
    "            min_file_size_mb = min(all_storage_sizes_bytes) / (1024 * 1024)\n",
    "            max_file_size_mb = max(all_storage_sizes_bytes) / (1024 * 1024)\n",
    "            print(f\"- **Average:** {avg_file_size_mb:.2f} MB\")\n",
    "            print(f\"- **Range:** ({min_file_size_mb:.2f} MB, {max_file_size_mb:.2f} MB)\")\n",
    "\n",
    "        # 3. Textual Content Size\n",
    "        all_word_counts = [f['word_count'] for f in file_data]\n",
    "        total_word_count = sum(all_word_counts)\n",
    "        print(f\"\\n### 3. Textual Content Size\") # Markdown Subheading\n",
    "        print(f\"- **Total words/tokens:** {total_word_count}\")\n",
    "        if all_word_counts:\n",
    "            avg_word_count = statistics.mean(all_word_counts)\n",
    "            print(f\"- **Average words/tokens per document:** {avg_word_count:.2f}\")\n",
    "\n",
    "            # Calculate unique tokens (vocabulary size) from accumulated text\n",
    "            all_tokens = re.findall(r'\\b\\w+\\b', all_extracted_text_for_vocab.lower())\n",
    "            unique_tokens = set(all_tokens)\n",
    "            vocabulary_size = len(unique_tokens)\n",
    "            print(f\"- **Unique tokens across dataset (vocabulary size):** {vocabulary_size}\") # Clarified label\n",
    "        else:\n",
    "             print(f\"- **Average words/tokens per document:** 0\")\n",
    "             print(f\"- **Unique tokens across dataset (vocabulary size):** 0\")\n",
    "\n",
    "        # 4. Information Content (Estimated)\n",
    "        total_estimated_info_content_bits_filesize = total_storage_size_bytes * 8\n",
    "        print(f\"\\n### 4. Information Content (Estimated)\") # Markdown Subheading\n",
    "        print(f\"- **Total estimated info content (based on total file size):** {total_estimated_info_content_bits_filesize} bits\")\n",
    "        print(f\"- *Note: This estimate is based on raw file size in bits.*\")\n",
    "\n",
    "        # 4b. Information Theory Metrics (Overall Dataset)\n",
    "        print(f\"\\n### 4b. Information Theory Metrics (Overall Dataset)\") # Markdown Subheading\n",
    "        print(f\"- **Overall Character Entropy:** {overall_char_entropy:.2f} bits/character\")\n",
    "        print(f\"- **Overall Word Entropy:** {overall_word_entropy:.2f} bits/word\")\n",
    "        print(f\"- **Average Bigram PMI (Threshold={PMI_BIGRAM_FREQ_THRESHOLD}):** {overall_average_pmi:.4f}\")\n",
    "        print(f\"  *Note: Average PMI is calculated for bigrams appearing at least {PMI_BIGRAM_FREQ_THRESHOLD} times across the dataset.*\")\n",
    "        print(f\"  *Note: Per-file JSD is calculated against the overall dataset word distribution.*\")\n",
    "\n",
    "\n",
    "        # Added Overall Average Document Length (Characters)\n",
    "        print(f\"- **Overall Average Document Length (Characters):** {overall_avg_doc_length_chars:.2f}\")\n",
    "\n",
    "\n",
    "        # 5. Document Length Distribution\n",
    "        print(f\"\\n### 5. Document Length Distribution (in words/tokens)\") # Markdown Subheading\n",
    "        if all_word_counts:\n",
    "            mean_len = statistics.mean(all_word_counts)\n",
    "            median_len = statistics.median(all_word_counts)\n",
    "            # Ensure std dev calculation is valid for >1 data points\n",
    "            std_dev_len = statistics.stdev(all_word_counts) if len(all_word_counts) > 1 else 0\n",
    "            print(f\"- **Mean length:** {mean_len:.2f}\")\n",
    "            print(f\"- **Median length:** {median_len}\")\n",
    "            print(f\"- **Standard deviation:** {std_dev_len:.2f}\")\n",
    "\n",
    "\n",
    "        # 6. Language Distribution\n",
    "        print(f\"\\n### 6. Language Distribution\") # Markdown Subheading\n",
    "        all_languages = [f['language'] for f in file_data]\n",
    "        language_counts = Counter(all_languages)\n",
    "        print(\"- **Counts:**\")\n",
    "        for lang, count in language_counts.most_common():\n",
    "            print(f\"   - {lang}: {count} files\")\n",
    "\n",
    "\n",
    "        # 7. Structural Elements Summary\n",
    "        print(f\"\\n### 7. Structural Elements Summary\") # Markdown Subheading\n",
    "        total_tables_found = sum([f['table_count'] for f in file_data])\n",
    "        total_figures_found = sum([f['figure_count'] for f in file_data])\n",
    "        total_annotations_found = sum([f['annotation_count'] for f in file_data])\n",
    "\n",
    "        files_with_tables = sum([f['has_tables'] for f in file_data])\n",
    "        files_with_figures = sum([f['has_figures'] for f in file_data])\n",
    "        files_with_annotations = sum([f['has_annotations'] for f in file_data])\n",
    "\n",
    "        print(f\"- **Total Tables found:** {total_tables_found}\")\n",
    "        print(f\"- **Total Figures/Images found:** {total_figures_found}\")\n",
    "        print(f\"- **Total Annotations found:** {total_annotations_found}\")\n",
    "        print(f\"- **Files with Tables:** {files_with_tables} ({files_with_tables/total_files*100:.2f}%)\")\n",
    "        print(f\"- **Files with Figures/Images:** {files_with_figures} ({files_with_figures/total_files*100:.2f}%)\")\n",
    "        print(f\"- **Files with Annotations:** {files_with_annotations} ({files_with_annotations/total_files*100:.2f}%)\")\n",
    "\n",
    "\n",
    "    else:\n",
    "        print(\"No data collected from PDF files.\")\n",
    "\n",
    "\n",
    "# --- End of Captured Output ---\n",
    "sys.stdout = original_stdout # Restore stdout\n",
    "sys.stdout.flush() # Explicitly flush the buffer\n",
    "\n",
    "\n",
    "# --- Display Report in Console ---\n",
    "markdown_content = report_output.getvalue() # Get the captured output\n",
    "print(\"\\n\" + \"=\"*30 + \" PDF Analysis Report \" + \"=\"*30) # Separator\n",
    "print(\"\\n*Note: Markdown rendering may vary depending on the viewer. Multi-column layout and font size control are not standard Markdown features.*\\n\")\n",
    "print(markdown_content) # Print the captured report content to console\n",
    "print(\"=\"*79) # Separator\n",
    "\n",
    "\n",
    "# --- Save Output to Markdown File ---\n",
    "try:\n",
    "    with open(output_markdown_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(markdown_content)\n",
    "    print(f\"\\nAnalysis report also saved to {output_markdown_file}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nError saving report to {output_markdown_file}: {e}\")\n",
    "\n",
    "report_output.close() # Close the StringIO object\n",
    "# --- End of Script ---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azure-openai-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
