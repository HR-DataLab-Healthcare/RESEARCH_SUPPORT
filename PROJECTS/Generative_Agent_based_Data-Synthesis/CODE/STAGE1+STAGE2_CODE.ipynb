{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7955132",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 12px;\">\n",
    "\n",
    "To run the Python code in this notebook locally, you'll need to install several packages.\n",
    "\n",
    "### Required Packages:\n",
    "\n",
    "*   **python-dotenv**: Used for managing environment variables, especially for API keys and endpoints.\n",
    "    ```bash\n",
    "    pip install python-dotenv\n",
    "    ```\n",
    "*   **PyMuPDF**: A library for accessing PDF files (extracting text, images, etc.). It is imported as `fitz`.\n",
    "    ```bash\n",
    "    pip install PyMuPDF\n",
    "    ```\n",
    "*   **openai**: The official Python library for interacting with OpenAI APIs, including Azure OpenAI.\n",
    "    ```bash\n",
    "    pip install openai\n",
    "    ```\n",
    "\n",
    "The `os` and `glob` modules are part of the Python standard library and do not require separate installation.\n",
    "\n",
    "### Optional for Ollama Users:\n",
    "\n",
    "If you plan to adapt the script to use Ollama (a platform for running large language models locally), you will also need:\n",
    "\n",
    "*   **ollama**: The Python client library for Ollama.\n",
    "    ```bash\n",
    "    pip install ollama\n",
    "    ```\n",
    "\n",
    "Ensure you have Python installed on your system. \n",
    "\n",
    "You can then install these packages using `pip`, Python's package installer. \n",
    "\n",
    "It's recommended to use a virtual environment to manage your project dependencies.\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b862e01",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 12px;\">\n",
    "\n",
    "## Guide: Selecting and Running an Ollama Model (e.g., `mistral-small`) Locally on Windows 11\n",
    "\n",
    "This guide will walk you through selecting an Ollama model from their library and running it locally on your Windows 11 PC. We'll use `mistral-small` as an example.\n",
    "\n",
    "### 1. Install Ollama on Windows 11\n",
    "\n",
    "*   **Download Ollama:** Go to the official Ollama website: [https://ollama.com/](https://ollama.com/)\n",
    "*   Click on the \"Download\" button.\n",
    "*   Select the \"Download for Windows\" option.\n",
    "*   Run the downloaded installer and follow the on-screen instructions. This will typically install Ollama and make the `ollama` command available in your terminal (Command Prompt, PowerShell, or Windows Terminal).\n",
    "\n",
    "### 2. Select a Model from the Ollama Library\n",
    "\n",
    "*   **Browse the Library:** Visit the Ollama model library: [https://ollama.com/library](https://ollama.com/library) (You can sort by newest, most popular, etc. The link provided `?sort=newest` sorts by newest).\n",
    "*   **Find Your Model:** For this example, we're looking for `mistral-small`. You can search for it or browse the list.\n",
    "    *   Clicking on a model (e.g., `mistral-small`) will take you to its page, showing details and tags (versions).\n",
    "\n",
    "### 3. Pull the Model using the Command Line\n",
    "\n",
    "*   **Open Your Terminal:** Open Command Prompt, PowerShell, or Windows Terminal.\n",
    "*   **Pull the Model:** To download the `mistral-small` model (it will default to the latest version/tag), type the following command and press Enter:\n",
    "    ```bash\n",
    "    ollama pull mistral-small\n",
    "    ```\n",
    "    You will see a download progress. The model files can be several gigabytes, so this might take some time depending on your internet connection.\n",
    "    *   If you want a specific version/tag of a model (e.g., if `mistral-small` had a tag like `7b-instruct-v0.2`), you would use `ollama pull mistral-small:7b-instruct-v0.2`. For `mistral-small`, just `ollama pull mistral-small` is usually sufficient for the latest.\n",
    "\n",
    "### 4. Run the Model Locally\n",
    "\n",
    "Once the model is downloaded, you can run it interactively.\n",
    "\n",
    "*   **Run Interactively:** In the same terminal, type:\n",
    "    ```bash\n",
    "    ollama run mistral-small\n",
    "    ```\n",
    "*   **Interact:** The Ollama CLI will load the model, and you'll see a prompt like `>>> Send a message (/? for help)`. You can now type your questions or prompts and press Enter. The model will generate a response.\n",
    "    *   Type `/?` for a list of commands within the interactive session (e.g., `/bye` to exit, `/save` to save a session).\n",
    "\n",
    "### 5. (Optional) Using the Model with Python\n",
    "\n",
    "The Python code in this notebook is set up for Azure OpenAI. To use Ollama with Python (as briefly mentioned in the configuration cell):\n",
    "\n",
    "*   **Install the Ollama Python Library:**\n",
    "    ```bash\n",
    "    pip install ollama\n",
    "    ```\n",
    "*   **Adapt the Python Script:** You would need to modify the Python script to:\n",
    "    1.  Import the `ollama` library: `import ollama`\n",
    "    2.  Initialize the Ollama client: `client = ollama.Client()` (assuming Ollama is running on the default `http://localhost:11434`)\n",
    "    3.  Replace the Azure OpenAI API calls (`client.chat.completions.create`) with Ollama's equivalent, for example:\n",
    "        ```python\n",
    "        response = ollama.chat(\n",
    "            model='mistral-small', # Or the model you pulled\n",
    "            messages=[\n",
    "                {'role': 'user', 'content': 'Why is the sky blue?'}\n",
    "            ]\n",
    "        )\n",
    "        print(response['message']['content'])\n",
    "        ```\n",
    "    4.  Adjust prompts and parameters as needed, because different models respond best to different prompting styles.\n",
    "\n",
    "This provides a basic way to get started with Ollama and a specific model like `mistral-small` on your Windows 11 machine. For more advanced usage or troubleshooting, refer to the official Ollama documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008ef2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# --- Configuration for Azure OpenAI ---\n",
    "# IMPORTANT: To run this script, you need to set the following environment variables\n",
    "# with your Azure OpenAI service credentials.\n",
    "#\n",
    "# How to set environment variables:\n",
    "#\n",
    "# 1. Using a .env file (Recommended for local development):\n",
    "#    - Install the python-dotenv library: pip install python-dotenv\n",
    "#    - Create a file named .env in the same directory as this script.\n",
    "#    - Add your credentials to the .env file like this:\n",
    "#      AZURE_OPENAI_ENDPOINT=\"your_azure_openai_endpoint_here\"\n",
    "#      AZURE_OPENAI_API_KEY=\"your_azure_openai_api_key_here\"\n",
    "#      AZURE_OPENAI_DEPLOYMENT_NAME=\"your_gpt4_deployment_name_here\"\n",
    "#      AZURE_OPENAI_API_VERSION=\"2024-02-01\" # Or your specific API version\n",
    "#    - The script will then load these variables.\n",
    "#\n",
    "# 2. Setting them directly in your shell (Temporary for a session):\n",
    "#    For Linux/macOS:\n",
    "#      export AZURE_OPENAI_ENDPOINT=\"your_azure_openai_endpoint\"\n",
    "#      export AZURE_OPENAI_API_KEY=\"your_azure_openai_api_key\"\n",
    "#      export AZURE_OPENAI_DEPLOYMENT_NAME=\"your_gpt4_deployment_name\"\n",
    "#      export AZURE_OPENAI_API_VERSION=\"your_api_version\"\n",
    "#\n",
    "#    For Windows (Command Prompt):\n",
    "#      set AZURE_OPENAI_ENDPOINT=\"your_azure_openai_endpoint\"\n",
    "#      set AZURE_OPENAI_API_KEY=\"your_azure_openai_api_key\"\n",
    "#      set AZURE_OPENAI_DEPLOYMENT_NAME=\"your_gpt4_deployment_name\"\n",
    "#      set AZURE_OPENAI_API_VERSION=\"your_api_version\"\n",
    "#\n",
    "#    For Windows (PowerShell):\n",
    "#      $env:AZURE_OPENAI_ENDPOINT=\"your_azure_openai_endpoint\"\n",
    "#      $env:AZURE_OPENAI_API_KEY=\"your_azure_openai_api_key\"\n",
    "#      $env:AZURE_OPENAI_DEPLOYMENT_NAME=\"your_gpt4_deployment_name\"\n",
    "#      $env:AZURE_OPENAI_API_VERSION=\"your_api_version\"\n",
    "\n",
    "# Attempt to load .env file if python-dotenv is installed\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    if load_dotenv():\n",
    "        print(\"Loaded environment variables from .env file.\")\n",
    "    else:\n",
    "        # This can happen if .env exists but is empty, or if python-dotenv is not installed\n",
    "        # and load_dotenv() is a dummy function. We'll proceed to os.getenv() anyway.\n",
    "        pass\n",
    "except ImportError:\n",
    "    print(\"python-dotenv library not found, .env file will not be loaded. \"\n",
    "          \"Ensure environment variables are set manually if not using a .env file.\")\n",
    "\n",
    "AZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "AZURE_OPENAI_API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "AZURE_OPENAI_DEPLOYMENT_NAME = os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\")\n",
    "# Use the API version from your original script as a default if not set in environment\n",
    "API_VERSION = os.getenv(\"AZURE_OPENAI_API_VERSION\", \"2024-12-01-preview\")\n",
    "\n",
    "# --- Information for Ollama Users ---\n",
    "# This script is currently configured to use Azure OpenAI.\n",
    "# If you wish to adapt this script or use Ollama for other projects:\n",
    "#\n",
    "# 1. Ollama Installation and Setup:\n",
    "#    - Ensure Ollama is installed and running on your system.\n",
    "#      Visit https://ollama.com for installation instructions.\n",
    "#    - Download models you want to use, e.g., `ollama pull llama3`.\n",
    "#\n",
    "# 2. Ollama \"Credentials\" / Endpoint:\n",
    "#    - Ollama typically runs a local server. The default API endpoint is\n",
    "#      `http://localhost:11434`. This is the \"address\" you'd use to connect.\n",
    "#    - If Ollama is running on a different host or port, or behind a reverse proxy,\n",
    "#      you would use that specific URL.\n",
    "#\n",
    "# 3. Using Ollama with Python:\n",
    "#    - You can use the `ollama` Python library: `pip install ollama`\n",
    "#    - Example usage:\n",
    "#      ```python\n",
    "#      # import ollama\n",
    "#      # client = ollama.Client(host='http://localhost:11434') # Or your custom host\n",
    "#      # response = client.chat(\n",
    "#      #   model='llama3', # Or any model you have pulled\n",
    "#      #   messages=[{'role': 'user', 'content': 'Why is the sky blue?'}],\n",
    "#      # )\n",
    "#      # print(response['message']['content'])\n",
    "#      ```\n",
    "#    - To integrate Ollama into *this* script, you would need to:\n",
    "#      - Replace the `AzureOpenAI` client initialization.\n",
    "#      - Modify the `convert_text_to_markdown` and `pseudonymize_markdown` functions\n",
    "#        to use `ollama.chat` or `ollama.generate` calls.\n",
    "#      - Adjust prompts and parameters, as Ollama models may require different\n",
    "#        prompting strategies and have different capabilities than Azure OpenAI's GPT-4.\n",
    "#\n",
    "# 4. OpenAI-Compatible Endpoint (Advanced):\n",
    "#    - Some tools or versions of Ollama (or related projects like LiteLLM) can expose\n",
    "#      an OpenAI-compatible API endpoint (e.g., at `/v1/chat/completions`).\n",
    "#    - If you have such an endpoint, you might be able to use the `openai` Python library\n",
    "#      by configuring the `base_url` and `api_key` (Ollama often doesn't require an API key,\n",
    "#      so a dummy key like \"ollama\" might be used).\n",
    "#      ```python\n",
    "#      # from openai import OpenAI\n",
    "#      # client = OpenAI(\n",
    "#      #     base_url=\"http://localhost:11434/v1\", # Example, check Ollama docs\n",
    "#      #     api_key=\"ollama\", # Or any non-empty string if not required\n",
    "#      # )\n",
    "#      ```\n",
    "#      This approach would require fewer changes to the existing API call structures\n",
    "#      but depends on the Ollama setup providing this compatibility layer.\n",
    "\n",
    "# --- Safety checks for Azure OpenAI credentials ---\n",
    "if not all([AZURE_OPENAI_ENDPOINT, AZURE_OPENAI_API_KEY, AZURE_OPENAI_DEPLOYMENT_NAME, API_VERSION]):\n",
    "    missing_vars = []\n",
    "    if not AZURE_OPENAI_ENDPOINT: missing_vars.append(\"AZURE_OPENAI_ENDPOINT\")\n",
    "    if not AZURE_OPENAI_API_KEY: missing_vars.append(\"AZURE_OPENAI_API_KEY\")\n",
    "    if not AZURE_OPENAI_DEPLOYMENT_NAME: missing_vars.append(\"AZURE_OPENAI_DEPLOYMENT_NAME\")\n",
    "    if not API_VERSION: missing_vars.append(\"API_VERSION\") # Should have a default\n",
    "    raise ValueError(\n",
    "        f\"Missing one or more Azure OpenAI environment variables: {', '.join(missing_vars)}. \"\n",
    "        \"Please set them (e.g., in a .env file or directly in your environment) \"\n",
    "        \"before running the script. Refer to the comments at the beginning of the script for instructions.\"\n",
    "    )\n",
    "else:\n",
    "    print(\"Azure OpenAI environment variables loaded successfully.\")\n",
    "\n",
    "# The rest of your imports (fitz, AzureOpenAI, glob) and script logic will follow.\n",
    "# Ensure you remove the old hardcoded assignments for the variables above.\n",
    "import fitz  # PyMuPDF\n",
    "from openai import AzureOpenAI\n",
    "import glob # Using glob is often easier for pattern matching files\n",
    "\n",
    "AZURE_OPENAI_ENDPOINT = \"https://xxxxx.openai.azure.com/\" # Your Azure OpenAI Endpoint\n",
    "AZURE_OPENAI_API_KEY = \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\" # Your Azure OpenAI API Key\n",
    "AZURE_OPENAI_DEPLOYMENT_NAME = \"GPT4.1\" # The name of your GPT-4 deployment in Azure OpenAI Studio\n",
    "API_VERSION = \"2024-12-01-preview\" \n",
    "\n",
    "# Specify the directory containing the PDF files\n",
    "PDF_DIRECTORY_PATH = r\"xxxx\\LR_EPDs\" # Use raw string for Windows paths\n",
    "# Specify the single output Markdown file\n",
    "# Specify the output paths for the combined files\n",
    "# These will be saved in the parent directory of the PDF folder\n",
    "OUTPUT_COMBINED_MD_FILE_PATH = os.path.join(os.path.dirname(PDF_DIRECTORY_PATH), \"combined_epds_markdown.md\")\n",
    "OUTPUT_COMBINED_PSEUDO_MD_FILE_PATH = os.path.join(os.path.dirname(PDF_DIRECTORY_PATH), \"pseudo_combined_epds_markdown.md\")\n",
    "\n",
    "# Categorieën van privacygevoelige gegevens (optioneel, maar helpt de AI)\n",
    "PRIVACY_CATEGORIES = [\n",
    "    \"Persoonsnamen (patiënt, arts, etc.)\",\n",
    "    \"Adressen\",\n",
    "    \"Telefoonnummers\",\n",
    "    \"E-mailadressen\",\n",
    "    \"Geboortedata\",\n",
    "    \"Burgerservicenummer (BSN) of andere ID-nummers\",\n",
    "    \"Medische klachten, symptomen of diagnoses\",\n",
    "    \"Medische behandelingen, medicatie of procedures\",\n",
    "    \"Verzekeringsgegevens\",\n",
    "    \"Financiële gegevens\",\n",
    "    \"Andere direct identificeerbare persoonlijke informatie\"\n",
    "]\n",
    "\n",
    "# Prompts for Pseudonymization\n",
    "PSEUDO_SYSTEM_MESSAGE_CONTENT = \"Vervang in de aangeleverde tekst uitsluitend de persoonsnamen (zoals patiëntnamen, namen van artsen, medewerkers, familieleden, etc.) door realistische, verzonnen pseudoniemen. Zorg ervoor dat de originele markdown opmaak van de tekst volledig behouden blijft. Geef als antwoord *alleen* de aangepaste tekst terug, zonder enige uitleg of extra commentaar.\"\n",
    "# Although a separate context_message_content variable was provided,\n",
    "# it's often more effective to incorporate the categories directly into the user prompt\n",
    "# or implicitly rely on the system prompt's instruction regarding \"persoonsnamen\".\n",
    "# Let's slightly adapt the user prompt based on common API interaction patterns.\n",
    "\n",
    "# Safety checks for placeholders\n",
    "if AZURE_OPENAI_API_KEY == \"<YOUR_AZURE_OPENAI_API_KEY>\" or AZURE_OPENAI_DEPLOYMENT_NAME == \"<YOUR_GPT4_DEPLOYMENT_NAME>\":\n",
    "    raise ValueError(\"Please replace the placeholder values for AZURE_OPENAI_API_KEY and AZURE_OPENAI_DEPLOYMENT_NAME.\")\n",
    "\n",
    "# --- PDF Text Extraction (Function remains the same) ---\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extracts text content from all pages of a PDF file.\"\"\"\n",
    "    text = \"\"\n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)\n",
    "        for page_num in range(len(doc)):\n",
    "            page = doc.load_page(page_num)\n",
    "            page_text = page.get_text(\"text\") # Extract plain text\n",
    "            if page_text:\n",
    "                text += page_text + \"\\n\\n\" # Add separator between pages\n",
    "        doc.close()\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading PDF {pdf_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# --- Azure OpenAI Client Initialization (Remains the same) ---\n",
    "client = AzureOpenAI(\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "    api_key=AZURE_OPENAI_API_KEY,\n",
    "    api_version=API_VERSION\n",
    ")\n",
    "\n",
    "# --- Markdown Conversion using Azure OpenAI (Function remains the same) ---\n",
    "def convert_text_to_markdown(text_content, pdf_filename):\n",
    "    \"\"\"Sends text to Azure OpenAI GPT-4 for Markdown conversion.\"\"\"\n",
    "    if not text_content:\n",
    "        return None\n",
    "\n",
    "    system_prompt = \"You are an AI assistant specialized in converting raw text extracted from documents into well-structured and readable Markdown format. Retain the core meaning, structure (headings, lists, paragraphs), and technical details accurately. Do not add any conversational preamble or explanation outside the Markdown itself.\"\n",
    "    user_prompt = f\"\"\"Please convert the following text, extracted from the PDF document '{pdf_filename}', into Markdown format.\n",
    "Pay close attention to potential headings, subheadings, bullet points, numbered lists, code blocks, and paragraph breaks based on the text structure.\n",
    "Format the output strictly as Markdown.\n",
    "\n",
    "--- BEGIN PDF TEXT ({pdf_filename}) ---\n",
    "{text_content}\n",
    "--- END PDF TEXT ({pdf_filename}) ---\n",
    "\n",
    "Generate only the Markdown content for this document.\n",
    "\"\"\"\n",
    "\n",
    "    print(f\"  Converting text to Markdown...\") # Simplified message\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=AZURE_OPENAI_DEPLOYMENT_NAME, # Your deployment name\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            temperature=0.2,\n",
    "            max_tokens=24000\n",
    "        )\n",
    "        markdown_output = response.choices[0].message.content\n",
    "        print(f\"  Markdown conversion successful.\") # Simplified message\n",
    "        return markdown_output\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting text to Markdown for '{pdf_filename}': {e}\")\n",
    "        return None\n",
    "\n",
    "# --- Pseudonymization using Azure OpenAI (New Function) ---\n",
    "def pseudonymize_markdown(markdown_content, pdf_filename):\n",
    "    \"\"\"Sends markdown text to Azure OpenAI GPT-4 for pseudonymization.\"\"\"\n",
    "    if not markdown_content:\n",
    "        print(f\"No markdown content to pseudonymize for {pdf_filename}.\")\n",
    "        return None\n",
    "\n",
    "    # Adapting the user prompt to include categories as context for the AI\n",
    "    pseudo_user_prompt = f\"\"\"{markdown_content}\n",
    "\n",
    "--- Instructions ---\n",
    "Vervang uitsluitend de persoonsnamen (zoals patiëntnamen, namen van artsen, medewerkers, familieleden, etc.) in de bovenstaande tekst door realistische, verzonnen pseudoniemen.\n",
    "Zorg ervoor dat de originele markdown opmaak van de tekst volledig behouden blijft.\n",
    "Geef als antwoord *alleen* de aangepaste tekst terug, zonder enige uitleg of extra commentaar.\n",
    "\"\"\"\n",
    "    # Note: Directly listing PRIVACY_CATEGORIES might be redundant if the system prompt is clear on \"persoonsnamen\".\n",
    "    # However, you could include them like:\n",
    "    # pseudo_user_prompt = f\"\"\"{markdown_content}\\n\\n--- Instructions ---\\nReplace names (like patient names, doctor names, staff names, family names, etc.) in the above text with realistic, made-up pseudonyms.\\nFocus on replacing names related to:\\n{'\\n'.join(PRIVACY_CATEGORIES)}\\n\\nEnsure the original markdown formatting is fully preserved. Provide *only* the modified text as the response, without any explanation or extra commentary.\"\"\"\n",
    "\n",
    "\n",
    "    print(f\"  Pseudonymizing markdown...\") # Simplified message\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=AZURE_OPENAI_DEPLOYMENT_NAME, # Your deployment name\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": PSEUDO_SYSTEM_MESSAGE_CONTENT},\n",
    "                {\"role\": \"user\", \"content\": pseudo_user_prompt} # Use the constructed user prompt\n",
    "            ],\n",
    "            temperature=0.2, # Keep low temperature\n",
    "            max_tokens=24000 # Keep similar max_tokens\n",
    "        )\n",
    "        pseudonymized_output = response.choices[0].message.content\n",
    "        print(f\"  Pseudonymization successful.\") # Simplified message\n",
    "        return pseudonymized_output\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error calling Azure OpenAI API for pseudonymization of '{pdf_filename}': {e}\")\n",
    "        # Consider adding a small delay before retrying or failing permanently if this is common\n",
    "        # time.sleep(5) # Example: wait for 5 seconds before potential retry logic\n",
    "        return None\n",
    "\n",
    "# --- Function to save individual Markdown files (Reused) ---\n",
    "def save_single_markdown_file(markdown_content, output_path):\n",
    "    \"\"\"Saves a single Markdown content string to a specified file.\"\"\"\n",
    "    try:\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(markdown_content)\n",
    "        print(f\"  Saved individual file: {os.path.basename(output_path)}\") # More concise message\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing individual Markdown file {output_path}: {e}\")\n",
    "\n",
    "# --- Function to save combined Markdown output (Reused) ---\n",
    "def save_combined_markdown_to_file(combined_markdown_content, output_path, file_description=\"Combined Markdown\"):\n",
    "    \"\"\"Saves the combined Markdown content from all PDFs to a single file.\"\"\"\n",
    "    try:\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(combined_markdown_content)\n",
    "        print(f\"{file_description} successfully saved to: {output_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing {file_description} file {output_path}: {e}\")\n",
    "\n",
    "\n",
    "# --- Main Execution Logic (Modified) ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Check if the input directory exists\n",
    "    if not os.path.isdir(PDF_DIRECTORY_PATH):\n",
    "        print(f\"Error: Input directory not found at '{PDF_DIRECTORY_PATH}'\")\n",
    "        exit()\n",
    "\n",
    "    # Find all PDF files in the directory (case-insensitive)\n",
    "    pdf_files = glob.glob(os.path.join(PDF_DIRECTORY_PATH, \"*.pdf\")) + \\\n",
    "                  glob.glob(os.path.join(PDF_DIRECTORY_PATH, \"*.PDF\"))\n",
    "    # Make unique in case of case variations on systems that differentiate\n",
    "    pdf_files = list(set(pdf_files))\n",
    "    pdf_files.sort() # Process in alphabetical order\n",
    "\n",
    "    if not pdf_files:\n",
    "        print(f\"No PDF files found in directory '{PDF_DIRECTORY_PATH}'\")\n",
    "        exit()\n",
    "\n",
    "    total_files = len(pdf_files)\n",
    "    print(f\"Found {total_files} PDF files to process in '{PDF_DIRECTORY_PATH}'.\")\n",
    "\n",
    "    all_markdown_content = []         # List for non-pseudonymized combined output\n",
    "    all_pseudonymized_content = []    # List for pseudonymized combined output\n",
    "\n",
    "    # Loop through each PDF file with enumeration for progress tracking\n",
    "    for index, pdf_path in enumerate(pdf_files):\n",
    "        pdf_filename = os.path.basename(pdf_path)\n",
    "        # Calculate and print progress\n",
    "        progress_percentage = ((index + 1) / total_files) * 100\n",
    "        print(f\"\\n--- Processing file {index + 1} of {total_files} ({progress_percentage:.1f}%) : {pdf_filename} ---\")\n",
    "\n",
    "        # 1. Extract text from the current PDF\n",
    "        extracted_text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "        generated_markdown = None\n",
    "        pseudonymized_markdown = None\n",
    "\n",
    "        if extracted_text:\n",
    "            # 2. Convert extracted text to Markdown using Azure OpenAI\n",
    "            generated_markdown = convert_text_to_markdown(extracted_text, pdf_filename)\n",
    "\n",
    "            if generated_markdown:\n",
    "                # Save individual Non-Pseudonymized Markdown file\n",
    "                base_filename = os.path.splitext(pdf_filename)[0]\n",
    "                individual_md_output_path = os.path.join(PDF_DIRECTORY_PATH, f\"{base_filename}.md\")\n",
    "                save_single_markdown_file(generated_markdown, individual_md_output_path)\n",
    "\n",
    "                # Add content to the list for the combined Non-Pseudonymized file\n",
    "                separator = f\"## Source PDF: {pdf_filename}\\n\\n\" # Header for combined file\n",
    "                all_markdown_content.append(separator + generated_markdown + \"\\n\\n---\\n\") # Add content with header and horizontal rule\n",
    "\n",
    "                # --- NEW: Pseudonymize the generated Markdown ---\n",
    "                pseudonymized_markdown = pseudonymize_markdown(generated_markdown, pdf_filename)\n",
    "\n",
    "                if pseudonymized_markdown:\n",
    "                    # Save individual Pseudonymized Markdown file\n",
    "                    individual_pseudo_md_output_path = os.path.join(PDF_DIRECTORY_PATH, f\"pseudo_{base_filename}.md\")\n",
    "                    save_single_markdown_file(pseudonymized_markdown, individual_pseudo_md_output_path)\n",
    "\n",
    "                    # Add content to the list for the combined Pseudonymized file\n",
    "                    pseudo_separator = f\"## Source PDF: {pdf_filename} (Pseudonymized)\\n\\n\" # Header for combined pseudo file\n",
    "                    all_pseudonymized_content.append(pseudo_separator + pseudonymized_markdown + \"\\n\\n---\\n\")\n",
    "                else:\n",
    "                    print(f\"Skipping pseudonymization and combined pseudo output for {pdf_filename} due to pseudonymization failure.\")\n",
    "                    # Add a placeholder to the combined pseudo file list\n",
    "                    all_pseudonymized_content.append(f\"## Source PDF: {pdf_filename} (Pseudonymization Failed)\\n\\n*Failed to pseudonymize this content.*\\n\\n---\\n\")\n",
    "\n",
    "            else:\n",
    "                print(f\"Skipping Markdown conversion and subsequent steps for {pdf_filename} due to Markdown conversion failure.\")\n",
    "                # Add placeholders to both combined lists\n",
    "                all_markdown_content.append(f\"## Source PDF: {pdf_filename}\\n\\n*Failed to convert this PDF to Markdown.*\\n\\n---\\n\")\n",
    "                all_pseudonymized_content.append(f\"## Source PDF: {pdf_filename} (Pseudonymization Skipped)\\n\\n*Markdown conversion failed.*\\n\\n---\\n\")\n",
    "        else:\n",
    "            print(f\"Skipping text extraction and subsequent steps for {pdf_filename} due to text extraction failure.\")\n",
    "            # Add placeholders to both combined lists\n",
    "            all_markdown_content.append(f\"## Source PDF: {pdf_filename}\\n\\n*Failed to extract text from this PDF.*\\n\\n---\\n\")\n",
    "            all_pseudonymized_content.append(f\"## Source PDF: {pdf_filename} (Pseudonymization Skipped)\\n\\n*Text extraction failed.*\\n\\n---\\n\")\n",
    "\n",
    "\n",
    "    # 4. Combine all collected markdown content\n",
    "    combined_content = \"\\n\".join(all_markdown_content)\n",
    "    combined_pseudo_content = \"\\n\".join(all_pseudonymized_content)\n",
    "\n",
    "    # 5. Save the combined content to the single output files\n",
    "    if combined_content.strip():\n",
    "        save_combined_markdown_to_file(combined_content, OUTPUT_COMBINED_MD_FILE_PATH, \"Combined Non-Pseudonymized Markdown\")\n",
    "    else:\n",
    "        print(\"No Non-Pseudonymized Markdown content was generated to save to the combined file.\")\n",
    "\n",
    "    if combined_pseudo_content.strip():\n",
    "        save_combined_markdown_to_file(combined_pseudo_content, OUTPUT_COMBINED_PSEUDO_MD_FILE_PATH, \"Combined Pseudonymized Markdown\")\n",
    "    else:\n",
    "        print(\"No Pseudonymized Markdown content was generated to save to the combined file.\")\n",
    "\n",
    "\n",
    "    print(\"\\nProcessing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328e828f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azure-openai-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
